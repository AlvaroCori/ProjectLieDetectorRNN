{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2afd9a5",
   "metadata": {},
   "source": [
    "## Antiguo proyecto de Redes Neuronales Recurrentes usando audios .wav, obteniendo su MFCC, aplicandolos a una RNN y validandolo con k fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37415a0a",
   "metadata": {},
   "source": [
    "#### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcf75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import soundfile as sf\n",
    "import json\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import EarlyStopping\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4830c4a",
   "metadata": {},
   "source": [
    "#### Funciones a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ffb974",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Obtiene el path con el nombre de todos los archivos de un directorio.\n",
    "def get_files_from_path(directory):\n",
    "    path_files = []\n",
    "    dir_list = os.listdir(directory)\n",
    "    for path in dir_list:\n",
    "        path_files.append(directory+\"\\\\\"+path)\n",
    "    return path_files\n",
    "\n",
    "\n",
    "#Extrae los paths que cumplan con un codigo\n",
    "def extract_paths_for_emotions_keys(emotions_code, files_path, get_code):\n",
    "    paths = []\n",
    "    emotions_set = set(emotions_code)\n",
    "    for code_file in files_path:\n",
    "        if (get_code(code_file) in emotions_set):\n",
    "            paths.append(code_file)\n",
    "    return paths\n",
    "\n",
    "\n",
    "#Obtiene el codigo en el nombre del archivo para el dataset CREMA-D\n",
    "def get_code_crema_d(path):\n",
    "    return path[119:122]\n",
    "\n",
    "\n",
    "#Obtiene el codigo en el nombre del archivo para el dataset SAVEE\n",
    "def get_code_savee(path):\n",
    "    return path[108]\n",
    "\n",
    "\n",
    "#Esta función abre el archivo .wav y obtiene el mfcc escalado en un vector de 40 elementos.\n",
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "    return mfccs_scaled_features\n",
    "\n",
    "\n",
    "#Esta función permite guardar los MFCC en 'features', el código en 'code' y su dirección en 'path' en un archivo json. \n",
    "def save_elements_in_json(examples_saved, name):\n",
    "    json_files = []\n",
    "    json_file = {}\n",
    "    index = 0\n",
    "    for file in examples_saved:\n",
    "        json_file = {\"id\": index, \"features\":[str(elem) for elem in file[0]] ,\"code\":file[1], \"path\":file[2]}\n",
    "        json_files.append(json_file)\n",
    "        index += 1\n",
    "    json_object = json.dumps(json_files)\n",
    "    with open(f\"{name}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "   \n",
    "\n",
    "#La función permite cargar datos del MFCC y código desde un archivo json.\n",
    "def load_elements_from_json(name):\n",
    "    f = open(f'{name}.json')\n",
    "    data = json.load(f)\n",
    "    examples = []\n",
    "    for element in data:\n",
    "        examples.append(([float(feature) for feature in (element[\"features\"])], element[\"code\"]))\n",
    "    return examples\n",
    "\n",
    "\n",
    "#La función nos permite devolver una lista de MFCC obtenidos de una lista de paths. \n",
    "#El MFCC tiene un límite que no le permite cargar archivos menor o igual a 44 kb.\n",
    "#Con el diccionario obtenemos el total de audios recuperados por emoción.\n",
    "def get_features(paths,get_code, files_filters = dict()):\n",
    "    examples = []\n",
    "    for path in paths:\n",
    "        code = get_code(path)\n",
    "        file_stats = os.stat(path)\n",
    "        if (file_stats.st_size > 44):\n",
    "            feature = features_extractor(path)\n",
    "            files_filters[code]+= 1\n",
    "            examples.append((feature,code))\n",
    "    print(f\"Se obtuvo el MFCC de unos {len(paths)} sobre {sum(files_filters[files] for files in files_filters)} audios.\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "#Selecciona n ejemplos que necesitemos y los mezcla.\n",
    "def select_elements(examples, code, quantity, new_code):\n",
    "    random.shuffle(examples)\n",
    "    elements = []\n",
    "    counter = 1\n",
    "    for example in examples:\n",
    "        if (counter > quantity):\n",
    "            break\n",
    "        if code == example[1]:\n",
    "            elements.append((example[0],new_code))\n",
    "            counter = counter + 1\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e14ce",
   "metadata": {},
   "source": [
    "Los siguientes bloques obtienen los paths y filtra las emociones que necesitemos en cada dataset (CREMA-D y SAVEE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97450846",
   "metadata": {},
   "source": [
    "#### Funciones para obtener los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f7c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtiene todos los datos de CREMA-D y SAVEE para un  \n",
    "def get_datas():\n",
    "    files_path = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\..\\\\Datasets\\\\AudioWav\")\n",
    "    emotions_code = [\"NEU\", \"FEA\",\"ANG\"]\n",
    "    datas_files = extract_paths_for_emotions_keys(emotions_code, files_path, get_code_crema_d)\n",
    "    files_path_s = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\..\\\\Datasets\\\\ALL\")\n",
    "    emotions_code_s = [\"a\", \"f\",\"n\"]\n",
    "    datas_files_s = extract_paths_for_emotions_keys(emotions_code_s, files_path_s, get_code_savee) \n",
    "    examples = []\n",
    "    files_filters = dict()\n",
    "    files_filters[\"NEU\"] = 0\n",
    "    files_filters[\"FEA\"] = 0\n",
    "    files_filters[\"ANG\"] = 0\n",
    "    files_filters[\"a\"] = 0\n",
    "    files_filters[\"f\"] = 0\n",
    "    files_filters[\"n\"] = 0\n",
    "    examples = get_features(datas_files, get_code_crema_d, files_filters)\n",
    "    examples_s = get_features(datas_files_s, get_code_savee, files_filters)\n",
    "    return examples + examples_s\n",
    "\n",
    "#Obtiene los datos de entrada para la red neuronal x: mfcc normalizados, y: labeles categorizados\n",
    "def get_entries():\n",
    "    all_examples = get_datas()\n",
    "    entries = []\n",
    "    for example in all_examples:\n",
    "        entries.append((example[0], example[1]))\n",
    "    '''\n",
    "    datas = select_elements(entries, 'NEU', 896,\"without_stress\")\n",
    "    datas += select_elements(entries, 'ANG', 550, \"stress\")\n",
    "    datas += select_elements(entries, 'FEA', 550, \"stress\")\n",
    "    datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "    datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "    datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "    '''\n",
    "    '''\n",
    "    datas = select_elements(entries, 'NEU', 1087,\"without_stress\")\n",
    "    datas += select_elements(entries, 'ANG', 1271, \"stress\")\n",
    "    datas += select_elements(entries, 'FEA', 1270, \"stress\")\n",
    "    datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "    datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "    datas += select_elements(entries, 'n', 120, \"without_stress\")  \n",
    "    '''\n",
    "    datas = select_elements(entries, 'NEU', 1086,\"without_stress\")\n",
    "    datas += select_elements(entries, 'ANG', 543, \"stress\")\n",
    "    datas += select_elements(entries, 'FEA', 543, \"stress\")\n",
    "    datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "    datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "    datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "    random.shuffle(datas)\n",
    "    X = []\n",
    "    y = []\n",
    "    for data in datas:\n",
    "        X.append(data[0])\n",
    "        y.append(data[1])\n",
    "    labelencoder=preprocessing.LabelEncoder()\n",
    "    y = to_categorical(labelencoder.fit_transform(y))\n",
    "    return X, y\n",
    "\n",
    "#Obtiene los datos divididos de entrenamiento y tests\n",
    "def obtain_datas_train_and_test(percentage):\n",
    "    X, y = get_entries()\n",
    "    X_test, X_train, y_test, y_train = train_test_split(X, y, test_size =percentage,random_state=0)\n",
    "    y = np.array(y)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff362da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtuvo el MFCC de unos 3629 sobre 3628 audios.\n",
      "Se obtuvo el MFCC de unos 240 sobre 3868 audios.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = obtain_datas_train_and_test(0.8)\n",
    "num_labels = y_train.shape[1] + y_test.shape[1]\n",
    "dim_entrada = (X_train.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa4a7128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1930, 40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc211300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCross-validated score (Accuracy score): 0.7929338103756708\\n-----------------------\\nResumen\\nFold score (Accuracy score): 0.7901785714285714\\nFold score (Accuracy score): 0.8277404921700223\\nFold score (Accuracy score): 0.7897091722595079\\nFold score (Accuracy score): 0.7472035794183445\\nFold score (Accuracy score): 0.8098434004474273\\n2\\n#https://keras.io/api/callbacks/early_stopping/\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#funciones de redes neuronales\n",
    "def previous_RNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def new_RNN_a():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7987477638640429\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.8102678571428571\n",
    "Fold score (Accuracy score): 0.7516778523489933\n",
    "Fold score (Accuracy score): 0.7740492170022372\n",
    "Fold score (Accuracy score): 0.8232662192393736\n",
    "Fold score (Accuracy score): 0.8344519015659956\n",
    "'''\n",
    "def new_RNN_b():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.8090339892665475\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.8214285714285714\n",
    "Fold score (Accuracy score): 0.785234899328859\n",
    "Fold score (Accuracy score): 0.8008948545861297\n",
    "Fold score (Accuracy score): 0.8098434004474273\n",
    "Fold score (Accuracy score): 0.8277404921700223\n",
    "'''\n",
    "\n",
    "def new_RNN_c():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7754919499105546\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7879464285714286\n",
    "Fold score (Accuracy score): 0.7539149888143176\n",
    "Fold score (Accuracy score): 0.7718120805369127\n",
    "Fold score (Accuracy score): 0.7628635346756152\n",
    "Fold score (Accuracy score): 0.8008948545861297\n",
    "'''\n",
    "\n",
    "def double_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7920393559928444\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.78125\n",
    "Fold score (Accuracy score): 0.767337807606264\n",
    "Fold score (Accuracy score): 0.8008948545861297\n",
    "Fold score (Accuracy score): 0.7941834451901566\n",
    "Fold score (Accuracy score): 0.8165548098434005\n",
    "'''\n",
    "\n",
    "#fail tarda mucho\n",
    "def triple_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7991949910554562\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7991071428571429\n",
    "Fold score (Accuracy score): 0.785234899328859\n",
    "Fold score (Accuracy score): 0.814317673378076\n",
    "Fold score (Accuracy score): 0.7762863534675615\n",
    "Fold score (Accuracy score): 0.8210290827740492\n",
    "'''\n",
    "def dense_Layer():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7844364937388193\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7589285714285714\n",
    "Fold score (Accuracy score): 0.7740492170022372\n",
    "Fold score (Accuracy score): 0.796420581655481\n",
    "Fold score (Accuracy score): 0.785234899328859\n",
    "Fold score (Accuracy score): 0.8076062639821029\n",
    "'''\n",
    "\n",
    "def possible_RNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7898032200357782\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7834821428571429\n",
    "Fold score (Accuracy score): 0.8299776286353467\n",
    "Fold score (Accuracy score): 0.767337807606264\n",
    "Fold score (Accuracy score): 0.7472035794183445\n",
    "Fold score (Accuracy score): 0.8210290827740492\n",
    "'''\n",
    "def activation_RNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.802772808586762\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.8080357142857143\n",
    "Fold score (Accuracy score): 0.8076062639821029\n",
    "Fold score (Accuracy score): 0.8008948545861297\n",
    "Fold score (Accuracy score): 0.767337807606264\n",
    "Fold score (Accuracy score): 0.8299776286353467\n",
    "'''\n",
    "def double_dropout():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.8019136281355056\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7945736434108527\n",
    "Fold score (Accuracy score): 0.8036175710594315\n",
    "Fold score (Accuracy score): 0.7800776196636481\n",
    "Fold score (Accuracy score): 0.815006468305304\n",
    "Fold score (Accuracy score): 0.8163001293661061\n",
    "'''\n",
    "def double_dropout_activation():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7944142746314973\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7622739018087855\n",
    "Fold score (Accuracy score): 0.810077519379845\n",
    "Fold score (Accuracy score): 0.7865459249676585\n",
    "Fold score (Accuracy score): 0.8020698576972833\n",
    "Fold score (Accuracy score): 0.8111254851228978\n",
    "'''\n",
    "def double_RNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(LSTM(units=25,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.8057926040858546\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7958656330749354\n",
    "Fold score (Accuracy score): 0.789405684754522\n",
    "Fold score (Accuracy score): 0.7878395860284605\n",
    "Fold score (Accuracy score): 0.8279430789133247\n",
    "Fold score (Accuracy score): 0.8279430789133247\n",
    "'''\n",
    "\n",
    "def basic():\n",
    "    dropout = 0.5\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50,input_shape= dim_entrada))\n",
    "    model.add(Dense(2,activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.8047582104990949\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7984496124031008\n",
    "Fold score (Accuracy score): 0.8126614987080103\n",
    "Fold score (Accuracy score): 0.7813712807244502\n",
    "Fold score (Accuracy score): 0.8227684346701164\n",
    "Fold score (Accuracy score): 0.8085381630012937\n",
    "\n",
    "'''\n",
    "\n",
    "def new_RNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(units=20,input_shape= dim_entrada, return_sequences=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(units=10,input_shape= dim_entrada, return_sequences=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return model\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=20000, \n",
    "                           output_dim=100, \n",
    "                           input_length=300))\n",
    "model.add(layers.Bidirectional(layers.LSTM(100, dropout=0.5, \n",
    "                                           recurrent_dropout=0.5, \n",
    "                                           return_sequences=True)))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "'''\n",
    "'''\n",
    "Cross-validated score (Accuracy score): 0.7929338103756708\n",
    "-----------------------\n",
    "Resumen\n",
    "Fold score (Accuracy score): 0.7901785714285714\n",
    "Fold score (Accuracy score): 0.8277404921700223\n",
    "Fold score (Accuracy score): 0.7897091722595079\n",
    "Fold score (Accuracy score): 0.7472035794183445\n",
    "Fold score (Accuracy score): 0.8098434004474273\n",
    "2\n",
    "#https://keras.io/api/callbacks/early_stopping/\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111940c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 40, 50)            10400     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 40, 50)            0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40, 50)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 40, 20)            5680      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 40, 20)            0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 40, 20)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10)                1240      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,342\n",
      "Trainable params: 17,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = new_RNN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "941cf153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train - X:(1929, 40) y:(1929, 2)\n",
      "Test - X:(483, 40) y:(483, 2)\n",
      "Epoch 1/200\n",
      "61/61 [==============================] - 37s 108ms/step - loss: 0.6853 - accuracy: 0.5397 - val_loss: 0.6624 - val_accuracy: 0.6398\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 5s 74ms/step - loss: 0.6316 - accuracy: 0.6376 - val_loss: 0.5840 - val_accuracy: 0.6915\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.6124 - accuracy: 0.6672 - val_loss: 0.5734 - val_accuracy: 0.7101\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.6126 - accuracy: 0.6563 - val_loss: 0.5694 - val_accuracy: 0.7205\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.5872 - accuracy: 0.7055 - val_loss: 0.5467 - val_accuracy: 0.7143\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 4s 74ms/step - loss: 0.5752 - accuracy: 0.6890 - val_loss: 0.5412 - val_accuracy: 0.7433\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.5791 - accuracy: 0.7133 - val_loss: 0.5357 - val_accuracy: 0.7350\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5675 - accuracy: 0.7061 - val_loss: 0.5341 - val_accuracy: 0.7246\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.5503 - accuracy: 0.7185 - val_loss: 0.5734 - val_accuracy: 0.7246\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.5532 - accuracy: 0.7149 - val_loss: 0.5504 - val_accuracy: 0.7164\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.5437 - accuracy: 0.7284 - val_loss: 0.5110 - val_accuracy: 0.7578\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.5370 - accuracy: 0.7335 - val_loss: 0.5038 - val_accuracy: 0.7578\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.5469 - accuracy: 0.7315 - val_loss: 0.4927 - val_accuracy: 0.7660\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.5363 - accuracy: 0.7377 - val_loss: 0.4954 - val_accuracy: 0.7660\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.5399 - accuracy: 0.7309 - val_loss: 0.4954 - val_accuracy: 0.7598\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.5274 - accuracy: 0.7341 - val_loss: 0.5069 - val_accuracy: 0.7350\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5269 - accuracy: 0.7356 - val_loss: 0.4779 - val_accuracy: 0.7702\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.5145 - accuracy: 0.7465 - val_loss: 0.4726 - val_accuracy: 0.7743\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.5092 - accuracy: 0.7439 - val_loss: 0.4899 - val_accuracy: 0.7640\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.5123 - accuracy: 0.7491 - val_loss: 0.5206 - val_accuracy: 0.7619\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.5149 - accuracy: 0.7449 - val_loss: 0.4826 - val_accuracy: 0.7702\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.5265 - accuracy: 0.7475 - val_loss: 0.5010 - val_accuracy: 0.7453\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.5143 - accuracy: 0.7501 - val_loss: 0.4948 - val_accuracy: 0.7391\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.5025 - accuracy: 0.7455 - val_loss: 0.4658 - val_accuracy: 0.7909\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5032 - accuracy: 0.7610 - val_loss: 0.4578 - val_accuracy: 0.7764\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4971 - accuracy: 0.7486 - val_loss: 0.4543 - val_accuracy: 0.7909\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.4998 - accuracy: 0.7646 - val_loss: 0.4606 - val_accuracy: 0.7909\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4970 - accuracy: 0.7543 - val_loss: 0.5057 - val_accuracy: 0.7805\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4902 - accuracy: 0.7641 - val_loss: 0.5607 - val_accuracy: 0.7640\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.4969 - accuracy: 0.7589 - val_loss: 0.4549 - val_accuracy: 0.7847\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4857 - accuracy: 0.7615 - val_loss: 0.4507 - val_accuracy: 0.7950\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4803 - accuracy: 0.7626 - val_loss: 0.4682 - val_accuracy: 0.7867\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.5007 - accuracy: 0.7564 - val_loss: 0.4897 - val_accuracy: 0.7681\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.4784 - accuracy: 0.7600 - val_loss: 0.4375 - val_accuracy: 0.7909\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4805 - accuracy: 0.7719 - val_loss: 0.4377 - val_accuracy: 0.8116\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4695 - accuracy: 0.7740 - val_loss: 0.4595 - val_accuracy: 0.7536\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4636 - accuracy: 0.7683 - val_loss: 0.4490 - val_accuracy: 0.7785\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4756 - accuracy: 0.7646 - val_loss: 0.4328 - val_accuracy: 0.8012\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4659 - accuracy: 0.7662 - val_loss: 0.4122 - val_accuracy: 0.7992\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4638 - accuracy: 0.7745 - val_loss: 0.4166 - val_accuracy: 0.8116\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4554 - accuracy: 0.7880 - val_loss: 0.4238 - val_accuracy: 0.7867\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4587 - accuracy: 0.7792 - val_loss: 0.4054 - val_accuracy: 0.8095\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4701 - accuracy: 0.7693 - val_loss: 0.4221 - val_accuracy: 0.7992\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4463 - accuracy: 0.7802 - val_loss: 0.5000 - val_accuracy: 0.8178\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4658 - accuracy: 0.7760 - val_loss: 0.3928 - val_accuracy: 0.8116\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4695 - accuracy: 0.7688 - val_loss: 0.4190 - val_accuracy: 0.8033\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4553 - accuracy: 0.7797 - val_loss: 0.4231 - val_accuracy: 0.8157\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4308 - accuracy: 0.7921 - val_loss: 0.4123 - val_accuracy: 0.7909\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4756 - accuracy: 0.7698 - val_loss: 0.4266 - val_accuracy: 0.8012\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4628 - accuracy: 0.7719 - val_loss: 0.4225 - val_accuracy: 0.7971\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.4367 - accuracy: 0.7890 - val_loss: 0.4150 - val_accuracy: 0.7971\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4481 - accuracy: 0.7869 - val_loss: 0.4678 - val_accuracy: 0.7371\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4521 - accuracy: 0.7781 - val_loss: 0.4140 - val_accuracy: 0.7971\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4414 - accuracy: 0.7926 - val_loss: 0.4278 - val_accuracy: 0.7805\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4402 - accuracy: 0.7890 - val_loss: 0.4286 - val_accuracy: 0.7847\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4377 - accuracy: 0.7916 - val_loss: 0.3956 - val_accuracy: 0.8364\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4279 - accuracy: 0.7911 - val_loss: 0.4122 - val_accuracy: 0.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4342 - accuracy: 0.7911 - val_loss: 0.3776 - val_accuracy: 0.8219\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4318 - accuracy: 0.7957 - val_loss: 0.3829 - val_accuracy: 0.8137\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4384 - accuracy: 0.7900 - val_loss: 0.3935 - val_accuracy: 0.8157\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4224 - accuracy: 0.8004 - val_loss: 0.3952 - val_accuracy: 0.8302\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4476 - accuracy: 0.7921 - val_loss: 0.4042 - val_accuracy: 0.8178\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4243 - accuracy: 0.7983 - val_loss: 0.4574 - val_accuracy: 0.7764\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4375 - accuracy: 0.7942 - val_loss: 0.4212 - val_accuracy: 0.8219\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4411 - accuracy: 0.7968 - val_loss: 0.4309 - val_accuracy: 0.8157\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4601 - accuracy: 0.7750 - val_loss: 0.4127 - val_accuracy: 0.8012\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4296 - accuracy: 0.7864 - val_loss: 0.4002 - val_accuracy: 0.8054\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4296 - accuracy: 0.7952 - val_loss: 0.3971 - val_accuracy: 0.8178\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4215 - accuracy: 0.7957 - val_loss: 0.4293 - val_accuracy: 0.7805\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4433 - accuracy: 0.7828 - val_loss: 0.5151 - val_accuracy: 0.7019\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4456 - accuracy: 0.7792 - val_loss: 0.4140 - val_accuracy: 0.8157\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4110 - accuracy: 0.7994 - val_loss: 0.4137 - val_accuracy: 0.7950\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4123 - accuracy: 0.7989 - val_loss: 0.4007 - val_accuracy: 0.8075\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.4313 - accuracy: 0.7906 - val_loss: 0.4200 - val_accuracy: 0.8282\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4421 - accuracy: 0.7869 - val_loss: 0.4185 - val_accuracy: 0.8219\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4139 - accuracy: 0.7994 - val_loss: 0.3979 - val_accuracy: 0.8075\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4050 - accuracy: 0.8113 - val_loss: 0.4100 - val_accuracy: 0.8137\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4064 - accuracy: 0.8061 - val_loss: 0.4003 - val_accuracy: 0.8178\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4317 - accuracy: 0.7916 - val_loss: 0.4691 - val_accuracy: 0.8116\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.4009 - accuracy: 0.8129 - val_loss: 0.4064 - val_accuracy: 0.8219\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 4s 63ms/step - loss: 0.4079 - accuracy: 0.8154 - val_loss: 0.4442 - val_accuracy: 0.8178\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4353 - accuracy: 0.7973 - val_loss: 0.3974 - val_accuracy: 0.8219\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.4001 - accuracy: 0.8129 - val_loss: 0.4170 - val_accuracy: 0.8344\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.4037 - accuracy: 0.8144 - val_loss: 0.4054 - val_accuracy: 0.8364\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.3986 - accuracy: 0.8087 - val_loss: 0.3871 - val_accuracy: 0.8364\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.3994 - accuracy: 0.8243 - val_loss: 0.3815 - val_accuracy: 0.8344\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.3847 - accuracy: 0.8300 - val_loss: 0.3906 - val_accuracy: 0.8157\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.3855 - accuracy: 0.8269 - val_loss: 0.4010 - val_accuracy: 0.8178\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.3972 - accuracy: 0.8160 - val_loss: 0.3856 - val_accuracy: 0.8364\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.4100 - accuracy: 0.8097 - val_loss: 0.4251 - val_accuracy: 0.8095\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.4099 - accuracy: 0.8061 - val_loss: 0.4119 - val_accuracy: 0.8054\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3853 - accuracy: 0.8284 - val_loss: 0.3946 - val_accuracy: 0.8302\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.3768 - accuracy: 0.8243 - val_loss: 0.3951 - val_accuracy: 0.8219\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.3778 - accuracy: 0.8269 - val_loss: 0.4249 - val_accuracy: 0.8178\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.4063 - accuracy: 0.8154 - val_loss: 0.3888 - val_accuracy: 0.8406\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.3840 - accuracy: 0.8269 - val_loss: 0.3788 - val_accuracy: 0.8364\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.3945 - accuracy: 0.8129 - val_loss: 0.4170 - val_accuracy: 0.8323\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.3827 - accuracy: 0.8263 - val_loss: 0.4110 - val_accuracy: 0.8302\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.3663 - accuracy: 0.8362 - val_loss: 0.4265 - val_accuracy: 0.7888\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.3706 - accuracy: 0.8310 - val_loss: 0.3825 - val_accuracy: 0.8323\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.3743 - accuracy: 0.8315 - val_loss: 0.4087 - val_accuracy: 0.8323\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.3905 - accuracy: 0.8118 - val_loss: 0.3865 - val_accuracy: 0.8157\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 4s 71ms/step - loss: 0.3730 - accuracy: 0.8336 - val_loss: 0.4031 - val_accuracy: 0.8385\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.3715 - accuracy: 0.8372 - val_loss: 0.4217 - val_accuracy: 0.8054\n",
      "Epoch 105/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.3818 - accuracy: 0.8248 - val_loss: 0.4215 - val_accuracy: 0.8344\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 5s 74ms/step - loss: 0.4420 - accuracy: 0.7766 - val_loss: 0.4388 - val_accuracy: 0.7971\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 4s 67ms/step - loss: 0.4342 - accuracy: 0.7859 - val_loss: 0.4098 - val_accuracy: 0.8075\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.4024 - accuracy: 0.8077 - val_loss: 0.4372 - val_accuracy: 0.8075\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.3804 - accuracy: 0.8279 - val_loss: 0.4149 - val_accuracy: 0.8385\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - 4s 69ms/step - loss: 0.3662 - accuracy: 0.8289 - val_loss: 0.4088 - val_accuracy: 0.8385\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 4s 69ms/step - loss: 0.3660 - accuracy: 0.8393 - val_loss: 0.4286 - val_accuracy: 0.7992\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 4s 72ms/step - loss: 0.3840 - accuracy: 0.8227 - val_loss: 0.5071 - val_accuracy: 0.8033\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.3735 - accuracy: 0.8279 - val_loss: 0.4501 - val_accuracy: 0.8344\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3722 - accuracy: 0.8341 - val_loss: 0.4865 - val_accuracy: 0.8261\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.3630 - accuracy: 0.8383 - val_loss: 0.4073 - val_accuracy: 0.8385\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.3443 - accuracy: 0.8393 - val_loss: 0.4200 - val_accuracy: 0.8406\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3664 - accuracy: 0.8419 - val_loss: 0.4493 - val_accuracy: 0.7992\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.3995 - accuracy: 0.8087 - val_loss: 0.4032 - val_accuracy: 0.8364\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3661 - accuracy: 0.8294 - val_loss: 0.4389 - val_accuracy: 0.8116\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3725 - accuracy: 0.8300 - val_loss: 0.4186 - val_accuracy: 0.8427\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3471 - accuracy: 0.8424 - val_loss: 0.4300 - val_accuracy: 0.8447\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.3688 - accuracy: 0.8429 - val_loss: 0.4039 - val_accuracy: 0.8323\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3465 - accuracy: 0.8476 - val_loss: 0.4453 - val_accuracy: 0.7764\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4123 - accuracy: 0.7994 - val_loss: 0.4013 - val_accuracy: 0.8427\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4528 - accuracy: 0.7818 - val_loss: 0.4365 - val_accuracy: 0.8157\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3812 - accuracy: 0.8331 - val_loss: 0.4701 - val_accuracy: 0.8116\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3434 - accuracy: 0.8497 - val_loss: 0.5351 - val_accuracy: 0.8468\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.3601 - accuracy: 0.8393 - val_loss: 0.4149 - val_accuracy: 0.8240\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3592 - accuracy: 0.8398 - val_loss: 0.4095 - val_accuracy: 0.8054\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3476 - accuracy: 0.8424 - val_loss: 0.4721 - val_accuracy: 0.8571\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3578 - accuracy: 0.8346 - val_loss: 0.4082 - val_accuracy: 0.8282\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.3259 - accuracy: 0.8548 - val_loss: 0.5171 - val_accuracy: 0.8282\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3301 - accuracy: 0.8559 - val_loss: 0.4485 - val_accuracy: 0.8509\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3266 - accuracy: 0.8533 - val_loss: 0.4155 - val_accuracy: 0.8364\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3260 - accuracy: 0.8548 - val_loss: 0.4045 - val_accuracy: 0.8406\n",
      "Epoch 136/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3131 - accuracy: 0.8683 - val_loss: 0.4645 - val_accuracy: 0.8427\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3392 - accuracy: 0.8512 - val_loss: 0.4746 - val_accuracy: 0.8447\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3304 - accuracy: 0.8528 - val_loss: 0.4187 - val_accuracy: 0.8219\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3640 - accuracy: 0.8269 - val_loss: 0.4222 - val_accuracy: 0.8157\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3831 - accuracy: 0.8186 - val_loss: 0.3905 - val_accuracy: 0.8302\n",
      "Epoch 141/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3283 - accuracy: 0.8554 - val_loss: 0.4580 - val_accuracy: 0.8468\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3163 - accuracy: 0.8569 - val_loss: 0.5398 - val_accuracy: 0.8406\n",
      "Epoch 143/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.3065 - accuracy: 0.8657 - val_loss: 0.4346 - val_accuracy: 0.8447\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.3129 - accuracy: 0.8704 - val_loss: 0.4414 - val_accuracy: 0.8178\n",
      "Epoch 145/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.3190 - accuracy: 0.8626 - val_loss: 0.5213 - val_accuracy: 0.8323\n",
      "Epoch 146/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.3148 - accuracy: 0.8616 - val_loss: 0.5882 - val_accuracy: 0.8240\n",
      "Epoch 147/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.2957 - accuracy: 0.8745 - val_loss: 0.4814 - val_accuracy: 0.8385\n",
      "Epoch 148/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3345 - accuracy: 0.8528 - val_loss: 0.4878 - val_accuracy: 0.8427\n",
      "Epoch 149/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3000 - accuracy: 0.8668 - val_loss: 0.4515 - val_accuracy: 0.8157\n",
      "Epoch 150/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3582 - accuracy: 0.8419 - val_loss: 0.4221 - val_accuracy: 0.8199\n",
      "Epoch 151/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.3124 - accuracy: 0.8652 - val_loss: 0.4554 - val_accuracy: 0.8489\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4052 - accuracy: 0.8040 - val_loss: 0.4397 - val_accuracy: 0.8178\n",
      "Epoch 153/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3281 - accuracy: 0.8590 - val_loss: 0.4422 - val_accuracy: 0.8385\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3171 - accuracy: 0.8605 - val_loss: 0.4244 - val_accuracy: 0.8178\n",
      "Epoch 155/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3088 - accuracy: 0.8673 - val_loss: 0.4764 - val_accuracy: 0.8468\n",
      "Epoch 156/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3126 - accuracy: 0.8683 - val_loss: 0.4133 - val_accuracy: 0.8282\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.3134 - accuracy: 0.8657 - val_loss: 0.5415 - val_accuracy: 0.8468\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3005 - accuracy: 0.8740 - val_loss: 0.4622 - val_accuracy: 0.8447\n",
      "Epoch 159/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.2988 - accuracy: 0.8704 - val_loss: 0.4157 - val_accuracy: 0.8447\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.2977 - accuracy: 0.8740 - val_loss: 0.5354 - val_accuracy: 0.8509\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.2904 - accuracy: 0.8756 - val_loss: 0.5731 - val_accuracy: 0.8364\n",
      "Epoch 162/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3045 - accuracy: 0.8652 - val_loss: 0.5549 - val_accuracy: 0.8406\n",
      "Epoch 163/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.2793 - accuracy: 0.8818 - val_loss: 0.5081 - val_accuracy: 0.8427\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3072 - accuracy: 0.8699 - val_loss: 0.4545 - val_accuracy: 0.8427\n",
      "Epoch 165/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.2927 - accuracy: 0.8735 - val_loss: 0.5341 - val_accuracy: 0.8364\n",
      "Epoch 166/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.2675 - accuracy: 0.8849 - val_loss: 0.7070 - val_accuracy: 0.8344\n",
      "Epoch 167/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3038 - accuracy: 0.8683 - val_loss: 0.5733 - val_accuracy: 0.8137\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3031 - accuracy: 0.8652 - val_loss: 0.4982 - val_accuracy: 0.8302\n",
      "Epoch 169/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.2869 - accuracy: 0.8802 - val_loss: 0.5549 - val_accuracy: 0.8427\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3333 - accuracy: 0.8523 - val_loss: 0.4566 - val_accuracy: 0.8530\n",
      "Epoch 171/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.2787 - accuracy: 0.8813 - val_loss: 0.6014 - val_accuracy: 0.8323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.2736 - accuracy: 0.8849 - val_loss: 0.6518 - val_accuracy: 0.8509\n",
      "Epoch 173/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.2859 - accuracy: 0.8792 - val_loss: 0.4700 - val_accuracy: 0.8261\n",
      "Epoch 174/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2847 - accuracy: 0.8740 - val_loss: 0.7866 - val_accuracy: 0.8427\n",
      "Epoch 175/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3100 - accuracy: 0.8647 - val_loss: 0.5039 - val_accuracy: 0.7971\n",
      "Epoch 176/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.3145 - accuracy: 0.8605 - val_loss: 0.6609 - val_accuracy: 0.8406\n",
      "Epoch 177/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.2854 - accuracy: 0.8792 - val_loss: 0.5090 - val_accuracy: 0.8406\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2668 - accuracy: 0.8917 - val_loss: 0.5804 - val_accuracy: 0.8447\n",
      "Epoch 179/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.2716 - accuracy: 0.8865 - val_loss: 0.5014 - val_accuracy: 0.8468\n",
      "Epoch 180/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2551 - accuracy: 0.8927 - val_loss: 0.6169 - val_accuracy: 0.8468\n",
      "Epoch 181/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2669 - accuracy: 0.8880 - val_loss: 0.5548 - val_accuracy: 0.8240\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2737 - accuracy: 0.8870 - val_loss: 0.4305 - val_accuracy: 0.8406\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.2775 - accuracy: 0.8787 - val_loss: 0.6985 - val_accuracy: 0.8282\n",
      "Epoch 184/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.2808 - accuracy: 0.8891 - val_loss: 0.5384 - val_accuracy: 0.8385\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.2593 - accuracy: 0.8932 - val_loss: 0.4925 - val_accuracy: 0.8427\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.2430 - accuracy: 0.8948 - val_loss: 0.5171 - val_accuracy: 0.8489\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2352 - accuracy: 0.9010 - val_loss: 0.5653 - val_accuracy: 0.8551\n",
      "Epoch 188/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.2363 - accuracy: 0.9046 - val_loss: 0.8117 - val_accuracy: 0.8447\n",
      "Epoch 189/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.2636 - accuracy: 0.8901 - val_loss: 0.5968 - val_accuracy: 0.8323\n",
      "Epoch 190/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.2729 - accuracy: 0.8937 - val_loss: 0.6174 - val_accuracy: 0.8385\n",
      "Epoch 191/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.2421 - accuracy: 0.8953 - val_loss: 0.6427 - val_accuracy: 0.8489\n",
      "Epoch 192/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.2356 - accuracy: 0.9041 - val_loss: 0.6315 - val_accuracy: 0.8447\n",
      "Epoch 193/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.2382 - accuracy: 0.9046 - val_loss: 0.6527 - val_accuracy: 0.8509\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2861 - accuracy: 0.8751 - val_loss: 0.5229 - val_accuracy: 0.8157\n",
      "Epoch 195/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.2652 - accuracy: 0.8834 - val_loss: 0.6023 - val_accuracy: 0.8385\n",
      "Epoch 196/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.2575 - accuracy: 0.8963 - val_loss: 0.7600 - val_accuracy: 0.8323\n",
      "Epoch 197/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.2521 - accuracy: 0.8917 - val_loss: 0.6391 - val_accuracy: 0.8344\n",
      "Epoch 198/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.2398 - accuracy: 0.9031 - val_loss: 0.8245 - val_accuracy: 0.8199\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2265 - accuracy: 0.9031 - val_loss: 0.5805 - val_accuracy: 0.8364\n",
      "Epoch 200/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.2161 - accuracy: 0.9124 - val_loss: 0.7545 - val_accuracy: 0.8282\n",
      "16/16 [==============================] - 4s 24ms/step\n",
      "Fold score (Accuracy score): 0.8281573498964804\n",
      "Matriz de confusion\n",
      "-------------------\n",
      "---------| Verdadero | Falso |\n",
      "Verdadero|  200        40\n",
      "Falso    |  43        200\n",
      "Fold #2\n",
      "Train - X:(1929, 40) y:(1929, 2)\n",
      "Test - X:(483, 40) y:(483, 2)\n",
      "Epoch 1/200\n",
      "61/61 [==============================] - 23s 145ms/step - loss: 0.6858 - accuracy: 0.5490 - val_loss: 0.6680 - val_accuracy: 0.5942\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.6398 - accuracy: 0.6480 - val_loss: 0.6041 - val_accuracy: 0.6729\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.6111 - accuracy: 0.6744 - val_loss: 0.6123 - val_accuracy: 0.6770\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5837 - accuracy: 0.7138 - val_loss: 0.5754 - val_accuracy: 0.7039\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.5835 - accuracy: 0.6978 - val_loss: 0.5636 - val_accuracy: 0.7039\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.5613 - accuracy: 0.7211 - val_loss: 0.5482 - val_accuracy: 0.7184\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.5540 - accuracy: 0.7180 - val_loss: 0.5499 - val_accuracy: 0.7101\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.5634 - accuracy: 0.7102 - val_loss: 0.5551 - val_accuracy: 0.6936\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5544 - accuracy: 0.7263 - val_loss: 0.5466 - val_accuracy: 0.7308\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.5455 - accuracy: 0.7195 - val_loss: 0.5344 - val_accuracy: 0.7226\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5454 - accuracy: 0.7180 - val_loss: 0.5333 - val_accuracy: 0.7371\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.5266 - accuracy: 0.7335 - val_loss: 0.5416 - val_accuracy: 0.7267\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.5233 - accuracy: 0.7398 - val_loss: 0.5372 - val_accuracy: 0.7246\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.5282 - accuracy: 0.7330 - val_loss: 0.5369 - val_accuracy: 0.7246\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.5230 - accuracy: 0.7475 - val_loss: 0.5224 - val_accuracy: 0.7391\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5172 - accuracy: 0.7382 - val_loss: 0.5374 - val_accuracy: 0.7184\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.5149 - accuracy: 0.7403 - val_loss: 0.5247 - val_accuracy: 0.7391\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5170 - accuracy: 0.7403 - val_loss: 0.5182 - val_accuracy: 0.7350\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5161 - accuracy: 0.7382 - val_loss: 0.5270 - val_accuracy: 0.7267\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5129 - accuracy: 0.7413 - val_loss: 0.5079 - val_accuracy: 0.7536\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.5378 - accuracy: 0.7278 - val_loss: 0.5394 - val_accuracy: 0.7184\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.5131 - accuracy: 0.7449 - val_loss: 0.5314 - val_accuracy: 0.7226\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.5016 - accuracy: 0.7444 - val_loss: 0.5132 - val_accuracy: 0.7433\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.5043 - accuracy: 0.7455 - val_loss: 0.5589 - val_accuracy: 0.6977\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4926 - accuracy: 0.7470 - val_loss: 0.5072 - val_accuracy: 0.7433\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4980 - accuracy: 0.7548 - val_loss: 0.5032 - val_accuracy: 0.7412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5050 - accuracy: 0.7470 - val_loss: 0.5091 - val_accuracy: 0.7391\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.4809 - accuracy: 0.7564 - val_loss: 0.4874 - val_accuracy: 0.7516\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4735 - accuracy: 0.7760 - val_loss: 0.5003 - val_accuracy: 0.7557\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4756 - accuracy: 0.7698 - val_loss: 0.4923 - val_accuracy: 0.7433\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4783 - accuracy: 0.7646 - val_loss: 0.4960 - val_accuracy: 0.7453\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.4949 - accuracy: 0.7569 - val_loss: 0.4793 - val_accuracy: 0.7640\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.4664 - accuracy: 0.7895 - val_loss: 0.4885 - val_accuracy: 0.7433\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5281 - accuracy: 0.7263 - val_loss: 0.5522 - val_accuracy: 0.7184\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4984 - accuracy: 0.7486 - val_loss: 0.5156 - val_accuracy: 0.7308\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.4887 - accuracy: 0.7558 - val_loss: 0.5240 - val_accuracy: 0.7288\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4784 - accuracy: 0.7631 - val_loss: 0.4974 - val_accuracy: 0.7350\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4779 - accuracy: 0.7652 - val_loss: 0.4974 - val_accuracy: 0.7329\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4627 - accuracy: 0.7724 - val_loss: 0.4948 - val_accuracy: 0.7329\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4724 - accuracy: 0.7652 - val_loss: 0.4904 - val_accuracy: 0.7495\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4532 - accuracy: 0.7854 - val_loss: 0.4783 - val_accuracy: 0.7805\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4495 - accuracy: 0.7760 - val_loss: 0.4757 - val_accuracy: 0.7785\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.4625 - accuracy: 0.7678 - val_loss: 0.5221 - val_accuracy: 0.7246\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.4522 - accuracy: 0.7833 - val_loss: 0.4997 - val_accuracy: 0.7516\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4378 - accuracy: 0.7854 - val_loss: 0.4694 - val_accuracy: 0.7640\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.4408 - accuracy: 0.7900 - val_loss: 0.4794 - val_accuracy: 0.7536\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4411 - accuracy: 0.7859 - val_loss: 0.4832 - val_accuracy: 0.7433\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4346 - accuracy: 0.7952 - val_loss: 0.4939 - val_accuracy: 0.7702\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.4479 - accuracy: 0.7864 - val_loss: 0.4584 - val_accuracy: 0.7702\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 6s 107ms/step - loss: 0.4316 - accuracy: 0.7916 - val_loss: 0.4753 - val_accuracy: 0.7640\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.4234 - accuracy: 0.7999 - val_loss: 0.5050 - val_accuracy: 0.7329\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4479 - accuracy: 0.7818 - val_loss: 0.4679 - val_accuracy: 0.7702\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4242 - accuracy: 0.8040 - val_loss: 0.4560 - val_accuracy: 0.7743\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.4181 - accuracy: 0.8030 - val_loss: 0.4471 - val_accuracy: 0.7805\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.4327 - accuracy: 0.7947 - val_loss: 0.4420 - val_accuracy: 0.7909\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4487 - accuracy: 0.7818 - val_loss: 0.4659 - val_accuracy: 0.7619\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4400 - accuracy: 0.7792 - val_loss: 0.4583 - val_accuracy: 0.7826\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.4190 - accuracy: 0.8072 - val_loss: 0.5042 - val_accuracy: 0.7723\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.4220 - accuracy: 0.8061 - val_loss: 0.4615 - val_accuracy: 0.7785\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4023 - accuracy: 0.8092 - val_loss: 0.4620 - val_accuracy: 0.7805\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4001 - accuracy: 0.8056 - val_loss: 0.4383 - val_accuracy: 0.7930\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 6s 104ms/step - loss: 0.4224 - accuracy: 0.7983 - val_loss: 0.4404 - val_accuracy: 0.7930\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4010 - accuracy: 0.8206 - val_loss: 0.4460 - val_accuracy: 0.7785\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.4004 - accuracy: 0.8103 - val_loss: 0.5189 - val_accuracy: 0.7371\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3979 - accuracy: 0.8154 - val_loss: 0.4386 - val_accuracy: 0.7909\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4039 - accuracy: 0.8077 - val_loss: 0.4560 - val_accuracy: 0.7909\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3913 - accuracy: 0.8108 - val_loss: 0.4652 - val_accuracy: 0.7867\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3859 - accuracy: 0.8154 - val_loss: 0.4371 - val_accuracy: 0.8012\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3856 - accuracy: 0.8217 - val_loss: 0.4521 - val_accuracy: 0.7950\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3996 - accuracy: 0.8191 - val_loss: 0.4427 - val_accuracy: 0.7847\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.4004 - accuracy: 0.8077 - val_loss: 0.4491 - val_accuracy: 0.7909\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3950 - accuracy: 0.8139 - val_loss: 0.4358 - val_accuracy: 0.7930\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3898 - accuracy: 0.8206 - val_loss: 0.4586 - val_accuracy: 0.7681\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3813 - accuracy: 0.8170 - val_loss: 0.4837 - val_accuracy: 0.7743\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.4005 - accuracy: 0.7999 - val_loss: 0.4441 - val_accuracy: 0.7909\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3828 - accuracy: 0.8258 - val_loss: 0.4351 - val_accuracy: 0.7930\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3724 - accuracy: 0.8191 - val_loss: 0.4606 - val_accuracy: 0.7723\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3879 - accuracy: 0.8217 - val_loss: 0.4341 - val_accuracy: 0.8054\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3675 - accuracy: 0.8305 - val_loss: 0.4488 - val_accuracy: 0.7785\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3765 - accuracy: 0.8165 - val_loss: 0.4460 - val_accuracy: 0.7930\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3694 - accuracy: 0.8248 - val_loss: 0.4669 - val_accuracy: 0.7909\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3945 - accuracy: 0.8097 - val_loss: 0.4651 - val_accuracy: 0.7888\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.3562 - accuracy: 0.8362 - val_loss: 0.4382 - val_accuracy: 0.7950\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3686 - accuracy: 0.8315 - val_loss: 0.4476 - val_accuracy: 0.7888\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 6s 104ms/step - loss: 0.3631 - accuracy: 0.8409 - val_loss: 0.4555 - val_accuracy: 0.7847\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3837 - accuracy: 0.8248 - val_loss: 0.4377 - val_accuracy: 0.7826\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3553 - accuracy: 0.8305 - val_loss: 0.4704 - val_accuracy: 0.7950\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3649 - accuracy: 0.8206 - val_loss: 0.4300 - val_accuracy: 0.7867\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3544 - accuracy: 0.8341 - val_loss: 0.4636 - val_accuracy: 0.8054\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3864 - accuracy: 0.8113 - val_loss: 0.4786 - val_accuracy: 0.7764\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.3651 - accuracy: 0.8217 - val_loss: 0.4428 - val_accuracy: 0.7867\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3592 - accuracy: 0.8289 - val_loss: 0.4530 - val_accuracy: 0.7723\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3688 - accuracy: 0.8154 - val_loss: 0.4575 - val_accuracy: 0.7930\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 6s 106ms/step - loss: 0.3645 - accuracy: 0.8284 - val_loss: 0.4632 - val_accuracy: 0.7867\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3726 - accuracy: 0.8258 - val_loss: 0.4537 - val_accuracy: 0.7950\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3394 - accuracy: 0.8450 - val_loss: 0.4726 - val_accuracy: 0.8033\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.3700 - accuracy: 0.8294 - val_loss: 0.4275 - val_accuracy: 0.7867\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3632 - accuracy: 0.8310 - val_loss: 0.4245 - val_accuracy: 0.7992\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3440 - accuracy: 0.8383 - val_loss: 0.4607 - val_accuracy: 0.7847\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4201 - accuracy: 0.8051 - val_loss: 0.4905 - val_accuracy: 0.7867\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3759 - accuracy: 0.8315 - val_loss: 0.4523 - val_accuracy: 0.7992\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3543 - accuracy: 0.8388 - val_loss: 0.4611 - val_accuracy: 0.7867\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3507 - accuracy: 0.8434 - val_loss: 0.5524 - val_accuracy: 0.7992\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.3663 - accuracy: 0.8341 - val_loss: 0.4653 - val_accuracy: 0.7992\n",
      "Epoch 105/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3465 - accuracy: 0.8434 - val_loss: 0.4634 - val_accuracy: 0.7660\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3949 - accuracy: 0.7999 - val_loss: 0.4817 - val_accuracy: 0.7743\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3440 - accuracy: 0.8424 - val_loss: 0.4861 - val_accuracy: 0.7992\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3604 - accuracy: 0.8294 - val_loss: 0.4964 - val_accuracy: 0.7743\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3561 - accuracy: 0.8383 - val_loss: 0.4752 - val_accuracy: 0.7867\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3344 - accuracy: 0.8497 - val_loss: 0.4484 - val_accuracy: 0.8157\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3308 - accuracy: 0.8476 - val_loss: 0.5418 - val_accuracy: 0.7826\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3375 - accuracy: 0.8409 - val_loss: 0.5144 - val_accuracy: 0.7992\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3386 - accuracy: 0.8362 - val_loss: 0.6407 - val_accuracy: 0.7619\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3435 - accuracy: 0.8403 - val_loss: 0.4544 - val_accuracy: 0.7847\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3345 - accuracy: 0.8466 - val_loss: 0.5497 - val_accuracy: 0.7516\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3605 - accuracy: 0.8294 - val_loss: 0.5026 - val_accuracy: 0.8012\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3704 - accuracy: 0.8300 - val_loss: 0.4525 - val_accuracy: 0.7930\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3396 - accuracy: 0.8351 - val_loss: 0.4548 - val_accuracy: 0.7992\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3166 - accuracy: 0.8533 - val_loss: 0.5049 - val_accuracy: 0.8012\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3105 - accuracy: 0.8611 - val_loss: 0.5187 - val_accuracy: 0.8012\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3275 - accuracy: 0.8497 - val_loss: 0.5944 - val_accuracy: 0.7930\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3231 - accuracy: 0.8595 - val_loss: 0.4629 - val_accuracy: 0.7867\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3656 - accuracy: 0.8403 - val_loss: 0.4755 - val_accuracy: 0.7743\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.4767 - accuracy: 0.7646 - val_loss: 0.4852 - val_accuracy: 0.7578\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3790 - accuracy: 0.8284 - val_loss: 0.4735 - val_accuracy: 0.7723\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3378 - accuracy: 0.8424 - val_loss: 0.4654 - val_accuracy: 0.7888\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3168 - accuracy: 0.8533 - val_loss: 0.4850 - val_accuracy: 0.7930\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3156 - accuracy: 0.8543 - val_loss: 0.4919 - val_accuracy: 0.7909\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3037 - accuracy: 0.8564 - val_loss: 0.4967 - val_accuracy: 0.7971\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3064 - accuracy: 0.8663 - val_loss: 0.4983 - val_accuracy: 0.8075\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.3106 - accuracy: 0.8600 - val_loss: 0.5001 - val_accuracy: 0.8012\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3054 - accuracy: 0.8621 - val_loss: 0.5149 - val_accuracy: 0.7826\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 7s 108ms/step - loss: 0.3163 - accuracy: 0.8605 - val_loss: 0.4331 - val_accuracy: 0.7971\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3069 - accuracy: 0.8673 - val_loss: 0.4556 - val_accuracy: 0.7867\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.2930 - accuracy: 0.8668 - val_loss: 0.5919 - val_accuracy: 0.7826\n",
      "Epoch 136/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.2755 - accuracy: 0.8787 - val_loss: 0.4913 - val_accuracy: 0.7743\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3145 - accuracy: 0.8554 - val_loss: 0.5391 - val_accuracy: 0.7743\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.3031 - accuracy: 0.8642 - val_loss: 0.5133 - val_accuracy: 0.7930\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2934 - accuracy: 0.8605 - val_loss: 0.5282 - val_accuracy: 0.7930\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3137 - accuracy: 0.8455 - val_loss: 0.5094 - val_accuracy: 0.7826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2981 - accuracy: 0.8605 - val_loss: 0.5593 - val_accuracy: 0.7867\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3000 - accuracy: 0.8621 - val_loss: 0.4782 - val_accuracy: 0.7950\n",
      "Epoch 143/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3135 - accuracy: 0.8585 - val_loss: 0.5029 - val_accuracy: 0.7619\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3205 - accuracy: 0.8621 - val_loss: 0.4935 - val_accuracy: 0.7764\n",
      "Epoch 145/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2967 - accuracy: 0.8590 - val_loss: 0.5011 - val_accuracy: 0.7930\n",
      "Epoch 146/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2946 - accuracy: 0.8657 - val_loss: 0.4423 - val_accuracy: 0.8054\n",
      "Epoch 147/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2908 - accuracy: 0.8663 - val_loss: 0.6065 - val_accuracy: 0.7867\n",
      "Epoch 148/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2770 - accuracy: 0.8740 - val_loss: 0.6214 - val_accuracy: 0.7971\n",
      "Epoch 149/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.3240 - accuracy: 0.8502 - val_loss: 0.4912 - val_accuracy: 0.7971\n",
      "Epoch 150/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3179 - accuracy: 0.8476 - val_loss: 0.5198 - val_accuracy: 0.7764\n",
      "Epoch 151/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2755 - accuracy: 0.8787 - val_loss: 0.5897 - val_accuracy: 0.7909\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2799 - accuracy: 0.8828 - val_loss: 0.6367 - val_accuracy: 0.7888\n",
      "Epoch 153/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2526 - accuracy: 0.8870 - val_loss: 0.6124 - val_accuracy: 0.7930\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3005 - accuracy: 0.8595 - val_loss: 0.4869 - val_accuracy: 0.7909\n",
      "Epoch 155/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2849 - accuracy: 0.8745 - val_loss: 0.5180 - val_accuracy: 0.8012\n",
      "Epoch 156/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3017 - accuracy: 0.8647 - val_loss: 0.4933 - val_accuracy: 0.7930\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2973 - accuracy: 0.8673 - val_loss: 0.4580 - val_accuracy: 0.8054\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.2960 - accuracy: 0.8600 - val_loss: 0.5001 - val_accuracy: 0.8054\n",
      "Epoch 159/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2725 - accuracy: 0.8777 - val_loss: 0.5286 - val_accuracy: 0.8054\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2716 - accuracy: 0.8782 - val_loss: 0.4839 - val_accuracy: 0.7888\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2777 - accuracy: 0.8730 - val_loss: 0.5820 - val_accuracy: 0.7992\n",
      "Epoch 162/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2708 - accuracy: 0.8756 - val_loss: 0.5363 - val_accuracy: 0.7992\n",
      "Epoch 163/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2622 - accuracy: 0.8792 - val_loss: 0.4915 - val_accuracy: 0.7992\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2520 - accuracy: 0.8875 - val_loss: 0.6295 - val_accuracy: 0.7930\n",
      "Epoch 165/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2556 - accuracy: 0.8880 - val_loss: 0.4970 - val_accuracy: 0.7930\n",
      "Epoch 166/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2708 - accuracy: 0.8761 - val_loss: 0.6491 - val_accuracy: 0.7971\n",
      "Epoch 167/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2994 - accuracy: 0.8657 - val_loss: 0.6401 - val_accuracy: 0.8054\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2585 - accuracy: 0.8911 - val_loss: 0.5754 - val_accuracy: 0.8116\n",
      "Epoch 169/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2476 - accuracy: 0.8896 - val_loss: 0.6511 - val_accuracy: 0.7805\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2744 - accuracy: 0.8678 - val_loss: 0.5555 - val_accuracy: 0.7950\n",
      "Epoch 171/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3121 - accuracy: 0.8600 - val_loss: 0.7507 - val_accuracy: 0.7805\n",
      "Epoch 172/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3082 - accuracy: 0.8751 - val_loss: 0.5324 - val_accuracy: 0.7867\n",
      "Epoch 173/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2716 - accuracy: 0.8849 - val_loss: 0.5791 - val_accuracy: 0.7867\n",
      "Epoch 174/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2657 - accuracy: 0.8766 - val_loss: 0.5787 - val_accuracy: 0.7847\n",
      "Epoch 175/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2673 - accuracy: 0.8875 - val_loss: 0.5490 - val_accuracy: 0.7826\n",
      "Epoch 176/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2714 - accuracy: 0.8720 - val_loss: 0.5588 - val_accuracy: 0.7702\n",
      "Epoch 177/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2640 - accuracy: 0.8756 - val_loss: 0.6593 - val_accuracy: 0.7785\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2441 - accuracy: 0.8917 - val_loss: 0.5886 - val_accuracy: 0.7909\n",
      "Epoch 179/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2564 - accuracy: 0.8849 - val_loss: 0.6003 - val_accuracy: 0.7847\n",
      "Epoch 180/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.3496 - accuracy: 0.8393 - val_loss: 0.5055 - val_accuracy: 0.7888\n",
      "Epoch 181/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.2664 - accuracy: 0.8880 - val_loss: 0.6791 - val_accuracy: 0.7847\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2497 - accuracy: 0.8901 - val_loss: 0.5510 - val_accuracy: 0.7826\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3467 - accuracy: 0.8517 - val_loss: 0.5365 - val_accuracy: 0.7971\n",
      "Epoch 184/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2345 - accuracy: 0.8984 - val_loss: 0.6474 - val_accuracy: 0.7992\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2789 - accuracy: 0.8688 - val_loss: 0.5489 - val_accuracy: 0.7888\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2636 - accuracy: 0.8777 - val_loss: 0.5952 - val_accuracy: 0.8033\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.2342 - accuracy: 0.8922 - val_loss: 0.5711 - val_accuracy: 0.8033\n",
      "Epoch 188/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.2459 - accuracy: 0.8901 - val_loss: 0.6226 - val_accuracy: 0.7826\n",
      "Epoch 189/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2366 - accuracy: 0.8984 - val_loss: 0.6558 - val_accuracy: 0.7867\n",
      "Epoch 190/200\n",
      "61/61 [==============================] - 5s 78ms/step - loss: 0.2447 - accuracy: 0.8828 - val_loss: 0.5430 - val_accuracy: 0.7909\n",
      "Epoch 191/200\n",
      "61/61 [==============================] - 4s 71ms/step - loss: 0.2960 - accuracy: 0.8631 - val_loss: 0.5355 - val_accuracy: 0.7764\n",
      "Epoch 192/200\n",
      "61/61 [==============================] - 4s 70ms/step - loss: 0.2635 - accuracy: 0.8808 - val_loss: 0.6114 - val_accuracy: 0.7992\n",
      "Epoch 193/200\n",
      "61/61 [==============================] - 5s 76ms/step - loss: 0.2396 - accuracy: 0.8834 - val_loss: 0.5023 - val_accuracy: 0.7950\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - 4s 71ms/step - loss: 0.2263 - accuracy: 0.9051 - val_loss: 0.7180 - val_accuracy: 0.7888\n",
      "Epoch 195/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.2166 - accuracy: 0.8963 - val_loss: 0.6216 - val_accuracy: 0.7950\n",
      "Epoch 196/200\n",
      "61/61 [==============================] - 4s 68ms/step - loss: 0.2277 - accuracy: 0.8948 - val_loss: 0.6167 - val_accuracy: 0.7888\n",
      "Epoch 197/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.2258 - accuracy: 0.9010 - val_loss: 0.5379 - val_accuracy: 0.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      "61/61 [==============================] - 4s 73ms/step - loss: 0.2065 - accuracy: 0.9082 - val_loss: 0.6557 - val_accuracy: 0.7992\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.2048 - accuracy: 0.9062 - val_loss: 0.7715 - val_accuracy: 0.7888\n",
      "Epoch 200/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.1954 - accuracy: 0.9124 - val_loss: 0.7009 - val_accuracy: 0.8075\n",
      "16/16 [==============================] - 3s 23ms/step\n",
      "Fold score (Accuracy score): 0.8074534161490683\n",
      "Matriz de confusion\n",
      "-------------------\n",
      "---------| Verdadero | Falso |\n",
      "Verdadero|  163        15\n",
      "Falso    |  78        227\n",
      "Fold #3\n",
      "Train - X:(1930, 40) y:(1930, 2)\n",
      "Test - X:(482, 40) y:(482, 2)\n",
      "Epoch 1/200\n",
      "61/61 [==============================] - 21s 145ms/step - loss: 0.6864 - accuracy: 0.5409 - val_loss: 0.6593 - val_accuracy: 0.6577\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.6431 - accuracy: 0.6368 - val_loss: 0.5978 - val_accuracy: 0.7033\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.6112 - accuracy: 0.6725 - val_loss: 0.5577 - val_accuracy: 0.7261\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.5853 - accuracy: 0.6995 - val_loss: 0.5413 - val_accuracy: 0.7324\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.5826 - accuracy: 0.6741 - val_loss: 0.5355 - val_accuracy: 0.7199\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.5787 - accuracy: 0.6876 - val_loss: 0.5258 - val_accuracy: 0.7303\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.5678 - accuracy: 0.7073 - val_loss: 0.5203 - val_accuracy: 0.7365\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5525 - accuracy: 0.7233 - val_loss: 0.5096 - val_accuracy: 0.7427\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5469 - accuracy: 0.7290 - val_loss: 0.5224 - val_accuracy: 0.7407\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.5462 - accuracy: 0.7223 - val_loss: 0.4896 - val_accuracy: 0.7656\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5427 - accuracy: 0.7321 - val_loss: 0.4919 - val_accuracy: 0.7635\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5293 - accuracy: 0.7275 - val_loss: 0.5367 - val_accuracy: 0.7054\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5335 - accuracy: 0.7264 - val_loss: 0.4683 - val_accuracy: 0.7884\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5320 - accuracy: 0.7337 - val_loss: 0.5019 - val_accuracy: 0.7427\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5211 - accuracy: 0.7420 - val_loss: 0.4610 - val_accuracy: 0.7780\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5237 - accuracy: 0.7373 - val_loss: 0.4994 - val_accuracy: 0.7407\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5183 - accuracy: 0.7378 - val_loss: 0.4732 - val_accuracy: 0.7656\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5495 - accuracy: 0.7130 - val_loss: 0.5598 - val_accuracy: 0.6950\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5462 - accuracy: 0.7155 - val_loss: 0.5125 - val_accuracy: 0.7324\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5221 - accuracy: 0.7306 - val_loss: 0.4840 - val_accuracy: 0.7676\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5302 - accuracy: 0.7244 - val_loss: 0.4735 - val_accuracy: 0.7676\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.5158 - accuracy: 0.7290 - val_loss: 0.4798 - val_accuracy: 0.7635\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5144 - accuracy: 0.7394 - val_loss: 0.4671 - val_accuracy: 0.7801\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4974 - accuracy: 0.7611 - val_loss: 0.4552 - val_accuracy: 0.7946\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.5669 - accuracy: 0.7067 - val_loss: 0.4703 - val_accuracy: 0.7697\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.5103 - accuracy: 0.7389 - val_loss: 0.4458 - val_accuracy: 0.8050\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4873 - accuracy: 0.7642 - val_loss: 0.4374 - val_accuracy: 0.8050\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5035 - accuracy: 0.7404 - val_loss: 0.4569 - val_accuracy: 0.7863\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4853 - accuracy: 0.7611 - val_loss: 0.4339 - val_accuracy: 0.7967\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4988 - accuracy: 0.7518 - val_loss: 0.4698 - val_accuracy: 0.7780\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4816 - accuracy: 0.7554 - val_loss: 0.4312 - val_accuracy: 0.7988\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4795 - accuracy: 0.7617 - val_loss: 0.4266 - val_accuracy: 0.7988\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4822 - accuracy: 0.7565 - val_loss: 0.4211 - val_accuracy: 0.7946\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4960 - accuracy: 0.7492 - val_loss: 0.4474 - val_accuracy: 0.7863\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4713 - accuracy: 0.7694 - val_loss: 0.4324 - val_accuracy: 0.7905\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5018 - accuracy: 0.7425 - val_loss: 0.4474 - val_accuracy: 0.7925\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4874 - accuracy: 0.7622 - val_loss: 0.5870 - val_accuracy: 0.7075\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4999 - accuracy: 0.7461 - val_loss: 0.4520 - val_accuracy: 0.7822\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4827 - accuracy: 0.7611 - val_loss: 0.4836 - val_accuracy: 0.7407\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4777 - accuracy: 0.7570 - val_loss: 0.4261 - val_accuracy: 0.8008\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4850 - accuracy: 0.7539 - val_loss: 0.4432 - val_accuracy: 0.7967\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4710 - accuracy: 0.7606 - val_loss: 0.4325 - val_accuracy: 0.8071\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4704 - accuracy: 0.7601 - val_loss: 0.4324 - val_accuracy: 0.7988\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4485 - accuracy: 0.7777 - val_loss: 0.4072 - val_accuracy: 0.8050\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4511 - accuracy: 0.7788 - val_loss: 0.3980 - val_accuracy: 0.8112\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4514 - accuracy: 0.7855 - val_loss: 0.4131 - val_accuracy: 0.8008\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4492 - accuracy: 0.7777 - val_loss: 0.4071 - val_accuracy: 0.8091\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4491 - accuracy: 0.7746 - val_loss: 0.4158 - val_accuracy: 0.8071\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4542 - accuracy: 0.7710 - val_loss: 0.4448 - val_accuracy: 0.7697\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4364 - accuracy: 0.7741 - val_loss: 0.4073 - val_accuracy: 0.8154\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4411 - accuracy: 0.7782 - val_loss: 0.3920 - val_accuracy: 0.8154\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4392 - accuracy: 0.7824 - val_loss: 0.4242 - val_accuracy: 0.7967\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4667 - accuracy: 0.7606 - val_loss: 0.4095 - val_accuracy: 0.8091\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4484 - accuracy: 0.7793 - val_loss: 0.4023 - val_accuracy: 0.8091\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4605 - accuracy: 0.7611 - val_loss: 0.4941 - val_accuracy: 0.7469\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4747 - accuracy: 0.7679 - val_loss: 0.4569 - val_accuracy: 0.7905\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4511 - accuracy: 0.7668 - val_loss: 0.4301 - val_accuracy: 0.7946\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4449 - accuracy: 0.7865 - val_loss: 0.4648 - val_accuracy: 0.7531\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4494 - accuracy: 0.7679 - val_loss: 0.4310 - val_accuracy: 0.7925\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4571 - accuracy: 0.7705 - val_loss: 0.4212 - val_accuracy: 0.8029\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4387 - accuracy: 0.7803 - val_loss: 0.4492 - val_accuracy: 0.7884\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4330 - accuracy: 0.7782 - val_loss: 0.4042 - val_accuracy: 0.8216\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4235 - accuracy: 0.7855 - val_loss: 0.4051 - val_accuracy: 0.8112\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4410 - accuracy: 0.7813 - val_loss: 0.4104 - val_accuracy: 0.7946\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4165 - accuracy: 0.7850 - val_loss: 0.4047 - val_accuracy: 0.8154\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4402 - accuracy: 0.7767 - val_loss: 0.4169 - val_accuracy: 0.8195\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4315 - accuracy: 0.7850 - val_loss: 0.4324 - val_accuracy: 0.8091\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4173 - accuracy: 0.7912 - val_loss: 0.4136 - val_accuracy: 0.7988\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4293 - accuracy: 0.7865 - val_loss: 0.4228 - val_accuracy: 0.8008\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4389 - accuracy: 0.7741 - val_loss: 0.4713 - val_accuracy: 0.7842\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4424 - accuracy: 0.7777 - val_loss: 0.4327 - val_accuracy: 0.8071\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4172 - accuracy: 0.7948 - val_loss: 0.4296 - val_accuracy: 0.8112\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4133 - accuracy: 0.7927 - val_loss: 0.4152 - val_accuracy: 0.8112\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4039 - accuracy: 0.8026 - val_loss: 0.4035 - val_accuracy: 0.7967\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3950 - accuracy: 0.7959 - val_loss: 0.4728 - val_accuracy: 0.7988\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3915 - accuracy: 0.8005 - val_loss: 0.3880 - val_accuracy: 0.8133\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4016 - accuracy: 0.7953 - val_loss: 0.4219 - val_accuracy: 0.8133\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4199 - accuracy: 0.7819 - val_loss: 0.5387 - val_accuracy: 0.7780\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4065 - accuracy: 0.7922 - val_loss: 0.4358 - val_accuracy: 0.8133\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3877 - accuracy: 0.8119 - val_loss: 0.4125 - val_accuracy: 0.8112\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4390 - accuracy: 0.7850 - val_loss: 0.4492 - val_accuracy: 0.7759\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4128 - accuracy: 0.8000 - val_loss: 0.4220 - val_accuracy: 0.7946\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4092 - accuracy: 0.7943 - val_loss: 0.4729 - val_accuracy: 0.8050\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3823 - accuracy: 0.8145 - val_loss: 0.4206 - val_accuracy: 0.8237\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3979 - accuracy: 0.7995 - val_loss: 0.4221 - val_accuracy: 0.7988\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3827 - accuracy: 0.8140 - val_loss: 0.4089 - val_accuracy: 0.8008\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3818 - accuracy: 0.8119 - val_loss: 0.4847 - val_accuracy: 0.8237\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.4086 - accuracy: 0.8093 - val_loss: 0.4563 - val_accuracy: 0.8091\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4149 - accuracy: 0.7886 - val_loss: 0.4287 - val_accuracy: 0.8174\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3899 - accuracy: 0.8124 - val_loss: 0.5849 - val_accuracy: 0.7759\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3999 - accuracy: 0.8067 - val_loss: 0.4049 - val_accuracy: 0.8091\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3634 - accuracy: 0.8171 - val_loss: 0.4301 - val_accuracy: 0.8195\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3638 - accuracy: 0.8280 - val_loss: 0.4090 - val_accuracy: 0.8112\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3778 - accuracy: 0.8119 - val_loss: 0.4117 - val_accuracy: 0.8216\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3850 - accuracy: 0.8093 - val_loss: 0.4513 - val_accuracy: 0.7905\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3796 - accuracy: 0.8166 - val_loss: 0.4510 - val_accuracy: 0.8133\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3720 - accuracy: 0.8114 - val_loss: 0.4658 - val_accuracy: 0.8237\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.3685 - accuracy: 0.8192 - val_loss: 0.4228 - val_accuracy: 0.8071\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3609 - accuracy: 0.8311 - val_loss: 0.4634 - val_accuracy: 0.8154\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3765 - accuracy: 0.8135 - val_loss: 0.4234 - val_accuracy: 0.7822\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3747 - accuracy: 0.8187 - val_loss: 0.3930 - val_accuracy: 0.8320\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3929 - accuracy: 0.7974 - val_loss: 0.4703 - val_accuracy: 0.7946\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3694 - accuracy: 0.8155 - val_loss: 0.4299 - val_accuracy: 0.8112\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3843 - accuracy: 0.8026 - val_loss: 0.4689 - val_accuracy: 0.7988\n",
      "Epoch 105/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3720 - accuracy: 0.8228 - val_loss: 0.4169 - val_accuracy: 0.8216\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3670 - accuracy: 0.8207 - val_loss: 0.5041 - val_accuracy: 0.7822\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3684 - accuracy: 0.8238 - val_loss: 0.4194 - val_accuracy: 0.8278\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3676 - accuracy: 0.8176 - val_loss: 0.4666 - val_accuracy: 0.8320\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3502 - accuracy: 0.8290 - val_loss: 0.4189 - val_accuracy: 0.8008\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4043 - accuracy: 0.7927 - val_loss: 0.4232 - val_accuracy: 0.8257\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.3578 - accuracy: 0.8295 - val_loss: 0.4187 - val_accuracy: 0.8091\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3424 - accuracy: 0.8363 - val_loss: 0.4761 - val_accuracy: 0.8154\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3382 - accuracy: 0.8337 - val_loss: 0.4273 - val_accuracy: 0.8216\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3433 - accuracy: 0.8280 - val_loss: 0.4511 - val_accuracy: 0.8195\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3475 - accuracy: 0.8373 - val_loss: 0.4279 - val_accuracy: 0.8340\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3532 - accuracy: 0.8269 - val_loss: 0.4072 - val_accuracy: 0.8257\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3380 - accuracy: 0.8378 - val_loss: 0.4310 - val_accuracy: 0.8195\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3365 - accuracy: 0.8306 - val_loss: 0.4109 - val_accuracy: 0.8361\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3309 - accuracy: 0.8326 - val_loss: 0.4110 - val_accuracy: 0.8216\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3256 - accuracy: 0.8420 - val_loss: 0.4393 - val_accuracy: 0.8257\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3378 - accuracy: 0.8358 - val_loss: 0.4644 - val_accuracy: 0.8278\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3262 - accuracy: 0.8389 - val_loss: 0.4791 - val_accuracy: 0.8050\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.4016 - accuracy: 0.8052 - val_loss: 0.4703 - val_accuracy: 0.8216\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3494 - accuracy: 0.8228 - val_loss: 0.4118 - val_accuracy: 0.8174\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.3316 - accuracy: 0.8363 - val_loss: 0.4373 - val_accuracy: 0.8216\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3427 - accuracy: 0.8280 - val_loss: 0.4553 - val_accuracy: 0.8174\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3855 - accuracy: 0.8181 - val_loss: 0.4447 - val_accuracy: 0.7884\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3451 - accuracy: 0.8301 - val_loss: 0.4847 - val_accuracy: 0.8112\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3219 - accuracy: 0.8425 - val_loss: 0.4490 - val_accuracy: 0.8154\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3334 - accuracy: 0.8368 - val_loss: 0.4454 - val_accuracy: 0.8216\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.3344 - accuracy: 0.8332 - val_loss: 0.4506 - val_accuracy: 0.8071\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.3251 - accuracy: 0.8352 - val_loss: 0.5077 - val_accuracy: 0.8154\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.3190 - accuracy: 0.8373 - val_loss: 0.5078 - val_accuracy: 0.7842\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.3293 - accuracy: 0.8358 - val_loss: 0.4472 - val_accuracy: 0.8091\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3737 - accuracy: 0.8135 - val_loss: 0.4976 - val_accuracy: 0.7697\n",
      "Epoch 136/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3693 - accuracy: 0.8166 - val_loss: 0.4733 - val_accuracy: 0.8154\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 5s 78ms/step - loss: 0.3039 - accuracy: 0.8554 - val_loss: 0.4494 - val_accuracy: 0.8278\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 4s 72ms/step - loss: 0.3269 - accuracy: 0.8347 - val_loss: 0.4745 - val_accuracy: 0.8195\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 5s 74ms/step - loss: 0.3363 - accuracy: 0.8285 - val_loss: 0.4410 - val_accuracy: 0.8278\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.3076 - accuracy: 0.8461 - val_loss: 0.4874 - val_accuracy: 0.8112\n",
      "Epoch 141/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.2891 - accuracy: 0.8503 - val_loss: 0.5239 - val_accuracy: 0.8174\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.2824 - accuracy: 0.8632 - val_loss: 0.4930 - val_accuracy: 0.8257\n",
      "Epoch 143/200\n",
      "61/61 [==============================] - 5s 78ms/step - loss: 0.3292 - accuracy: 0.8337 - val_loss: 0.5203 - val_accuracy: 0.8091\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.2908 - accuracy: 0.8554 - val_loss: 0.4706 - val_accuracy: 0.8237\n",
      "Epoch 145/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.2801 - accuracy: 0.8544 - val_loss: 0.5406 - val_accuracy: 0.8091\n",
      "Epoch 146/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.3839 - accuracy: 0.8301 - val_loss: 0.4919 - val_accuracy: 0.7676\n",
      "Epoch 147/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3876 - accuracy: 0.7933 - val_loss: 0.4740 - val_accuracy: 0.7863\n",
      "Epoch 148/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.3391 - accuracy: 0.8259 - val_loss: 0.4561 - val_accuracy: 0.8112\n",
      "Epoch 149/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3115 - accuracy: 0.8425 - val_loss: 0.4857 - val_accuracy: 0.8133\n",
      "Epoch 150/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.2946 - accuracy: 0.8560 - val_loss: 0.5048 - val_accuracy: 0.8112\n",
      "Epoch 151/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.3072 - accuracy: 0.8420 - val_loss: 0.4823 - val_accuracy: 0.8154\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - 5s 78ms/step - loss: 0.2982 - accuracy: 0.8446 - val_loss: 0.4824 - val_accuracy: 0.7946\n",
      "Epoch 153/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3134 - accuracy: 0.8554 - val_loss: 0.4340 - val_accuracy: 0.8154\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.3037 - accuracy: 0.8451 - val_loss: 0.4868 - val_accuracy: 0.8174\n",
      "Epoch 155/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.2748 - accuracy: 0.8741 - val_loss: 0.5253 - val_accuracy: 0.8174\n",
      "Epoch 156/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.2628 - accuracy: 0.8819 - val_loss: 0.5105 - val_accuracy: 0.7967\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.2770 - accuracy: 0.8715 - val_loss: 0.4791 - val_accuracy: 0.8257\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.2632 - accuracy: 0.8782 - val_loss: 0.5885 - val_accuracy: 0.8029\n",
      "Epoch 159/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.3461 - accuracy: 0.8435 - val_loss: 0.4474 - val_accuracy: 0.7988\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - 5s 78ms/step - loss: 0.2936 - accuracy: 0.8710 - val_loss: 0.4801 - val_accuracy: 0.8257\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.2897 - accuracy: 0.8803 - val_loss: 0.4972 - val_accuracy: 0.8133\n",
      "Epoch 162/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.2806 - accuracy: 0.8793 - val_loss: 0.4638 - val_accuracy: 0.8216\n",
      "Epoch 163/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.2612 - accuracy: 0.8788 - val_loss: 0.5363 - val_accuracy: 0.8091\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2897 - accuracy: 0.8793 - val_loss: 0.5003 - val_accuracy: 0.8008\n",
      "Epoch 165/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.2562 - accuracy: 0.8927 - val_loss: 0.5186 - val_accuracy: 0.8091\n",
      "Epoch 166/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.2526 - accuracy: 0.8819 - val_loss: 0.5830 - val_accuracy: 0.7967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.3265 - accuracy: 0.8544 - val_loss: 0.5081 - val_accuracy: 0.8112\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.2645 - accuracy: 0.8793 - val_loss: 0.6157 - val_accuracy: 0.8091\n",
      "Epoch 169/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.2530 - accuracy: 0.8819 - val_loss: 0.5408 - val_accuracy: 0.8237\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2331 - accuracy: 0.8881 - val_loss: 0.5543 - val_accuracy: 0.8299\n",
      "Epoch 171/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2833 - accuracy: 0.8772 - val_loss: 0.5017 - val_accuracy: 0.8050\n",
      "Epoch 172/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3364 - accuracy: 0.8482 - val_loss: 0.5085 - val_accuracy: 0.8112\n",
      "Epoch 173/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2512 - accuracy: 0.8927 - val_loss: 0.5084 - val_accuracy: 0.8091\n",
      "Epoch 174/200\n",
      "61/61 [==============================] - 5s 77ms/step - loss: 0.2386 - accuracy: 0.8855 - val_loss: 0.5527 - val_accuracy: 0.8133\n",
      "Epoch 175/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.2569 - accuracy: 0.8917 - val_loss: 0.5559 - val_accuracy: 0.8174\n",
      "Epoch 176/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.2316 - accuracy: 0.9067 - val_loss: 0.5769 - val_accuracy: 0.8257\n",
      "Epoch 177/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.2834 - accuracy: 0.8751 - val_loss: 0.4636 - val_accuracy: 0.8091\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2424 - accuracy: 0.8912 - val_loss: 0.5442 - val_accuracy: 0.8091\n",
      "Epoch 179/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2400 - accuracy: 0.8907 - val_loss: 0.6558 - val_accuracy: 0.8112\n",
      "Epoch 180/200\n",
      "61/61 [==============================] - 7s 109ms/step - loss: 0.2259 - accuracy: 0.8964 - val_loss: 0.6224 - val_accuracy: 0.8195\n",
      "Epoch 181/200\n",
      "61/61 [==============================] - 7s 107ms/step - loss: 0.2495 - accuracy: 0.8808 - val_loss: 0.6183 - val_accuracy: 0.8133\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.2492 - accuracy: 0.8876 - val_loss: 0.6020 - val_accuracy: 0.8008\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.2174 - accuracy: 0.9010 - val_loss: 0.6239 - val_accuracy: 0.8237\n",
      "Epoch 184/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.2222 - accuracy: 0.9036 - val_loss: 0.6150 - val_accuracy: 0.7967\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2398 - accuracy: 0.8948 - val_loss: 0.5436 - val_accuracy: 0.7842\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.2796 - accuracy: 0.8694 - val_loss: 0.4861 - val_accuracy: 0.8154\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2291 - accuracy: 0.8953 - val_loss: 0.5923 - val_accuracy: 0.8174\n",
      "Epoch 188/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.2223 - accuracy: 0.9036 - val_loss: 0.6003 - val_accuracy: 0.8112\n",
      "Epoch 189/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2241 - accuracy: 0.9010 - val_loss: 0.7093 - val_accuracy: 0.7946\n",
      "Epoch 190/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2113 - accuracy: 0.9041 - val_loss: 0.6640 - val_accuracy: 0.8133\n",
      "Epoch 191/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.2853 - accuracy: 0.8788 - val_loss: 0.6885 - val_accuracy: 0.7822\n",
      "Epoch 192/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.3303 - accuracy: 0.8554 - val_loss: 0.4792 - val_accuracy: 0.8112\n",
      "Epoch 193/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2299 - accuracy: 0.9073 - val_loss: 0.5559 - val_accuracy: 0.7967\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - 6s 106ms/step - loss: 0.2005 - accuracy: 0.9130 - val_loss: 0.6172 - val_accuracy: 0.7967\n",
      "Epoch 195/200\n",
      "61/61 [==============================] - 7s 107ms/step - loss: 0.1875 - accuracy: 0.9130 - val_loss: 0.6280 - val_accuracy: 0.7967\n",
      "Epoch 196/200\n",
      "61/61 [==============================] - 7s 118ms/step - loss: 0.2515 - accuracy: 0.8870 - val_loss: 0.6309 - val_accuracy: 0.7718\n",
      "Epoch 197/200\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.2015 - accuracy: 0.9093 - val_loss: 0.6715 - val_accuracy: 0.7905\n",
      "Epoch 198/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.2162 - accuracy: 0.9021 - val_loss: 0.6816 - val_accuracy: 0.8154\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.1860 - accuracy: 0.9176 - val_loss: 0.6792 - val_accuracy: 0.7884\n",
      "Epoch 200/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.2533 - accuracy: 0.8964 - val_loss: 0.5788 - val_accuracy: 0.7988\n",
      "16/16 [==============================] - 4s 28ms/step\n",
      "Fold score (Accuracy score): 0.7987551867219918\n",
      "Matriz de confusion\n",
      "-------------------\n",
      "---------| Verdadero | Falso |\n",
      "Verdadero|  203        57\n",
      "Falso    |  40        182\n",
      "Fold #4\n",
      "Train - X:(1930, 40) y:(1930, 2)\n",
      "Test - X:(482, 40) y:(482, 2)\n",
      "Epoch 1/200\n",
      "61/61 [==============================] - 18s 124ms/step - loss: 0.6888 - accuracy: 0.5280 - val_loss: 0.6715 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.6538 - accuracy: 0.6057 - val_loss: 0.6034 - val_accuracy: 0.7054\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.6284 - accuracy: 0.6839 - val_loss: 0.5884 - val_accuracy: 0.7178\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.6146 - accuracy: 0.6855 - val_loss: 0.5777 - val_accuracy: 0.7137\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 7s 118ms/step - loss: 0.6078 - accuracy: 0.7067 - val_loss: 0.6928 - val_accuracy: 0.5705\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.6183 - accuracy: 0.6756 - val_loss: 0.5583 - val_accuracy: 0.7407\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.6011 - accuracy: 0.6907 - val_loss: 0.5532 - val_accuracy: 0.7199\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.5848 - accuracy: 0.7192 - val_loss: 0.5469 - val_accuracy: 0.7490\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 6s 104ms/step - loss: 0.5864 - accuracy: 0.7187 - val_loss: 0.5436 - val_accuracy: 0.7386\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 0.5712 - accuracy: 0.7202 - val_loss: 0.5486 - val_accuracy: 0.7199\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.5745 - accuracy: 0.7155 - val_loss: 0.5303 - val_accuracy: 0.7510\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.5569 - accuracy: 0.7301 - val_loss: 0.5275 - val_accuracy: 0.7469\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.5577 - accuracy: 0.7192 - val_loss: 0.5430 - val_accuracy: 0.7220\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.5612 - accuracy: 0.7275 - val_loss: 0.5167 - val_accuracy: 0.7531\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.5645 - accuracy: 0.7254 - val_loss: 0.5283 - val_accuracy: 0.7386\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.5536 - accuracy: 0.7244 - val_loss: 0.5161 - val_accuracy: 0.7386\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.5539 - accuracy: 0.7368 - val_loss: 0.5063 - val_accuracy: 0.7656\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.5519 - accuracy: 0.7321 - val_loss: 0.5055 - val_accuracy: 0.7656\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.5599 - accuracy: 0.7155 - val_loss: 0.5193 - val_accuracy: 0.7510\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.5321 - accuracy: 0.7358 - val_loss: 0.5133 - val_accuracy: 0.7261\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 7s 109ms/step - loss: 0.5371 - accuracy: 0.7332 - val_loss: 0.5002 - val_accuracy: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.5499 - accuracy: 0.7373 - val_loss: 0.5139 - val_accuracy: 0.7386\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 6s 106ms/step - loss: 0.5311 - accuracy: 0.7404 - val_loss: 0.4864 - val_accuracy: 0.7635\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.5308 - accuracy: 0.7456 - val_loss: 0.5279 - val_accuracy: 0.7324\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.5420 - accuracy: 0.7394 - val_loss: 0.5083 - val_accuracy: 0.7448\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.5301 - accuracy: 0.7482 - val_loss: 0.4767 - val_accuracy: 0.7718\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.5293 - accuracy: 0.7446 - val_loss: 0.4733 - val_accuracy: 0.7801\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.5117 - accuracy: 0.7440 - val_loss: 0.5046 - val_accuracy: 0.7490\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.5158 - accuracy: 0.7425 - val_loss: 0.4754 - val_accuracy: 0.7635\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.5100 - accuracy: 0.7539 - val_loss: 0.4761 - val_accuracy: 0.7676\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.5029 - accuracy: 0.7575 - val_loss: 0.4740 - val_accuracy: 0.7842\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.5058 - accuracy: 0.7492 - val_loss: 0.4660 - val_accuracy: 0.7780\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.4997 - accuracy: 0.7430 - val_loss: 0.4734 - val_accuracy: 0.7573\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.5189 - accuracy: 0.7440 - val_loss: 0.4890 - val_accuracy: 0.7469\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.5038 - accuracy: 0.7674 - val_loss: 0.4712 - val_accuracy: 0.7739\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.4882 - accuracy: 0.7637 - val_loss: 0.4653 - val_accuracy: 0.7697\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.4952 - accuracy: 0.7715 - val_loss: 0.4872 - val_accuracy: 0.7427\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.4998 - accuracy: 0.7617 - val_loss: 0.4638 - val_accuracy: 0.7822\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.5005 - accuracy: 0.7705 - val_loss: 0.4607 - val_accuracy: 0.7884\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4768 - accuracy: 0.7767 - val_loss: 0.4546 - val_accuracy: 0.7905\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 5s 76ms/step - loss: 0.4782 - accuracy: 0.7782 - val_loss: 0.4604 - val_accuracy: 0.7614\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.4802 - accuracy: 0.7762 - val_loss: 0.4877 - val_accuracy: 0.7697\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.4864 - accuracy: 0.7777 - val_loss: 0.4817 - val_accuracy: 0.7427\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.4744 - accuracy: 0.7896 - val_loss: 0.4640 - val_accuracy: 0.7822\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.4718 - accuracy: 0.7855 - val_loss: 0.4730 - val_accuracy: 0.7842\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.4777 - accuracy: 0.7777 - val_loss: 0.4468 - val_accuracy: 0.7842\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.4630 - accuracy: 0.7907 - val_loss: 0.4557 - val_accuracy: 0.7925\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - 7s 107ms/step - loss: 0.4567 - accuracy: 0.7865 - val_loss: 0.4465 - val_accuracy: 0.7863\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 6s 107ms/step - loss: 0.4427 - accuracy: 0.7927 - val_loss: 0.4701 - val_accuracy: 0.7780\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.4785 - accuracy: 0.7808 - val_loss: 0.4579 - val_accuracy: 0.7490\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 6s 104ms/step - loss: 0.4633 - accuracy: 0.7984 - val_loss: 0.4756 - val_accuracy: 0.7407\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.4664 - accuracy: 0.7741 - val_loss: 0.4642 - val_accuracy: 0.7863\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4568 - accuracy: 0.7886 - val_loss: 0.4461 - val_accuracy: 0.7863\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.4553 - accuracy: 0.8031 - val_loss: 0.4684 - val_accuracy: 0.7552\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.4583 - accuracy: 0.7959 - val_loss: 0.5313 - val_accuracy: 0.7282\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.4667 - accuracy: 0.7876 - val_loss: 0.4519 - val_accuracy: 0.7967\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.4503 - accuracy: 0.8031 - val_loss: 0.4353 - val_accuracy: 0.7967\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.4405 - accuracy: 0.8083 - val_loss: 0.4676 - val_accuracy: 0.7656\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.4345 - accuracy: 0.8062 - val_loss: 0.4411 - val_accuracy: 0.7863\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4405 - accuracy: 0.8078 - val_loss: 0.4951 - val_accuracy: 0.7490\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.4399 - accuracy: 0.7979 - val_loss: 0.4537 - val_accuracy: 0.8008\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.4339 - accuracy: 0.8119 - val_loss: 0.4525 - val_accuracy: 0.7822\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 0.4359 - accuracy: 0.8207 - val_loss: 0.4289 - val_accuracy: 0.7905\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 6s 106ms/step - loss: 0.4359 - accuracy: 0.7964 - val_loss: 0.4332 - val_accuracy: 0.7988\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.4319 - accuracy: 0.8155 - val_loss: 0.4219 - val_accuracy: 0.8008\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.4141 - accuracy: 0.8109 - val_loss: 0.4348 - val_accuracy: 0.8008\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.4172 - accuracy: 0.8093 - val_loss: 0.4161 - val_accuracy: 0.8112\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.4252 - accuracy: 0.8124 - val_loss: 0.4365 - val_accuracy: 0.8071\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.4117 - accuracy: 0.8275 - val_loss: 0.4469 - val_accuracy: 0.8029\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 5s 78ms/step - loss: 0.4285 - accuracy: 0.8093 - val_loss: 0.4436 - val_accuracy: 0.7676\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.4215 - accuracy: 0.8124 - val_loss: 0.4750 - val_accuracy: 0.7718\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 7s 123ms/step - loss: 0.4340 - accuracy: 0.8026 - val_loss: 0.4380 - val_accuracy: 0.8029\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.4261 - accuracy: 0.8031 - val_loss: 0.4513 - val_accuracy: 0.7801\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 7s 107ms/step - loss: 0.3999 - accuracy: 0.8316 - val_loss: 0.4661 - val_accuracy: 0.7801\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3972 - accuracy: 0.8321 - val_loss: 0.4407 - val_accuracy: 0.7967\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.4170 - accuracy: 0.8316 - val_loss: 0.4644 - val_accuracy: 0.7946\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.3944 - accuracy: 0.8321 - val_loss: 0.4455 - val_accuracy: 0.8071\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.4106 - accuracy: 0.8223 - val_loss: 0.4333 - val_accuracy: 0.7884\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.3876 - accuracy: 0.8378 - val_loss: 0.4395 - val_accuracy: 0.7905\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3760 - accuracy: 0.8456 - val_loss: 0.4445 - val_accuracy: 0.8112\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.3799 - accuracy: 0.8399 - val_loss: 0.5036 - val_accuracy: 0.7552\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.4036 - accuracy: 0.8207 - val_loss: 0.4615 - val_accuracy: 0.7801\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3754 - accuracy: 0.8358 - val_loss: 0.4416 - val_accuracy: 0.7842\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3616 - accuracy: 0.8518 - val_loss: 0.4705 - val_accuracy: 0.7967\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.4505 - accuracy: 0.7979 - val_loss: 0.4406 - val_accuracy: 0.8029\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.4133 - accuracy: 0.8104 - val_loss: 0.4493 - val_accuracy: 0.7884\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3695 - accuracy: 0.8477 - val_loss: 0.4305 - val_accuracy: 0.8091\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3700 - accuracy: 0.8420 - val_loss: 0.4473 - val_accuracy: 0.7925\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3631 - accuracy: 0.8487 - val_loss: 0.4293 - val_accuracy: 0.7884\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3688 - accuracy: 0.8383 - val_loss: 0.4195 - val_accuracy: 0.7988\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3680 - accuracy: 0.8497 - val_loss: 0.4332 - val_accuracy: 0.7905\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.3500 - accuracy: 0.8544 - val_loss: 0.4293 - val_accuracy: 0.7905\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3553 - accuracy: 0.8472 - val_loss: 0.4489 - val_accuracy: 0.7925\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.3683 - accuracy: 0.8373 - val_loss: 0.4687 - val_accuracy: 0.8091\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.3611 - accuracy: 0.8508 - val_loss: 0.5367 - val_accuracy: 0.7573\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3708 - accuracy: 0.8487 - val_loss: 0.4441 - val_accuracy: 0.7967\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3697 - accuracy: 0.8420 - val_loss: 0.4379 - val_accuracy: 0.8029\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.3406 - accuracy: 0.8585 - val_loss: 0.4374 - val_accuracy: 0.7967\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.3422 - accuracy: 0.8689 - val_loss: 0.5095 - val_accuracy: 0.7801\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.3534 - accuracy: 0.8440 - val_loss: 0.4410 - val_accuracy: 0.8050\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3410 - accuracy: 0.8565 - val_loss: 0.4649 - val_accuracy: 0.8091\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3496 - accuracy: 0.8539 - val_loss: 0.4353 - val_accuracy: 0.7697\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.3328 - accuracy: 0.8653 - val_loss: 0.4535 - val_accuracy: 0.7863\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.3159 - accuracy: 0.8746 - val_loss: 0.5730 - val_accuracy: 0.7718\n",
      "Epoch 105/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3668 - accuracy: 0.8518 - val_loss: 0.4538 - val_accuracy: 0.8008\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.3182 - accuracy: 0.8715 - val_loss: 0.4679 - val_accuracy: 0.7988\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3106 - accuracy: 0.8756 - val_loss: 0.4837 - val_accuracy: 0.7925\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.3112 - accuracy: 0.8705 - val_loss: 0.4525 - val_accuracy: 0.7925\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3303 - accuracy: 0.8679 - val_loss: 0.5066 - val_accuracy: 0.8050\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3395 - accuracy: 0.8632 - val_loss: 0.4366 - val_accuracy: 0.7925\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.3076 - accuracy: 0.8870 - val_loss: 0.5128 - val_accuracy: 0.7967\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3029 - accuracy: 0.8756 - val_loss: 0.5138 - val_accuracy: 0.7988\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.3015 - accuracy: 0.8798 - val_loss: 0.4931 - val_accuracy: 0.8008\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3230 - accuracy: 0.8674 - val_loss: 0.4752 - val_accuracy: 0.8050\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.3003 - accuracy: 0.8834 - val_loss: 0.5208 - val_accuracy: 0.7988\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3024 - accuracy: 0.8798 - val_loss: 0.5218 - val_accuracy: 0.7718\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3963 - accuracy: 0.8264 - val_loss: 0.4617 - val_accuracy: 0.7780\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3137 - accuracy: 0.8746 - val_loss: 0.4973 - val_accuracy: 0.8008\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.2927 - accuracy: 0.8876 - val_loss: 0.5309 - val_accuracy: 0.8154\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.3010 - accuracy: 0.8876 - val_loss: 0.4910 - val_accuracy: 0.7967\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2955 - accuracy: 0.8948 - val_loss: 0.5906 - val_accuracy: 0.7842\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.3328 - accuracy: 0.8705 - val_loss: 0.4994 - val_accuracy: 0.7739\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.2921 - accuracy: 0.8917 - val_loss: 0.4988 - val_accuracy: 0.8029\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.2871 - accuracy: 0.8995 - val_loss: 0.5034 - val_accuracy: 0.7946\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 6s 104ms/step - loss: 0.2761 - accuracy: 0.8922 - val_loss: 0.5159 - val_accuracy: 0.8071\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2625 - accuracy: 0.9000 - val_loss: 0.5949 - val_accuracy: 0.8050\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.2723 - accuracy: 0.8990 - val_loss: 0.5324 - val_accuracy: 0.8050\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 7s 106ms/step - loss: 0.2854 - accuracy: 0.8902 - val_loss: 0.4714 - val_accuracy: 0.8091\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.2738 - accuracy: 0.9021 - val_loss: 0.5267 - val_accuracy: 0.7925\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.2720 - accuracy: 0.8990 - val_loss: 0.4759 - val_accuracy: 0.7905\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.2668 - accuracy: 0.8995 - val_loss: 0.5290 - val_accuracy: 0.8133\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 6s 98ms/step - loss: 0.2746 - accuracy: 0.9000 - val_loss: 0.5041 - val_accuracy: 0.8050\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.2869 - accuracy: 0.8959 - val_loss: 0.4978 - val_accuracy: 0.8174\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.2626 - accuracy: 0.8990 - val_loss: 0.4875 - val_accuracy: 0.8216\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.2381 - accuracy: 0.9176 - val_loss: 0.5974 - val_accuracy: 0.8112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 0.2706 - accuracy: 0.9114 - val_loss: 0.5068 - val_accuracy: 0.8071\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.2659 - accuracy: 0.9010 - val_loss: 0.5719 - val_accuracy: 0.7946\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2745 - accuracy: 0.8943 - val_loss: 0.4921 - val_accuracy: 0.8237\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.2411 - accuracy: 0.9171 - val_loss: 0.5266 - val_accuracy: 0.8133\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.2409 - accuracy: 0.9109 - val_loss: 0.5153 - val_accuracy: 0.8133\n",
      "Epoch 141/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.2762 - accuracy: 0.9021 - val_loss: 0.6378 - val_accuracy: 0.7635\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - 6s 97ms/step - loss: 0.3314 - accuracy: 0.8694 - val_loss: 0.4853 - val_accuracy: 0.8050\n",
      "Epoch 143/200\n",
      "61/61 [==============================] - 6s 102ms/step - loss: 0.2832 - accuracy: 0.8870 - val_loss: 0.4487 - val_accuracy: 0.8029\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - 6s 105ms/step - loss: 0.2582 - accuracy: 0.9098 - val_loss: 0.5029 - val_accuracy: 0.8133\n",
      "Epoch 145/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.2394 - accuracy: 0.9207 - val_loss: 0.5092 - val_accuracy: 0.8133\n",
      "Epoch 146/200\n",
      "61/61 [==============================] - 6s 104ms/step - loss: 0.3225 - accuracy: 0.8850 - val_loss: 0.5202 - val_accuracy: 0.7842\n",
      "Epoch 147/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.2524 - accuracy: 0.9104 - val_loss: 0.5592 - val_accuracy: 0.8112\n",
      "Epoch 148/200\n",
      "61/61 [==============================] - 7s 108ms/step - loss: 0.2504 - accuracy: 0.9021 - val_loss: 0.5773 - val_accuracy: 0.8071\n",
      "Epoch 149/200\n",
      "61/61 [==============================] - 7s 121ms/step - loss: 0.2132 - accuracy: 0.9280 - val_loss: 0.5560 - val_accuracy: 0.8154\n",
      "Epoch 150/200\n",
      "61/61 [==============================] - 6s 99ms/step - loss: 0.2224 - accuracy: 0.9233 - val_loss: 0.5791 - val_accuracy: 0.8257\n",
      "Epoch 151/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.2427 - accuracy: 0.9192 - val_loss: 0.6570 - val_accuracy: 0.8008\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.2887 - accuracy: 0.8917 - val_loss: 0.5369 - val_accuracy: 0.8154\n",
      "Epoch 153/200\n",
      "61/61 [==============================] - 4s 66ms/step - loss: 0.2228 - accuracy: 0.9161 - val_loss: 0.6005 - val_accuracy: 0.8195\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - 6s 106ms/step - loss: 0.2615 - accuracy: 0.9010 - val_loss: 0.5334 - val_accuracy: 0.8091\n",
      "Epoch 155/200\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 0.2313 - accuracy: 0.9155 - val_loss: 0.6012 - val_accuracy: 0.8112\n",
      "Epoch 156/200\n",
      "61/61 [==============================] - 7s 109ms/step - loss: 0.2308 - accuracy: 0.9166 - val_loss: 0.5851 - val_accuracy: 0.8008\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.2296 - accuracy: 0.9119 - val_loss: 0.5898 - val_accuracy: 0.8112\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1941 - accuracy: 0.9337 - val_loss: 0.6367 - val_accuracy: 0.7988\n",
      "Epoch 159/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.1987 - accuracy: 0.9295 - val_loss: 0.6267 - val_accuracy: 0.8112\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - 6s 94ms/step - loss: 0.1928 - accuracy: 0.9358 - val_loss: 0.6659 - val_accuracy: 0.8195\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - 6s 95ms/step - loss: 0.1936 - accuracy: 0.9337 - val_loss: 0.7780 - val_accuracy: 0.7967\n",
      "Epoch 162/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.2283 - accuracy: 0.9238 - val_loss: 0.6071 - val_accuracy: 0.7967\n",
      "Epoch 163/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2480 - accuracy: 0.9104 - val_loss: 0.6065 - val_accuracy: 0.8029\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.2448 - accuracy: 0.9041 - val_loss: 0.5482 - val_accuracy: 0.8029\n",
      "Epoch 165/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.2542 - accuracy: 0.9016 - val_loss: 0.5753 - val_accuracy: 0.8174\n",
      "Epoch 166/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.2164 - accuracy: 0.9212 - val_loss: 0.5425 - val_accuracy: 0.8029\n",
      "Epoch 167/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2109 - accuracy: 0.9212 - val_loss: 0.6857 - val_accuracy: 0.8112\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - 5s 80ms/step - loss: 0.1988 - accuracy: 0.9316 - val_loss: 0.6094 - val_accuracy: 0.8174\n",
      "Epoch 169/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.2134 - accuracy: 0.9207 - val_loss: 0.6060 - val_accuracy: 0.8091\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.1811 - accuracy: 0.9358 - val_loss: 0.7840 - val_accuracy: 0.7905\n",
      "Epoch 171/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.1861 - accuracy: 0.9301 - val_loss: 0.7414 - val_accuracy: 0.8112\n",
      "Epoch 172/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.1746 - accuracy: 0.9440 - val_loss: 0.7100 - val_accuracy: 0.8133\n",
      "Epoch 173/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.2612 - accuracy: 0.9135 - val_loss: 0.6435 - val_accuracy: 0.8050\n",
      "Epoch 174/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.1899 - accuracy: 0.9337 - val_loss: 0.7100 - val_accuracy: 0.8071\n",
      "Epoch 175/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.1809 - accuracy: 0.9301 - val_loss: 0.7130 - val_accuracy: 0.8174\n",
      "Epoch 176/200\n",
      "61/61 [==============================] - 5s 81ms/step - loss: 0.1720 - accuracy: 0.9466 - val_loss: 0.7702 - val_accuracy: 0.8112\n",
      "Epoch 177/200\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 0.1616 - accuracy: 0.9487 - val_loss: 0.7535 - val_accuracy: 0.8112\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.1945 - accuracy: 0.9285 - val_loss: 0.7536 - val_accuracy: 0.8071\n",
      "Epoch 179/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.2084 - accuracy: 0.9295 - val_loss: 0.6710 - val_accuracy: 0.7905\n",
      "Epoch 180/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2264 - accuracy: 0.9197 - val_loss: 0.7080 - val_accuracy: 0.7884\n",
      "Epoch 181/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.1949 - accuracy: 0.9311 - val_loss: 0.7225 - val_accuracy: 0.8071\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.1587 - accuracy: 0.9430 - val_loss: 0.8139 - val_accuracy: 0.8112\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 0.1820 - accuracy: 0.9352 - val_loss: 0.6478 - val_accuracy: 0.8278\n",
      "Epoch 184/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.2083 - accuracy: 0.9259 - val_loss: 0.7069 - val_accuracy: 0.8029\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.1926 - accuracy: 0.9249 - val_loss: 0.6728 - val_accuracy: 0.8029\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 0.1553 - accuracy: 0.9404 - val_loss: 0.7606 - val_accuracy: 0.8029\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 0.2229 - accuracy: 0.9202 - val_loss: 0.5822 - val_accuracy: 0.8112\n",
      "Epoch 188/200\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 0.2280 - accuracy: 0.9098 - val_loss: 0.6798 - val_accuracy: 0.8029\n",
      "Epoch 189/200\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 0.1890 - accuracy: 0.9301 - val_loss: 0.6341 - val_accuracy: 0.8071\n",
      "Epoch 190/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.1735 - accuracy: 0.9368 - val_loss: 0.6496 - val_accuracy: 0.8133\n",
      "Epoch 191/200\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.1833 - accuracy: 0.9337 - val_loss: 0.6942 - val_accuracy: 0.8071\n",
      "Epoch 192/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.1434 - accuracy: 0.9508 - val_loss: 0.8263 - val_accuracy: 0.8133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      "61/61 [==============================] - 5s 84ms/step - loss: 0.1576 - accuracy: 0.9430 - val_loss: 0.8455 - val_accuracy: 0.8091\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 0.1728 - accuracy: 0.9368 - val_loss: 0.9270 - val_accuracy: 0.8050\n",
      "Epoch 195/200\n",
      "61/61 [==============================] - 5s 79ms/step - loss: 0.1640 - accuracy: 0.9451 - val_loss: 0.8831 - val_accuracy: 0.7884\n",
      "Epoch 196/200\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 0.2275 - accuracy: 0.9078 - val_loss: 0.7049 - val_accuracy: 0.7801\n",
      "Epoch 197/200\n",
      "61/61 [==============================] - 6s 100ms/step - loss: 0.1687 - accuracy: 0.9378 - val_loss: 0.7964 - val_accuracy: 0.7925\n",
      "Epoch 198/200\n",
      "61/61 [==============================] - 6s 101ms/step - loss: 0.1336 - accuracy: 0.9518 - val_loss: 0.8516 - val_accuracy: 0.7946\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 0.1309 - accuracy: 0.9580 - val_loss: 0.8922 - val_accuracy: 0.8174\n",
      "Epoch 200/200\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 0.1858 - accuracy: 0.9316 - val_loss: 0.6754 - val_accuracy: 0.7884\n",
      "16/16 [==============================] - 4s 22ms/step\n",
      "Fold score (Accuracy score): 0.7883817427385892\n",
      "Matriz de confusion\n",
      "-------------------\n",
      "---------| Verdadero | Falso |\n",
      "Verdadero|  178        43\n",
      "Falso    |  59        202\n",
      "Fold #5\n",
      "Train - X:(1930, 40) y:(1930, 2)\n",
      "Test - X:(482, 40) y:(482, 2)\n",
      "Epoch 1/200\n",
      "61/61 [==============================] - 77s 168ms/step - loss: 0.6866 - accuracy: 0.5358 - val_loss: 0.6694 - val_accuracy: 0.6079\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.6484 - accuracy: 0.6233 - val_loss: 0.6290 - val_accuracy: 0.6535\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.6346 - accuracy: 0.6585 - val_loss: 0.6130 - val_accuracy: 0.6660\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.6074 - accuracy: 0.6855 - val_loss: 0.5980 - val_accuracy: 0.6826\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 7s 111ms/step - loss: 0.6037 - accuracy: 0.6959 - val_loss: 0.5941 - val_accuracy: 0.6784\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 8s 127ms/step - loss: 0.5946 - accuracy: 0.7062 - val_loss: 0.5706 - val_accuracy: 0.7012\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 7s 122ms/step - loss: 0.5789 - accuracy: 0.7073 - val_loss: 0.5701 - val_accuracy: 0.6929\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.5673 - accuracy: 0.7202 - val_loss: 0.6039 - val_accuracy: 0.6556\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 8s 128ms/step - loss: 0.5654 - accuracy: 0.7202 - val_loss: 0.5537 - val_accuracy: 0.7012\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.5629 - accuracy: 0.7130 - val_loss: 0.5440 - val_accuracy: 0.7241\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.5531 - accuracy: 0.7295 - val_loss: 0.5432 - val_accuracy: 0.7158\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 7s 122ms/step - loss: 0.5491 - accuracy: 0.7295 - val_loss: 0.5551 - val_accuracy: 0.6992\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 8s 124ms/step - loss: 0.5484 - accuracy: 0.7280 - val_loss: 0.5440 - val_accuracy: 0.7012\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.5401 - accuracy: 0.7295 - val_loss: 0.5394 - val_accuracy: 0.7012\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.5391 - accuracy: 0.7311 - val_loss: 0.5217 - val_accuracy: 0.7365\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 8s 134ms/step - loss: 0.5347 - accuracy: 0.7389 - val_loss: 0.5191 - val_accuracy: 0.7344\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.5330 - accuracy: 0.7363 - val_loss: 0.5088 - val_accuracy: 0.7427\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.5163 - accuracy: 0.7534 - val_loss: 0.5104 - val_accuracy: 0.7386\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 7s 118ms/step - loss: 0.5182 - accuracy: 0.7554 - val_loss: 0.5046 - val_accuracy: 0.7448\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 8s 131ms/step - loss: 0.5272 - accuracy: 0.7575 - val_loss: 0.4978 - val_accuracy: 0.7510\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 8s 124ms/step - loss: 0.5102 - accuracy: 0.7627 - val_loss: 0.4927 - val_accuracy: 0.7573\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - 7s 120ms/step - loss: 0.5184 - accuracy: 0.7503 - val_loss: 0.5087 - val_accuracy: 0.7282\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 8s 125ms/step - loss: 0.5169 - accuracy: 0.7523 - val_loss: 0.5012 - val_accuracy: 0.7344\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.5003 - accuracy: 0.7674 - val_loss: 0.4896 - val_accuracy: 0.7552\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.5062 - accuracy: 0.7674 - val_loss: 0.4945 - val_accuracy: 0.7365\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.5140 - accuracy: 0.7539 - val_loss: 0.4801 - val_accuracy: 0.7469\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.5011 - accuracy: 0.7720 - val_loss: 0.4804 - val_accuracy: 0.7469\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.5008 - accuracy: 0.7648 - val_loss: 0.4858 - val_accuracy: 0.7427\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.5053 - accuracy: 0.7601 - val_loss: 0.4807 - val_accuracy: 0.7697\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.5077 - accuracy: 0.7663 - val_loss: 0.4890 - val_accuracy: 0.7324\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.4894 - accuracy: 0.7694 - val_loss: 0.5536 - val_accuracy: 0.6805\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.5123 - accuracy: 0.7487 - val_loss: 0.5092 - val_accuracy: 0.7303\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 8s 125ms/step - loss: 0.4956 - accuracy: 0.7725 - val_loss: 0.4709 - val_accuracy: 0.7490\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 7s 121ms/step - loss: 0.4796 - accuracy: 0.7881 - val_loss: 0.4638 - val_accuracy: 0.7593\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.4805 - accuracy: 0.7746 - val_loss: 0.4759 - val_accuracy: 0.7407\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 8s 127ms/step - loss: 0.4744 - accuracy: 0.7793 - val_loss: 0.4576 - val_accuracy: 0.7531\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.4669 - accuracy: 0.7855 - val_loss: 0.4381 - val_accuracy: 0.7739\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.4701 - accuracy: 0.7881 - val_loss: 0.4457 - val_accuracy: 0.7614\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.4773 - accuracy: 0.7772 - val_loss: 0.4647 - val_accuracy: 0.7510\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.4866 - accuracy: 0.7596 - val_loss: 0.4940 - val_accuracy: 0.7282\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.4603 - accuracy: 0.7896 - val_loss: 0.5057 - val_accuracy: 0.7531\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.4776 - accuracy: 0.7850 - val_loss: 0.4526 - val_accuracy: 0.7697\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 0.4705 - accuracy: 0.7798 - val_loss: 0.4324 - val_accuracy: 0.7718\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.4610 - accuracy: 0.7798 - val_loss: 0.4366 - val_accuracy: 0.7656\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 8s 123ms/step - loss: 0.4707 - accuracy: 0.7813 - val_loss: 0.4296 - val_accuracy: 0.7697\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.4508 - accuracy: 0.7839 - val_loss: 0.4540 - val_accuracy: 0.7510\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.4427 - accuracy: 0.7834 - val_loss: 0.5243 - val_accuracy: 0.7324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200\n",
      "61/61 [==============================] - 7s 123ms/step - loss: 0.4476 - accuracy: 0.7793 - val_loss: 0.4615 - val_accuracy: 0.7448\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 8s 127ms/step - loss: 0.4493 - accuracy: 0.7808 - val_loss: 0.4391 - val_accuracy: 0.7635\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.4420 - accuracy: 0.7948 - val_loss: 0.5406 - val_accuracy: 0.7407\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.4489 - accuracy: 0.7896 - val_loss: 0.4220 - val_accuracy: 0.7676\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.4202 - accuracy: 0.8005 - val_loss: 0.4269 - val_accuracy: 0.7759\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.4326 - accuracy: 0.8021 - val_loss: 0.4186 - val_accuracy: 0.7801\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 8s 131ms/step - loss: 0.4207 - accuracy: 0.8062 - val_loss: 0.4166 - val_accuracy: 0.7759\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 8s 124ms/step - loss: 0.4201 - accuracy: 0.8010 - val_loss: 0.4416 - val_accuracy: 0.7676\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 7s 120ms/step - loss: 0.4125 - accuracy: 0.8036 - val_loss: 0.4256 - val_accuracy: 0.7884\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.4103 - accuracy: 0.8041 - val_loss: 0.4206 - val_accuracy: 0.7863\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - 7s 110ms/step - loss: 0.4162 - accuracy: 0.8073 - val_loss: 0.5117 - val_accuracy: 0.7822\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 11s 174ms/step - loss: 0.4222 - accuracy: 0.7912 - val_loss: 0.4898 - val_accuracy: 0.7676\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 0.4199 - accuracy: 0.7974 - val_loss: 0.4622 - val_accuracy: 0.7427\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 0.4206 - accuracy: 0.7959 - val_loss: 0.4355 - val_accuracy: 0.7967\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.4041 - accuracy: 0.8031 - val_loss: 0.4373 - val_accuracy: 0.7842\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.4260 - accuracy: 0.7870 - val_loss: 0.4330 - val_accuracy: 0.7697\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 8s 128ms/step - loss: 0.4001 - accuracy: 0.8098 - val_loss: 0.4542 - val_accuracy: 0.7718\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 7s 118ms/step - loss: 0.4184 - accuracy: 0.7922 - val_loss: 0.4257 - val_accuracy: 0.7925\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 8s 125ms/step - loss: 0.3859 - accuracy: 0.8161 - val_loss: 0.4377 - val_accuracy: 0.7635\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.3985 - accuracy: 0.8031 - val_loss: 0.4005 - val_accuracy: 0.7988\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 7s 117ms/step - loss: 0.4051 - accuracy: 0.7974 - val_loss: 0.4458 - val_accuracy: 0.7676\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.3735 - accuracy: 0.8109 - val_loss: 0.4575 - val_accuracy: 0.8050\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.4088 - accuracy: 0.8010 - val_loss: 0.4599 - val_accuracy: 0.7573\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 8s 128ms/step - loss: 0.3868 - accuracy: 0.8109 - val_loss: 0.4529 - val_accuracy: 0.7780\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 8s 128ms/step - loss: 0.3851 - accuracy: 0.8114 - val_loss: 0.5252 - val_accuracy: 0.7967\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 8s 125ms/step - loss: 0.3701 - accuracy: 0.8171 - val_loss: 0.4309 - val_accuracy: 0.7842\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.3773 - accuracy: 0.8212 - val_loss: 0.4652 - val_accuracy: 0.7822\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.3674 - accuracy: 0.8212 - val_loss: 0.4196 - val_accuracy: 0.7946\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.4177 - accuracy: 0.7943 - val_loss: 0.4491 - val_accuracy: 0.7718\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.4039 - accuracy: 0.7891 - val_loss: 0.4210 - val_accuracy: 0.8008\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.3853 - accuracy: 0.8078 - val_loss: 0.4643 - val_accuracy: 0.7905\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.3985 - accuracy: 0.8078 - val_loss: 0.4399 - val_accuracy: 0.7988\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.3842 - accuracy: 0.8088 - val_loss: 0.4465 - val_accuracy: 0.7718\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.3565 - accuracy: 0.8295 - val_loss: 0.4439 - val_accuracy: 0.7863\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.3666 - accuracy: 0.8249 - val_loss: 0.4771 - val_accuracy: 0.8029\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.3577 - accuracy: 0.8259 - val_loss: 0.4308 - val_accuracy: 0.7988\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.3587 - accuracy: 0.8295 - val_loss: 0.4423 - val_accuracy: 0.8029\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.3641 - accuracy: 0.8259 - val_loss: 0.4379 - val_accuracy: 0.7739\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.3388 - accuracy: 0.8264 - val_loss: 0.4663 - val_accuracy: 0.7988\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.3538 - accuracy: 0.8249 - val_loss: 0.4177 - val_accuracy: 0.8008\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.3454 - accuracy: 0.8285 - val_loss: 0.4849 - val_accuracy: 0.7946\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.3854 - accuracy: 0.8228 - val_loss: 0.4168 - val_accuracy: 0.7988\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.3324 - accuracy: 0.8549 - val_loss: 0.4348 - val_accuracy: 0.7988\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.3533 - accuracy: 0.8492 - val_loss: 0.4328 - val_accuracy: 0.8008\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.3770 - accuracy: 0.8337 - val_loss: 0.4326 - val_accuracy: 0.7946\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.3817 - accuracy: 0.8207 - val_loss: 0.4380 - val_accuracy: 0.7905\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.3403 - accuracy: 0.8378 - val_loss: 0.4257 - val_accuracy: 0.8133\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 8s 124ms/step - loss: 0.3511 - accuracy: 0.8420 - val_loss: 0.4283 - val_accuracy: 0.7946\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.3466 - accuracy: 0.8601 - val_loss: 0.4320 - val_accuracy: 0.8112\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.3968 - accuracy: 0.8218 - val_loss: 0.4177 - val_accuracy: 0.7905\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.3338 - accuracy: 0.8368 - val_loss: 0.4338 - val_accuracy: 0.8174\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 8s 131ms/step - loss: 0.3327 - accuracy: 0.8352 - val_loss: 0.4768 - val_accuracy: 0.7988\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.3438 - accuracy: 0.8342 - val_loss: 0.4260 - val_accuracy: 0.8050\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.3541 - accuracy: 0.8275 - val_loss: 0.4398 - val_accuracy: 0.8050\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.3245 - accuracy: 0.8518 - val_loss: 0.4743 - val_accuracy: 0.7946\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 0.3280 - accuracy: 0.8420 - val_loss: 0.4439 - val_accuracy: 0.8154\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 0.3186 - accuracy: 0.8394 - val_loss: 0.4175 - val_accuracy: 0.8154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.3320 - accuracy: 0.8451 - val_loss: 0.4569 - val_accuracy: 0.8133\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.3566 - accuracy: 0.8218 - val_loss: 0.4507 - val_accuracy: 0.8008\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.3063 - accuracy: 0.8523 - val_loss: 0.4311 - val_accuracy: 0.8029\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.3133 - accuracy: 0.8492 - val_loss: 0.4473 - val_accuracy: 0.8050\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.3068 - accuracy: 0.8539 - val_loss: 0.4344 - val_accuracy: 0.8174\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.3180 - accuracy: 0.8456 - val_loss: 0.4864 - val_accuracy: 0.7925\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.3047 - accuracy: 0.8601 - val_loss: 0.4386 - val_accuracy: 0.8008\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.3174 - accuracy: 0.8477 - val_loss: 0.5566 - val_accuracy: 0.7718\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 0.3146 - accuracy: 0.8477 - val_loss: 0.4349 - val_accuracy: 0.8071\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.2949 - accuracy: 0.8788 - val_loss: 0.4723 - val_accuracy: 0.8112\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.3358 - accuracy: 0.8648 - val_loss: 0.4514 - val_accuracy: 0.8091\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.3000 - accuracy: 0.8782 - val_loss: 0.5389 - val_accuracy: 0.8008\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.2916 - accuracy: 0.8813 - val_loss: 0.4605 - val_accuracy: 0.7925\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.3305 - accuracy: 0.8782 - val_loss: 0.4426 - val_accuracy: 0.8071\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.3542 - accuracy: 0.8534 - val_loss: 0.4329 - val_accuracy: 0.8133\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.2882 - accuracy: 0.8845 - val_loss: 0.5074 - val_accuracy: 0.8008\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.3201 - accuracy: 0.8637 - val_loss: 0.4536 - val_accuracy: 0.8029\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.2942 - accuracy: 0.8653 - val_loss: 0.5518 - val_accuracy: 0.7739\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.3206 - accuracy: 0.8560 - val_loss: 0.4673 - val_accuracy: 0.7946\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.3031 - accuracy: 0.8741 - val_loss: 0.4357 - val_accuracy: 0.8071\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.2879 - accuracy: 0.8829 - val_loss: 0.4662 - val_accuracy: 0.8029\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.2843 - accuracy: 0.8824 - val_loss: 0.5274 - val_accuracy: 0.7863\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.2998 - accuracy: 0.8824 - val_loss: 0.4772 - val_accuracy: 0.7988\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.2814 - accuracy: 0.8808 - val_loss: 0.5316 - val_accuracy: 0.8008\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 0.3047 - accuracy: 0.8756 - val_loss: 0.5327 - val_accuracy: 0.7905\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.2650 - accuracy: 0.8933 - val_loss: 0.5570 - val_accuracy: 0.7988\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.2905 - accuracy: 0.8772 - val_loss: 0.5501 - val_accuracy: 0.7739\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.2784 - accuracy: 0.8860 - val_loss: 0.6380 - val_accuracy: 0.7863\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.2958 - accuracy: 0.8782 - val_loss: 0.4938 - val_accuracy: 0.8029\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 0.2552 - accuracy: 0.8870 - val_loss: 0.5388 - val_accuracy: 0.8029\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.2366 - accuracy: 0.9047 - val_loss: 0.5200 - val_accuracy: 0.7967\n",
      "Epoch 136/200\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 0.2673 - accuracy: 0.8948 - val_loss: 0.5444 - val_accuracy: 0.8091\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.2744 - accuracy: 0.8922 - val_loss: 0.5713 - val_accuracy: 0.7614\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.2777 - accuracy: 0.8793 - val_loss: 0.5765 - val_accuracy: 0.8029\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.2415 - accuracy: 0.9052 - val_loss: 0.5656 - val_accuracy: 0.8029\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 0.2716 - accuracy: 0.8824 - val_loss: 0.5013 - val_accuracy: 0.7967\n",
      "Epoch 141/200\n",
      "61/61 [==============================] - 11s 173ms/step - loss: 0.2483 - accuracy: 0.9057 - val_loss: 0.5504 - val_accuracy: 0.7988\n",
      "Epoch 142/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.2787 - accuracy: 0.8881 - val_loss: 0.5278 - val_accuracy: 0.7635\n",
      "Epoch 143/200\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 0.2816 - accuracy: 0.8922 - val_loss: 0.6660 - val_accuracy: 0.7739\n",
      "Epoch 144/200\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.2774 - accuracy: 0.8881 - val_loss: 0.5223 - val_accuracy: 0.7925\n",
      "Epoch 145/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.2580 - accuracy: 0.9021 - val_loss: 0.5895 - val_accuracy: 0.7842\n",
      "Epoch 146/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.2584 - accuracy: 0.9031 - val_loss: 0.6667 - val_accuracy: 0.7905\n",
      "Epoch 147/200\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.2526 - accuracy: 0.9010 - val_loss: 0.6033 - val_accuracy: 0.7925\n",
      "Epoch 148/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.2867 - accuracy: 0.8798 - val_loss: 0.5352 - val_accuracy: 0.8154\n",
      "Epoch 149/200\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.2494 - accuracy: 0.8984 - val_loss: 0.5456 - val_accuracy: 0.7925\n",
      "Epoch 150/200\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.2377 - accuracy: 0.9119 - val_loss: 0.6263 - val_accuracy: 0.7884\n",
      "Epoch 151/200\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.2870 - accuracy: 0.8938 - val_loss: 0.5222 - val_accuracy: 0.7801\n",
      "Epoch 152/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.2575 - accuracy: 0.9021 - val_loss: 0.5448 - val_accuracy: 0.7842\n",
      "Epoch 153/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.2862 - accuracy: 0.8829 - val_loss: 0.5449 - val_accuracy: 0.8050\n",
      "Epoch 154/200\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.2789 - accuracy: 0.8850 - val_loss: 0.5049 - val_accuracy: 0.8050\n",
      "Epoch 155/200\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.2315 - accuracy: 0.9078 - val_loss: 0.5850 - val_accuracy: 0.7988\n",
      "Epoch 156/200\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 0.2476 - accuracy: 0.9093 - val_loss: 0.5966 - val_accuracy: 0.7967\n",
      "Epoch 157/200\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.2596 - accuracy: 0.9010 - val_loss: 0.5523 - val_accuracy: 0.8029\n",
      "Epoch 158/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.2939 - accuracy: 0.8819 - val_loss: 0.5794 - val_accuracy: 0.8008\n",
      "Epoch 159/200\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.2411 - accuracy: 0.9098 - val_loss: 0.6244 - val_accuracy: 0.7863\n",
      "Epoch 160/200\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.2276 - accuracy: 0.9062 - val_loss: 0.6174 - val_accuracy: 0.7925\n",
      "Epoch 161/200\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.2154 - accuracy: 0.9181 - val_loss: 0.6449 - val_accuracy: 0.7946\n",
      "Epoch 162/200\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.2234 - accuracy: 0.9176 - val_loss: 0.6050 - val_accuracy: 0.7925\n",
      "Epoch 163/200\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.2327 - accuracy: 0.9119 - val_loss: 0.7375 - val_accuracy: 0.7905\n",
      "Epoch 164/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.2218 - accuracy: 0.9150 - val_loss: 0.5913 - val_accuracy: 0.8029\n",
      "Epoch 165/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.2423 - accuracy: 0.9036 - val_loss: 0.5873 - val_accuracy: 0.7739\n",
      "Epoch 166/200\n",
      "61/61 [==============================] - 8s 134ms/step - loss: 0.2348 - accuracy: 0.9130 - val_loss: 0.6620 - val_accuracy: 0.7842\n",
      "Epoch 167/200\n",
      "61/61 [==============================] - 8s 124ms/step - loss: 0.2499 - accuracy: 0.9109 - val_loss: 0.6168 - val_accuracy: 0.8029\n",
      "Epoch 168/200\n",
      "61/61 [==============================] - 8s 131ms/step - loss: 0.2335 - accuracy: 0.9135 - val_loss: 0.5992 - val_accuracy: 0.7946\n",
      "Epoch 169/200\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.2226 - accuracy: 0.9104 - val_loss: 0.6258 - val_accuracy: 0.7863\n",
      "Epoch 170/200\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 0.2442 - accuracy: 0.9047 - val_loss: 0.6276 - val_accuracy: 0.7842\n",
      "Epoch 171/200\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.2379 - accuracy: 0.9026 - val_loss: 0.5146 - val_accuracy: 0.7967\n",
      "Epoch 172/200\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.2001 - accuracy: 0.9223 - val_loss: 0.6196 - val_accuracy: 0.8071\n",
      "Epoch 173/200\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.2301 - accuracy: 0.9166 - val_loss: 0.5728 - val_accuracy: 0.8050\n",
      "Epoch 174/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.1770 - accuracy: 0.9332 - val_loss: 0.6442 - val_accuracy: 0.8029\n",
      "Epoch 175/200\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.2078 - accuracy: 0.9223 - val_loss: 0.6403 - val_accuracy: 0.8008\n",
      "Epoch 176/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.2412 - accuracy: 0.9078 - val_loss: 0.6860 - val_accuracy: 0.8112\n",
      "Epoch 177/200\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 0.1997 - accuracy: 0.9244 - val_loss: 0.6099 - val_accuracy: 0.8008\n",
      "Epoch 178/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.2134 - accuracy: 0.9280 - val_loss: 0.6302 - val_accuracy: 0.7822\n",
      "Epoch 179/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.2083 - accuracy: 0.9238 - val_loss: 0.6597 - val_accuracy: 0.8050\n",
      "Epoch 180/200\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.2028 - accuracy: 0.9264 - val_loss: 0.7607 - val_accuracy: 0.7905\n",
      "Epoch 181/200\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.2024 - accuracy: 0.9259 - val_loss: 0.6772 - val_accuracy: 0.7988\n",
      "Epoch 182/200\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.1658 - accuracy: 0.9347 - val_loss: 0.6568 - val_accuracy: 0.8050\n",
      "Epoch 183/200\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.1775 - accuracy: 0.9358 - val_loss: 0.6498 - val_accuracy: 0.7925\n",
      "Epoch 184/200\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.2990 - accuracy: 0.8824 - val_loss: 0.5240 - val_accuracy: 0.8050\n",
      "Epoch 185/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.3001 - accuracy: 0.8632 - val_loss: 0.5717 - val_accuracy: 0.7656\n",
      "Epoch 186/200\n",
      "61/61 [==============================] - 9s 139ms/step - loss: 0.2435 - accuracy: 0.9026 - val_loss: 0.6190 - val_accuracy: 0.7801\n",
      "Epoch 187/200\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 0.1991 - accuracy: 0.9197 - val_loss: 0.7201 - val_accuracy: 0.7780\n",
      "Epoch 188/200\n",
      "61/61 [==============================] - 10s 162ms/step - loss: 0.2488 - accuracy: 0.8984 - val_loss: 0.5879 - val_accuracy: 0.8008\n",
      "Epoch 189/200\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 0.1979 - accuracy: 0.9301 - val_loss: 0.6274 - val_accuracy: 0.7884\n",
      "Epoch 190/200\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.1930 - accuracy: 0.9342 - val_loss: 0.6578 - val_accuracy: 0.7905\n",
      "Epoch 191/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.1796 - accuracy: 0.9389 - val_loss: 0.7181 - val_accuracy: 0.7884\n",
      "Epoch 192/200\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.2110 - accuracy: 0.9202 - val_loss: 0.6920 - val_accuracy: 0.7801\n",
      "Epoch 193/200\n",
      "61/61 [==============================] - 10s 162ms/step - loss: 0.1952 - accuracy: 0.9280 - val_loss: 0.6590 - val_accuracy: 0.7863\n",
      "Epoch 194/200\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.1866 - accuracy: 0.9337 - val_loss: 0.6935 - val_accuracy: 0.7988\n",
      "Epoch 195/200\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 0.1780 - accuracy: 0.9394 - val_loss: 0.7251 - val_accuracy: 0.8112\n",
      "Epoch 196/200\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 0.1700 - accuracy: 0.9352 - val_loss: 0.7079 - val_accuracy: 0.8112\n",
      "Epoch 197/200\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.1683 - accuracy: 0.9456 - val_loss: 0.7588 - val_accuracy: 0.7988\n",
      "Epoch 198/200\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.2015 - accuracy: 0.9249 - val_loss: 0.7823 - val_accuracy: 0.7614\n",
      "Epoch 199/200\n",
      "61/61 [==============================] - 8s 130ms/step - loss: 0.2065 - accuracy: 0.9238 - val_loss: 0.7390 - val_accuracy: 0.8008\n",
      "Epoch 200/200\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.1950 - accuracy: 0.9249 - val_loss: 0.6885 - val_accuracy: 0.7822\n",
      "16/16 [==============================] - 5s 40ms/step\n",
      "Fold score (Accuracy score): 0.7821576763485477\n",
      "Matriz de confusion\n",
      "-------------------\n",
      "---------| Verdadero | Falso |\n",
      "Verdadero|  179        42\n",
      "Falso    |  63        198\n",
      "-----------------------\n",
      "Cross-validated score (Accuracy score): 0.8009950248756219\n",
      "-----------------------\n",
      "Resumen\n",
      "Fold score (Accuracy score): 0.8281573498964804\n",
      "Fold score (Accuracy score): 0.8074534161490683\n",
      "Fold score (Accuracy score): 0.7987551867219918\n",
      "Fold score (Accuracy score): 0.7883817427385892\n",
      "Fold score (Accuracy score): 0.7821576763485477\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(5)\n",
    "fold = 0\n",
    "y_tests = []\n",
    "predictions = []\n",
    "scores = []\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "for train, test in k_fold.split(X):\n",
    "    fold = fold + 1\n",
    "    print(f\"Fold #{fold}\")\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    print(f\"Train - X:{X_train.shape} y:{y_train.shape}\")\n",
    "    print(f\"Test - X:{X_test.shape} y:{y_test.shape}\")\n",
    "    \n",
    "    num_labels = y.shape[1]\n",
    "    dim_entrada = (X_train.shape[1],1)\n",
    "\n",
    "    #model = new_RNN()\n",
    "    model = new_RNN()\n",
    "    callbacks = []\n",
    "    '''\n",
    "    callbacks = [\n",
    "    EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=1e-2,\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "        )\n",
    "    ]\n",
    "    '''\n",
    "    num_epochs = 200\n",
    "    num_batch_size = 32\n",
    "    start = datetime.datetime.now()\n",
    "   \n",
    "    results = model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test),callbacks=callbacks)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    y_tests.append(y_test)\n",
    "    pred=[([1,0] if i[0]>i[1] else [0,1]) for i in pred]\n",
    "    predictions.append(pred)\n",
    "    score = metrics.accuracy_score(pred, y_test)\n",
    "    math = confusion_matrix([(1 if x[0]==1 else 0) for x in pred],[(1 if x[0]==1 else 0) for x in y_test], labels=[1,0])\n",
    "    scores.append([score])\n",
    "    print(f\"Fold score (Accuracy score): {score}\")\n",
    "    print(\"Matriz de confusion\")\n",
    "    print(\"-------------------\")\n",
    "    print(\"---------| Verdadero | Falso |\")\n",
    "    print(f\"Verdadero|  {math[0][0]}        {math[0][1]}\")\n",
    "    print(f\"Falso    |  {math[1][0]}        {math[1][1]}\")\n",
    "\n",
    "y_tests = np.concatenate(y_tests)\n",
    "predictions = np.concatenate(predictions)\n",
    "score = metrics.accuracy_score(predictions, y_tests)\n",
    "print(\"-----------------------\")\n",
    "print(f\"Cross-validated score (Accuracy score): {score}\")\n",
    "print(\"-----------------------\")\n",
    "print(\"Resumen\")\n",
    "for result in scores:\n",
    "    print(f\"Fold score (Accuracy score): {result[0]}\")\n",
    "    \n",
    "#https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_2_kfold.ipynb\n",
    "#https://www.youtube.com/watch?v=maiQf8ray_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bc2e9ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_21 (LSTM)              (None, 40, 50)            10400     \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 40, 50)            0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 40, 50)            0         \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 40, 20)            5680      \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 40, 20)            0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 40, 20)            0         \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 10)                1240      \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,342\n",
      "Trainable params: 17,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 55s 646ms/step - loss: 0.6818 - accuracy: 0.5347 - val_loss: 0.6548 - val_accuracy: 0.6017\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.6602 - accuracy: 0.6290 - val_loss: 0.6326 - val_accuracy: 0.6286\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 0.6351 - accuracy: 0.6575 - val_loss: 0.6046 - val_accuracy: 0.6577\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.6064 - accuracy: 0.6938 - val_loss: 0.5864 - val_accuracy: 0.6846\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.5896 - accuracy: 0.6876 - val_loss: 0.5726 - val_accuracy: 0.7054\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.5653 - accuracy: 0.7223 - val_loss: 0.5535 - val_accuracy: 0.7054\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 0.5531 - accuracy: 0.7192 - val_loss: 0.5462 - val_accuracy: 0.7095\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.5487 - accuracy: 0.7228 - val_loss: 0.5359 - val_accuracy: 0.7178\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 9s 146ms/step - loss: 0.5411 - accuracy: 0.7311 - val_loss: 0.5274 - val_accuracy: 0.7344\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 0.5237 - accuracy: 0.7425 - val_loss: 0.5289 - val_accuracy: 0.7324\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 0.5115 - accuracy: 0.7585 - val_loss: 0.5230 - val_accuracy: 0.7344\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.5190 - accuracy: 0.7373 - val_loss: 0.5343 - val_accuracy: 0.7178\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 8s 129ms/step - loss: 0.5118 - accuracy: 0.7596 - val_loss: 0.5307 - val_accuracy: 0.7344\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.5024 - accuracy: 0.7565 - val_loss: 0.5132 - val_accuracy: 0.7344\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 9s 156ms/step - loss: 0.5018 - accuracy: 0.7617 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 0.4982 - accuracy: 0.7653 - val_loss: 0.5251 - val_accuracy: 0.7199\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.5131 - accuracy: 0.7508 - val_loss: 0.5131 - val_accuracy: 0.7365\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.5158 - accuracy: 0.7456 - val_loss: 0.5268 - val_accuracy: 0.7365\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 10s 170ms/step - loss: 0.4858 - accuracy: 0.7689 - val_loss: 0.5333 - val_accuracy: 0.7344\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.4985 - accuracy: 0.7606 - val_loss: 0.5342 - val_accuracy: 0.7448\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.4902 - accuracy: 0.7736 - val_loss: 0.5201 - val_accuracy: 0.7407\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 9s 153ms/step - loss: 0.5020 - accuracy: 0.7642 - val_loss: 0.5271 - val_accuracy: 0.7220\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 11s 176ms/step - loss: 0.4861 - accuracy: 0.7591 - val_loss: 0.5528 - val_accuracy: 0.7303\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 10s 158ms/step - loss: 0.5021 - accuracy: 0.7513 - val_loss: 0.5213 - val_accuracy: 0.7344\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.5105 - accuracy: 0.7404 - val_loss: 0.5316 - val_accuracy: 0.7158\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 10s 163ms/step - loss: 0.4978 - accuracy: 0.7575 - val_loss: 0.5354 - val_accuracy: 0.7137\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.4886 - accuracy: 0.7674 - val_loss: 0.5300 - val_accuracy: 0.7469\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.4998 - accuracy: 0.7518 - val_loss: 0.5610 - val_accuracy: 0.6846\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.5136 - accuracy: 0.7383 - val_loss: 0.5245 - val_accuracy: 0.7344\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.4879 - accuracy: 0.7720 - val_loss: 0.5572 - val_accuracy: 0.7386\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.4825 - accuracy: 0.7684 - val_loss: 0.5403 - val_accuracy: 0.7573\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.4851 - accuracy: 0.7642 - val_loss: 0.5210 - val_accuracy: 0.7241\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 10s 166ms/step - loss: 0.4846 - accuracy: 0.7679 - val_loss: 0.5082 - val_accuracy: 0.7324\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 12s 190ms/step - loss: 0.4758 - accuracy: 0.7725 - val_loss: 0.5127 - val_accuracy: 0.7427\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 10s 168ms/step - loss: 0.4758 - accuracy: 0.7777 - val_loss: 0.5063 - val_accuracy: 0.7510\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 9s 144ms/step - loss: 0.4730 - accuracy: 0.7793 - val_loss: 0.5204 - val_accuracy: 0.7303\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 11s 183ms/step - loss: 0.4851 - accuracy: 0.7663 - val_loss: 0.5161 - val_accuracy: 0.7282\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 11s 178ms/step - loss: 0.4583 - accuracy: 0.7762 - val_loss: 0.5180 - val_accuracy: 0.7448\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 10s 165ms/step - loss: 0.4917 - accuracy: 0.7632 - val_loss: 0.5142 - val_accuracy: 0.7448\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 9s 142ms/step - loss: 0.4756 - accuracy: 0.7746 - val_loss: 0.5478 - val_accuracy: 0.7510\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.4742 - accuracy: 0.7741 - val_loss: 0.5210 - val_accuracy: 0.7344\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.4737 - accuracy: 0.7715 - val_loss: 0.4882 - val_accuracy: 0.7324\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.4637 - accuracy: 0.7803 - val_loss: 0.5088 - val_accuracy: 0.7510\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 9s 155ms/step - loss: 0.4473 - accuracy: 0.7881 - val_loss: 0.5326 - val_accuracy: 0.7448\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 10s 167ms/step - loss: 0.4529 - accuracy: 0.7834 - val_loss: 0.5030 - val_accuracy: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "61/61 [==============================] - 10s 169ms/step - loss: 0.4491 - accuracy: 0.7824 - val_loss: 0.5368 - val_accuracy: 0.7573\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.4372 - accuracy: 0.7948 - val_loss: 0.5190 - val_accuracy: 0.7158\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.4595 - accuracy: 0.7777 - val_loss: 0.5004 - val_accuracy: 0.7531\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.4526 - accuracy: 0.7850 - val_loss: 0.4928 - val_accuracy: 0.7552\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.4330 - accuracy: 0.7870 - val_loss: 0.4768 - val_accuracy: 0.7573\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 8s 126ms/step - loss: 0.4318 - accuracy: 0.7938 - val_loss: 0.4922 - val_accuracy: 0.7490\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.4518 - accuracy: 0.7886 - val_loss: 0.5336 - val_accuracy: 0.7033\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.4377 - accuracy: 0.7938 - val_loss: 0.4739 - val_accuracy: 0.7469\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.4313 - accuracy: 0.7969 - val_loss: 0.4822 - val_accuracy: 0.7573\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.4236 - accuracy: 0.8026 - val_loss: 0.4795 - val_accuracy: 0.7593\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.4334 - accuracy: 0.7855 - val_loss: 0.4732 - val_accuracy: 0.7635\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.4253 - accuracy: 0.8057 - val_loss: 0.5440 - val_accuracy: 0.7593\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.4291 - accuracy: 0.7938 - val_loss: 0.4903 - val_accuracy: 0.7780\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 10s 157ms/step - loss: 0.4315 - accuracy: 0.7917 - val_loss: 0.4749 - val_accuracy: 0.7780\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 9s 139ms/step - loss: 0.4259 - accuracy: 0.8057 - val_loss: 0.4845 - val_accuracy: 0.7510\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.4172 - accuracy: 0.8031 - val_loss: 0.4899 - val_accuracy: 0.7573\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.4125 - accuracy: 0.8109 - val_loss: 0.4783 - val_accuracy: 0.7510\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.4172 - accuracy: 0.8062 - val_loss: 0.4744 - val_accuracy: 0.7697\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 8s 134ms/step - loss: 0.4004 - accuracy: 0.8098 - val_loss: 0.4731 - val_accuracy: 0.7614\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 8s 131ms/step - loss: 0.4215 - accuracy: 0.8000 - val_loss: 0.4613 - val_accuracy: 0.7656\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 0.4215 - accuracy: 0.7959 - val_loss: 0.4593 - val_accuracy: 0.7614\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.3874 - accuracy: 0.8202 - val_loss: 0.5147 - val_accuracy: 0.7697\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.4095 - accuracy: 0.8052 - val_loss: 0.4853 - val_accuracy: 0.7365\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 8s 136ms/step - loss: 0.4169 - accuracy: 0.8041 - val_loss: 0.4621 - val_accuracy: 0.7676\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 8s 139ms/step - loss: 0.5101 - accuracy: 0.7342 - val_loss: 0.5565 - val_accuracy: 0.7386\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 8s 135ms/step - loss: 0.4211 - accuracy: 0.8016 - val_loss: 0.4719 - val_accuracy: 0.7593\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 8s 132ms/step - loss: 0.4200 - accuracy: 0.8016 - val_loss: 0.5513 - val_accuracy: 0.7780\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 8s 133ms/step - loss: 0.4076 - accuracy: 0.8067 - val_loss: 0.4978 - val_accuracy: 0.7780\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.3970 - accuracy: 0.8124 - val_loss: 0.4763 - val_accuracy: 0.7614\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 10s 156ms/step - loss: 0.3951 - accuracy: 0.8145 - val_loss: 0.4926 - val_accuracy: 0.7759\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 10s 160ms/step - loss: 0.4236 - accuracy: 0.8005 - val_loss: 0.4875 - val_accuracy: 0.7739\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 8s 134ms/step - loss: 0.3970 - accuracy: 0.8218 - val_loss: 0.4665 - val_accuracy: 0.7593\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 9s 151ms/step - loss: 0.4039 - accuracy: 0.8145 - val_loss: 0.4891 - val_accuracy: 0.7739\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.3782 - accuracy: 0.8228 - val_loss: 0.4743 - val_accuracy: 0.7780\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 0.3897 - accuracy: 0.8233 - val_loss: 0.4710 - val_accuracy: 0.7656\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 9s 141ms/step - loss: 0.4088 - accuracy: 0.8083 - val_loss: 0.4548 - val_accuracy: 0.7739\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.4045 - accuracy: 0.8052 - val_loss: 0.4677 - val_accuracy: 0.7739\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 8s 138ms/step - loss: 0.3880 - accuracy: 0.8181 - val_loss: 0.4692 - val_accuracy: 0.7863\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 8s 137ms/step - loss: 0.3859 - accuracy: 0.8249 - val_loss: 0.5242 - val_accuracy: 0.7635\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 9s 148ms/step - loss: 0.4010 - accuracy: 0.8062 - val_loss: 0.4582 - val_accuracy: 0.7946\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.3859 - accuracy: 0.8212 - val_loss: 0.4567 - val_accuracy: 0.7531\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 9s 140ms/step - loss: 0.3913 - accuracy: 0.8187 - val_loss: 0.4744 - val_accuracy: 0.7593\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.4309 - accuracy: 0.7938 - val_loss: 0.4947 - val_accuracy: 0.7510\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.4354 - accuracy: 0.7902 - val_loss: 0.4597 - val_accuracy: 0.7739\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 9s 143ms/step - loss: 0.3848 - accuracy: 0.8264 - val_loss: 0.4389 - val_accuracy: 0.7822\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.3763 - accuracy: 0.8238 - val_loss: 0.4451 - val_accuracy: 0.7988\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 9s 150ms/step - loss: 0.3974 - accuracy: 0.8166 - val_loss: 0.5243 - val_accuracy: 0.7863\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.4275 - accuracy: 0.7927 - val_loss: 0.4807 - val_accuracy: 0.7614\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 10s 161ms/step - loss: 0.3976 - accuracy: 0.8166 - val_loss: 0.4739 - val_accuracy: 0.7925\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 9s 152ms/step - loss: 0.3817 - accuracy: 0.8249 - val_loss: 0.5395 - val_accuracy: 0.7759\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 9s 149ms/step - loss: 0.3973 - accuracy: 0.8145 - val_loss: 0.4668 - val_accuracy: 0.7780\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 9s 145ms/step - loss: 0.3730 - accuracy: 0.8337 - val_loss: 0.4561 - val_accuracy: 0.7759\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 9s 154ms/step - loss: 0.3628 - accuracy: 0.8347 - val_loss: 0.4843 - val_accuracy: 0.7884\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 9s 147ms/step - loss: 0.3595 - accuracy: 0.8440 - val_loss: 0.4500 - val_accuracy: 0.8050\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 10s 159ms/step - loss: 0.3729 - accuracy: 0.8321 - val_loss: 0.4501 - val_accuracy: 0.7759\n",
      "val_loss: 0.4500848650932312 val_accuracy: 0.7759336233139038\n",
      "16/16 [==============================] - 6s 58ms/step\n",
      "El algoritmo acerto 374 veces sobre los 482 casos.\n"
     ]
    }
   ],
   "source": [
    "#50 0.77\n",
    "model = new_RNN()\n",
    "model.summary()\n",
    "#https://keras.io/api/callbacks/early_stopping/\n",
    "callbacks = []\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        min_delta=0.01, #si un epochs mejora como un min_delta respecto a la anterior, no contara como mejora\n",
    "        patience=70,#numero de epochs sin mejoras que se tendra paciencia\n",
    "        verbose=1,#mostrar informacion extra, 0 no mostrar\n",
    "     )\n",
    "]\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "results = model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test),callbacks=callbacks)\n",
    "duration = datetime.datetime.now() - start\n",
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"val_loss: {test_accuracy[0]}\", f\"val_accuracy: {test_accuracy[1]}\")\n",
    "y_values = model.predict(X_test)\n",
    "y_prediction=[([1,0] if i[0]>i[1] else [0,1]) for i in y_values]\n",
    "y_i = len(y_values)\n",
    "i = 0\n",
    "true_values = 0\n",
    "while (i < y_i):\n",
    "    true_values += (1 if (y_test[i][0] == y_prediction[i][0] or y_test[i][1] == y_prediction[i][1]) else 0)\n",
    "    i = i + 1 \n",
    "print(f\"El algoritmo acerto {true_values} veces sobre los {y_i} casos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47073eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = results.history['loss']\n",
    "test_loss = results.history['val_loss']\n",
    "epochs_range = range(1, len(training_loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb0e73ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABl4AAAKnCAYAAADnZTDVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hUddrG8XsyCaEmICUEEgkqIF1EYQGjICigIhixYPe17GKDxYqugA2sCJYVZa2rYMHYERUkGLGgIIKABaQTQFoiIAQm8/7x25M6k8xMzsyZJN/PdeU6kzNnzvwiQ3Y593mex+X1er0CAAAAAAAAAABApcU4vQAAAAAAAAAAAIDqguAFAAAAAAAAAADAJgQvAAAAAAAAAAAANiF4AQAAAAAAAAAAsAnBCwAAAAAAAAAAgE0IXgAAAAAAAAAAAGxC8AIAAAAAAAAAAGATghcAAAAAAAAAAACbxDq9gGhUUFCgLVu2qEGDBnK5XE4vBwAAAAAAAAAAOMjr9erPP/9UixYtFBNTfk0LwYsPW7ZsUWpqqtPLAAAAAAAAAAAAUWTjxo1KSUkp9xiCFx8aNGggyfwHTEhIcHg1AAAAAAAAAADASXl5eUpNTS3MD8pD8OKD1V4sISGB4AUAAAAAAAAAAEhSQONJym9EBgAAAAAAAAAAgIARvAAAAAAAAAAAANiE4AUAAAAAAAAAAMAmzHgBAAAAAAAAAFRrXq9Xhw8flsfjcXopiFJut1uxsbEBzXCpCMELAAAAAAAAAKDays/PV05Ojvbv3+/0UhDl6tatq+TkZNWqVatS5yF4AQAAAAAAAABUSwUFBVq7dq3cbrdatGihWrVq2VLRgOrF6/UqPz9ff/zxh9auXas2bdooJib0SS0ELwAAAAAAAACAaik/P18FBQVKTU1V3bp1nV4OolidOnUUFxen9evXKz8/X7Vr1w75XKFHNgAAAAAAAAAAVAGVqV5AzWHX54RPGwAAAAAAAAAAgE0IXgAAAAAAAAAAqAHS0tI0ZcqUgI/PysqSy+XSnj17wram6ojgBQAAAAAAAACAing8UlaWNHOm2Xo8YXsrl8tV7teECRNCOu93332na6+9NuDje/furZycHCUmJob0foGqbgFPrNMLAAAAAAAAAAAgqmVmSqNGSZs2Fe1LSZGmTpUyMmx/u5ycnMLHb7zxhsaNG6dffvmlcF/9+vULH3u9Xnk8HsXGVny5v2nTpkGto1atWmrevHlQrwEVLwAAAAAAAAAA+JeZKQ0fXjJ0kaTNm83+zEzb37J58+aFX4mJiXK5XIXf//zzz2rQoIE+/vhjde/eXfHx8fryyy+1Zs0aDR06VElJSapfv75OPPFEzZ07t8R5S7cac7lc+s9//qNzzjlHdevWVZs2bfT+++8XPl+6EuWll15Sw4YN9cknn6h9+/aqX7++Bg0aVCIoOnz4sG666SY1bNhQjRs31u23367LL79cw4YNC/m/x+7du3XZZZepUaNGqlu3rgYPHqzffvut8Pn169dryJAhatSokerVq6eOHTtq9uzZha+9+OKL1bRpU9WpU0dt2rTRiy++GPJaAkHwAgAAAAAAAACoefbt8/914IA5xuMxlS5eb9nXW/tGjSrZdszfOW12xx136MEHH9SqVavUpUsX7d27V2eccYbmzZunH374QYMGDdKQIUO0YcOGcs9zzz336Pzzz9eyZct0xhln6OKLL9auXbv8Hr9//349+uij+u9//6svvvhCGzZs0C233FL4/EMPPaTXXntNL774ohYuXKi8vDy9++67lfpZr7jiCn3//fd6//339fXXX8vr9eqMM87QoUOHJEnXX3+9Dh48qC+++ELLly/XQw89VFgVdPfdd2vlypX6+OOPtWrVKj3zzDNq0qRJpdZTEVqNAQAAAAAAAABqnmLtuso44wzpo4+k7OyylS7Feb3m+exsqW9fsy8tTdqxw/exNrr33nt12mmnFX5/xBFHqGvXroXf33fffXrnnXf0/vvv64YbbvB7niuuuEIjRoyQJE2cOFFPPPGEFi1apEGDBvk8/tChQ5o2bZqOPvpoSdINN9yge++9t/D5J598UmPHjtU555wjSXrqqacKq09C8dtvv+n999/XwoUL1bt3b0nSa6+9ptTUVL377rs677zztGHDBp177rnq3LmzJOmoo44qfP2GDRvUrVs3nXDCCZJM1U+4UfECAAAAAAAAAIAvxVpo2XKcjawgwbJ3717dcsstat++vRo2bKj69etr1apVFVa8dOnSpfBxvXr1lJCQoO3bt/s9vm7duoWhiyQlJycXHp+bm6tt27apR48ehc+73W517949qJ+tuFWrVik2NlY9e/Ys3Ne4cWO1a9dOq1atkiTddNNNuv/++9WnTx+NHz9ey5YtKzx25MiRev3113Xcccfptttu01dffRXyWgJF8AIAAAAAAAAAqHn27vX/9fbb5pjk5MDOVfy4det8n9Nm9erVK/H9LbfconfeeUcTJ05Udna2li5dqs6dOys/P7/c88TFxZX43uVyqaCgIKjjvTZX8wTr6quv1u+//65LL71Uy5cv1wknnKAnn3xSkjR48GCtX79e//znP7Vlyxb179+/RGu0cCB4AQAAAAAAAADUPPXq+f+qXdsck54upaRILpfvc7hcUmqqOa6i84bZwoULdcUVV+icc85R586d1bx5c61bty7s71tcYmKikpKS9N133xXu83g8WrJkScjnbN++vQ4fPqxvv/22cN/OnTv1yy+/qEOHDoX7UlNT9Y9//EOZmZm6+eabNX369MLnmjZtqssvv1yvvvqqpkyZoueeey7k9QSCGS8AAAAAAAAAAPjidktTp0rDh5uQpXhlhxXGTJlijnNYmzZtlJmZqSFDhsjlcunuu+8ut3IlXG688UZNmjRJxxxzjI499lg9+eST2r17t1z+wqtili9frgYNGhR+73K51LVrVw0dOlTXXHONnn32WTVo0EB33HGHWrZsqaFDh0qSRo8ercGDB6tt27bavXu35s+fr/bt20uSxo0bp+7du6tjx446ePCgPvzww8LnwoXgBQAAAAAAAAAAfzIypFmzpFGjpE2bivanpJjQJSPDsaUVN3nyZP3f//2fevfurSZNmuj2229XXl5exNdx++23a+vWrbrsssvkdrt17bXXauDAgXIHEE6dfPLJJb53u906fPiwXnzxRY0aNUpnnXWW8vPzdfLJJ2v27NmFbc88Ho+uv/56bdq0SQkJCRo0aJAef/xxSVKtWrU0duxYrVu3TnXq1FF6erpef/11+3/wYlxep5uvRaG8vDwlJiYqNzdXCQkJTi8nOng8Una2GRKVnGxK56IgxQUAAAAAAAAAfw4cOKC1a9eqdevWqm21DwsV10hDUlBQoPbt2+v888/Xfffd5/RyylXe5yWY3ICKF1QsM9N3mjt1atSkuQAAAAAAAAAQVm631Lev06uIeuvXr9enn36qU045RQcPHtRTTz2ltWvX6qKLLnJ6aRET4/QCEOUyM03/wuKhiyRt3mz2Z2Y6sy4AAAAAAAAAQNSJiYnRSy+9pBNPPFF9+vTR8uXLNXfu3LDPVYkmVLzAP4/HVLr46kbn9ZrhUaNHS0OHUlIHAAAAAAAAAFBqaqoWLlzo9DIcRcUL/MvOLlvpUpzXK23caI4DAAAAAAAAAAAELyhHTo69xwEAAAAAAAAAUM0RvMC/5GR7jwMAAAAAAAAAoJpzPHh5+umnlZaWptq1a6tnz55atGhRucfv2bNH119/vZKTkxUfH6+2bdtq9uzZhc9PmDBBLperxNexxx4b7h+jekpPl1JSzCwXX1wuKTXVHAcAAAAAAAAAABTr5Ju/8cYbGjNmjKZNm6aePXtqypQpGjhwoH755Rc1a9aszPH5+fk67bTT1KxZM82aNUstW7bU+vXr1bBhwxLHdezYUXPnzi38PjbW0R+z6nK7palTpeHDTcji9RY9Z4UxU6aY4wAAAAAAAAAAgLPBy+TJk3XNNdfoyiuvlCRNmzZNH330kV544QXdcccdZY5/4YUXtGvXLn311VeKi4uTJKWlpZU5LjY2Vs2bNw/r2muMjAxp1ixp1Chp06ai/UccIT33nHkeAAAAAAAAAABIcrDVWH5+vhYvXqwBAwYULSYmRgMGDNDXX3/t8zXvv/++evXqpeuvv15JSUnq1KmTJk6cKI/HU+K43377TS1atNBRRx2liy++WBs2bAjrz1LtZWRI69ZJ8+dLJ59s9l1/PaELAAAAAAAAAAClOBa87NixQx6PR0lJSSX2JyUlaevWrT5f8/vvv2vWrFnyeDyaPXu27r77bj322GO6//77C4/p2bOnXnrpJc2ZM0fPPPOM1q5dq/T0dP35559+13Lw4EHl5eWV+EIpbrfUt680ZIj5/uefHV0OAAAAAAAAAESSxyNlZUkzZ5ptqXoAW5WeY176a8KECZU697vvvmvbcSirSg0/KSgoULNmzfTcc8/J7Xare/fu2rx5sx555BGNHz9ekjR48ODC47t06aKePXuqVatWevPNN3XVVVf5PO+kSZN0zz33RORnqPLOOktKSpKOP97plQAAAAAAAABARGRmlp3GkJJiRmSHozFQTk5O4eM33nhD48aN0y+//FK4r379+va/KWzjWMVLkyZN5Ha7tW3bthL7t23b5nc+S3Jystq2bSt3sWHu7du319atW5Wfn+/zNQ0bNlTbtm21evVqv2sZO3ascnNzC782btwYwk9UQxx7rHTppVLHjk6vBAAAAAAAAADCLjNTGj68ZOgiSZs3m/2Zmfa/Z/PmzQu/EhMT5XK5Sux7/fXX1b59e9WuXVvHHnus/v3vfxe+Nj8/XzfccIOSk5NVu3ZttWrVSpMmTZJUNDP9nHPOkcvl8jlDPRAFBQW69957lZKSovj4eB133HGaM2dOQGvwer2aMGGCjjzySMXHx6tFixa66aabQvsPFaUcq3ipVauWunfvrnnz5mnYsGGSzB/WvHnzdMMNN/h8TZ8+fTRjxgwVFBQoJsZkRr/++quSk5NVq1Ytn6/Zu3ev1qxZo0svvdTvWuLj4xUfH1+5HwgAAAAAAAAAEPW8Xmn//sCO9Xikm24yr/F1HpfLVMIMGGCmNVSkbl3zmsp47bXXNG7cOD311FPq1q2bfvjhB11zzTWqV6+eLr/8cj3xxBN6//339eabb+rII4/Uxo0bC4sNvvvuOzVr1kwvvviiBg0aVKLIIRhTp07VY489pmeffVbdunXTCy+8oLPPPlsrVqxQmzZtyl3D22+/rccff1yvv/66OnbsqK1bt+rHH3+s3H+UKONoq7ExY8bo8ssv1wknnKAePXpoypQp2rdvn6688kpJ0mWXXaaWLVsWJmEjR47UU089pVGjRunGG2/Ub7/9pokTJ5ZIw2655RYNGTJErVq10pYtWzR+/Hi53W6NGDHCkZ+xWvr+e+mbb6Q+faRu3ZxeDQAAAAAAAAAEbP9+ya5OXV6vqYRJTAzs+L17pXr1Kvee48eP12OPPaaM//U4a926tVauXKlnn31Wl19+uTZs2KA2bdropJNOksvlUqtWrQpf27RpU0mmU5S/zlOBePTRR3X77bfrwgsvlCQ99NBDmj9/vqZMmaKnn3663DVs2LBBzZs314ABAxQXF6cjjzxSPXr0CHkt0cixVmOSdMEFF+jRRx/VuHHjdNxxx2np0qWaM2eOkpKSJJk/gOK97FJTU/XJJ5/ou+++U5cuXXTTTTdp1KhRuuOOOwqP2bRpk0aMGKF27drp/PPPV+PGjfXNN98UfqBgg6eekm68UfroI6dXAgAAAAAAAAA1xr59+7RmzRpdddVVql+/fuHX/fffrzVr1kiSrrjiCi1dulTt2rXTTTfdpE8//dTWNeTl5WnLli3q06dPif19+vTRqlWrKlzDeeedp7/++ktHHXWUrrnmGr3zzjs6fPiwrWt0mqMVL5J0ww03+G0tlpWVVWZfr1699M033/g93+uvv27X0uBPhw5mu2KFs+sAAAAAAAAAgCDVrWsqTwLxxRfSGWdUfNzs2dLJJwf23pWx938Lnz59unr27FniOatt2PHHH6+1a9fq448/1ty5c3X++edrwIABmjVrVuXePAjlrSE1NVW//PKL5s6dq88++0zXXXedHnnkES1YsEBxcXERW2M4OR68oAqygpeVK51dBwAAAAAAAAAEyeUKvN3X6adLKSnS5s2+57y4XOb5008PbMZLZSUlJalFixb6/fffdfHFF/s9LiEhQRdccIEuuOACDR8+XIMGDdKuXbt0xBFHKC4uTh6PJ+Q1JCQkqEWLFlq4cKFOOeWUwv0LFy4s0TKsvDXUqVNHQ4YM0ZAhQ3T99dfr2GOP1fLly3X88ceHvK5oQvCC4HXsaLa//CIdPizF8jECAAAAAAAAUP243dLUqdLw4SZkKR6+uFxmO2VKZEIXyz333KObbrpJiYmJGjRokA4ePKjvv/9eu3fv1pgxYzR58mQlJyerW7duiomJ0VtvvaXmzZurYcOGkqS0tDTNmzdPffr0UXx8vBo1auT3vdauXaulS5eW2NemTRvdeuutGj9+vI4++mgdd9xxevHFF7V06VK99tprklTuGl566SV5PB717NlTdevW1auvvqo6deqUmANT1XHFHMFr1UqqU0f66y/p99+ltm2dXhEAAAAAAAAAhEVGhjRrljRqlLRpU9H+lBQTuvxvxn3EXH311apbt64eeeQR3XrrrapXr546d+6s0aNHS5IaNGighx9+WL/99pvcbrdOPPFEzZ49WzExZuT7Y489pjFjxmj69Olq2bKl1q1b5/e9xowZU2Zfdna2brrpJuXm5urmm2/W9u3b1aFDB73//vtq06ZNhWto2LChHnzwQY0ZM0Yej0edO3fWBx98oMaNG9v+38opLq/XV4FUzZaXl6fExETl5uYqISHB6eVEp+7dpSVLpHfekYYNc3o1AAAAAAAAAFDGgQMHtHbtWrVu3Vq1a9eu1Lk8Hik7W8rJkZKTpfT0yFa6IPzK+7wEkxtQ8YLQdOxogpeVKwleAAAAAAAAAFR7brfUt6/Tq0BVQPCC0Nx8s/SPfxTNewEAAAAAAAAAAAQvCFHXrk6vAAAAAAAAAACAqBPj9AIAAAAAAAAAAACqC4IXhO6VV6TRo6WNG51eCQAAAAAAAAAAUYFWYwjd449LS5dK/fpJqalOrwYAAAAAAAAAfPJ6vU4vAVWAXZ8TKl4Quo4dzXblSmfXAQAAAAAAAAA+xMXFSZL279/v8EpQFVifE+tzEyoqXhC6Dh3MluAFAAAAAAAAQBRyu91q2LChtm/fLkmqW7euXC6Xw6tCtPF6vdq/f7+2b9+uhg0byu12V+p8BC8InVXxsmKFs+sAAAAAAAAAAD+aN28uSYXhC+BPw4YNCz8vlUHwgtBZFS+rVkkej1TJFBAAAAAAAAAA7OZyuZScnKxmzZrp0KFDTi8HUSouLq7SlS4WgheE7qijpPh46cABaf168z0AAAAAAAAARCG3223bhXWgPDFOLwBVmNstHXusebxqlbNrAQAAAAAAAAAgClDxgsqZMUNq3Fhq1szplQAAAAAAAAAA4DiCF1SONecFAAAAAAAAAADQagwAAAAAAAAAAMAuBC+onL17pdtvlzIypIICp1cDAAAAAAAAAICjaDWGyqldW5oyRcrPlzZskNLSnF4RAAAAAAAAAACOoeIFlRMbK7VrZx6vWOHsWgAAAAAAAAAAcBjBCyqvQwezXbnS2XUAAAAAAAAAAOAwghdUXseOZkvwAgAAAAAAAACo4QheUHlWxQutxgAAAAAAAAAANRzBCyqveKsxr9fZtQAAAAAAAAAA4CCCF1TeMcdIcXFSTIy0bZvTqwEAAAAAAAAAwDGxTi8A1UBcnLRhg5SUJLlcTq8GAAAAAAAAAADHELzAHs2bO70CAAAAAAAAAAAcR6sxAAAAAAAAAAAAmxC8wB4rVkjnnitdeKHTKwEAAAAAAAAAwDG0GoM9XC4pM1OqX1/yepn1AgAAAAAAAACokah4gT2OOUaKjZX27pU2bXJ6NQAAAAAAAAAAOILgBfaoVUtq29Y8XrHC2bUAAAAAAAAAAOAQghfYp0MHs1250tl1AAAAAAAAAADgEIIX2KdjR7MleAEAAAAAAAAA1FAEL7CPVfFCqzEAAAAAAAAAQA1F8AL7dOgg1a5t5r0AAAAAAAAAAFADxTq9AFQjHTpIe/dKbrfTKwEAAAAAAAAAwBEEL7BPDAVUAAAAAAAAAICajSvlAAAAAAAAAAAANiF4gb3eeks67jjpppucXgkAAAAAAAAAABFHqzHYy+ORfvxRqlfP6ZUAAAAAAAAAABBxVLzAXh06mO3KlZLX6+xaAAAAAAAAAACIMIIX2KttWykmRtqzR8rJcXo1AAAAAAAAAABEFMEL7FW7tnTMMebxypXOrgUAAAAAAAAAgAgjeIH9ircbAwAAAAAAAACgBiF4gf2s4GXFCmfXAQAAAAAAAABAhBG8wH5du0rt20vNmjm9EgAAAAAAAAAAIirW6QWgGjr/fPMFAAAAAAAAAEANQ8ULAAAAAAAAAACATQheED5er5Sf7/QqAAAAAAAAAACIGIIXhMett0oJCdJzzzm9EgAAAAAAAAAAIobgBeERFyft3SutWOH0SgAAAAAAAAAAiBiCF4RHhw5mu3Kls+sAAAAAAAAAACCCCF4QHh07mi3BCwAAAAAAAACgBiF4QXi0aye5XNKOHdL27U6vBgAAAAAAAACAiCB4QXjUrSu1bm0eU/UCAAAAAAAAAKghCF4QPla7sRUrnF0HAAAAAAAAAAAREuv0AlCNnXKK5PVKLVs6vRIAAAAAAAAAACLC5fV6vU4vItrk5eUpMTFRubm5SkhIcHo5AAAAAAAAAADAQcHkBrQaAwAAAAAAAAAAsAnBC8LL65W2bpUOHnR6JQAAAAAAAAAAhB3BC8LrhBOk5GTp22+dXgkAAAAAAAAAAGFH8ILwSk422xUrnF0HAAAAAAAAAAARQPCC8OrQwWxXrnR2HQAAAAAAAAAARADBC8LLCl6oeAEAAAAAAAAA1AAELwivjh3NlooXAAAAAAAAAEANQPCC8Dr2WLPdtk167jkpK0vyeBxdEgAAAAAAAAAA4ULwgvD67DPJ7TaP//53qV8/KS1Nysx0dFkAAAAAAAAAAIQDwQvCJzNTGj68bIXL5s1mP+ELAAAAAAAAAKCaIXhBeHg80qhRktdb9jlr3+jRtB0DAAAAAAAAAFQrBC8Ij+xsadMm/897vdLGjeY4AAAAAAAAAACqCceDl6efflppaWmqXbu2evbsqUWLFpV7/J49e3T99dcrOTlZ8fHxatu2rWbPnl2pcyIMcnLsPQ4AAAAAAAAAgCrA0eDljTfe0JgxYzR+/HgtWbJEXbt21cCBA7V9+3afx+fn5+u0007TunXrNGvWLP3yyy+aPn26WrZsGfI5ESbJyfYeBwAAAAAAAABAFeDyen0N4YiMnj176sQTT9RTTz0lSSooKFBqaqpuvPFG3XHHHWWOnzZtmh555BH9/PPPiouLs+WcvuTl5SkxMVG5ublKSEgI8aer4TweKS1N2rzZ95wXl0tKSZHWrpXc7ogvDwAAAAAAAACAQAWTGzhW8ZKfn6/FixdrwIABRYuJidGAAQP09ddf+3zN+++/r169eun6669XUlKSOnXqpIkTJ8rzvwHtoZwTYeJ2S1OnmscuV8nnrO+nTCF0AQAAAAAAAABUK44FLzt27JDH41FSUlKJ/UlJSdq6davP1/z++++aNWuWPB6PZs+erbvvvluPPfaY7r///pDPKUkHDx5UXl5eiS/YICNDmjVLKtYKTpLUooXZn5HhzLoAAAAAAAAAAAgTR2e8BKugoEDNmjXTc889p+7du+uCCy7QXXfdpWnTplXqvJMmTVJiYmLhV2pqqk0rhjIypHXrpPnzpWbNzL6nnyZ0AQAAAAAAAABUS44FL02aNJHb7da2bdtK7N+2bZuaN2/u8zXJyclq27at3MXaU7Vv315bt25Vfn5+SOeUpLFjxyo3N7fwa+PGjZX4yVCG2y317SsNG2a+z8pycDEAAAAAAAAAAISPY8FLrVq11L17d82bN69wX0FBgebNm6devXr5fE2fPn20evVqFRQUFO779ddflZycrFq1aoV0TkmKj49XQkJCiS+EQf/+ZlvszwcAAAAAAAAAgOrE0VZjY8aM0fTp0/Xyyy9r1apVGjlypPbt26crr7xSknTZZZdp7NixhcePHDlSu3bt0qhRo/Trr7/qo48+0sSJE3X99dcHfE44qH9/6YknpNdfd3olAAAAAAAAAACERayTb37BBRfojz/+0Lhx47R161Ydd9xxmjNnjpKSkiRJGzZsUExMUTaUmpqqTz75RP/85z/VpUsXtWzZUqNGjdLtt98e8DnhoMaNpRtvdHoVAAAAAAAAAACEjcvr9XqdXkS0ycvLU2JionJzc2k7BgAAAAAAAABADRdMbuBoqzHUQHv3StOnU/kCAAAAAAAAAKiWCF4QWQUF0siR0lNPSevXO70aAAAAAAAAAABsRfCCyEpIkHr0MI/nzXN2LQAAAAAAAAAA2IzgBZHXv7/ZErwAAAAAAAAAAKoZghdE3qmnmu3nn0ter7NrAQAAAAAAAADARgQviLxevaTataWtW6VVq5xeDQAAAAAAAAAAtiF4QeTVri2ddJJ5TLsxAAAAAAAAAEA1QvCCgHg8UlaWNHOm2Xo8lTxh//6SyyWtX2/D6gAAAAAAAAAAiA4ur5chG6Xl5eUpMTFRubm5SkhIcHo5jsvMlEaNkjZtKtqXkiJNnSplZIR40l27TPDSqJEtawQAAAAAAAAAIFyCyQ2oeEG5MjOl4cNLhi6StHmz2Z+ZGeKJjziC0AUAAAAAAAAAUO0QvMAvj8dUuviqibL2jR5tQ9sxiq4AAAAAAAAAANUEwQv8ys4uW+lSnNcrbdxojgvJt99KJ50kDRkS4gkAAAAAAAAAAIgusU4vANErJ8fe48qoX19auFCqU0c6eFCKjw/xRAAAAAAAAAAARAcqXuBXcrK9x5XRoYOUlCT99Zf0zTchngQAAAAAAAAAgOhB8AK/0tOllBTJ5fL9vMslpaaa40Lickmnnmoez5sX4kkAAAAAAAAAAIgeBC/wy+2Wpk41j/2FL1OmmONC1r+/2RK8AAAAAAAAAACqAYIXlCsjQ5o1S2rZsuT+evXM/oyMSr6BFbwsWiT9+WclTwYAAAAAAAAAgLMIXlChjAxp3Tpp/nxpzBizLzXVhtBFktLSpNatpcOHpexsG04IAAAAAAAAAIBzCF4QELdb6ttXuvNO8/3PP0t//GHTyTMypGHDpIQEm04IAAAAAAAAAIAzYp1eAKqWxo2ljh2lFSukL7+UzjnHhpM++qgNJwEAAAAAAAAAwHlUvCBo6elmS2cwAAAAAAAAAABKInhB0MISvHi90po10u+/23hSAAAAAAAAAAAii+AFQbOClyVLpD//tOmk48dLxxwjPfywTScEAAAAAAAAACDyCF4QtNRUKS1NKiiQvv7appOecILZzptn0wkBAAAAAAAAAIg8gheExPZ2Y6ecIsXESKtXSxs22HRSAAAAAAAAAAAii+AFIbE9eElMlE480Tz+/HObTgoAAAAAAAAAQGQRvCAkVvDyzTfSwYM2nbR/f7Ol3RgAAAAAAAAAoIoieEFI2rWTmjY1ocv339t00lNPNdvPP5e8XptOCgAAAAAAAABA5BC8ICQuVxjajfXuLcXHS1u2SL/8YtNJAQAAAAAAAACIHIIXhMz24KVOHemRR6R335VSU206KQAAAAAAAAAAkRPr9AJQdVnBy5dfSh6P5HbbcNIbb7ThJAAAAAAAAAAAOIOKF4Ssa1epQQMpL09avtzp1QAAAAAAAAAA4DyCF4QsNtaMZZFsbDcmSV99JY0bJ61aZeNJAQAAAAAAAAAIP4IXVIrtc14kadIk6b77pA8/tPGkAAAAAAAAAACEH8ELKsUKXr74QvJ6bTpp//5mO2+eTScEAAAAAAAAACAyCF5QKT16SLVqSdu2SatX23RSK3jJzpby8206KQAAAAAAAAAA4UfwgkqpXduEL5KN7cY6dZKaNZP275e++camkwIAAAAAAAAAEH4EL6g02+e8uFzSqaeax59/btNJAQAAAAAAAAAIP4IXVFrxOS+2sYKXt9+WZs6UsrIkj8fGNwAAAAAAAAAAwH4EL6i03r2lmBjp99+lLVtsOunhw2b700/SRRdJ/fpJaWlSZqZNbwAAAAAAAAAAgP0IXlBpiYlS167msS3txjIzpeuvL7t/82Zp+HDCFwAAAAAAAABA1CJ4gS1sazfm8UijRkleb9nnrH2jR9N2DAAAAAAAAAAQlQheYIuTTzbbSle8ZGdLmzb5f97rlTZutKm0BgAAAAAAAAAAexG8wBYnnWS2P/0k7d5diRPl5Nh7HAAAAAAAAAAAEUTwAlskJUlt25qClIULK3Gi5GR7jwMAAAAAAAAAIIIIXmAbW+a8pKdLKSmSy+X7eZdLSk0tejMAAAAAAAAAAKIIwQtsY8ucF7dbmjrVPPYXvkyZYo4DAAAAAAAAACDKELzANlYRyvffS/v3V+JEGRnSrFlSy5Zln+vSRTrnnEqcHAAAAAAAAACA8CF4gW3S0kxWcviw9O23lTxZRoa0bp00f740Y4b03/9KtWpJP/4ovfWWDasFAAAAAAAAAMB+BC+wjctl05wXi9st9e0rjRghXXKJNHasCV+2bLHh5AAAAAAAAAAA2I/gBbayZc6LP3fcIS1fLo0eHYaTAwAAAAAAAABQeQQvsJVV8fL119KhQzafvHZtqW1bm08KAAAAAAAAAIB9CF5gqw4dpEaNpP37pR9+COMbffeddPXVkscTxjcBAAAAAAAAACA4BC+wVUyMdNJJ5rEtc1582btXOv106fnnpeeeC9ObAAAAAAAAAAAQPIIX2C6sc14kqX596d57zeOxY6Vt28L0RgAAAAAAAAAABIfgBbaz5rx8+aVUUBCmN7nuOql7dyk3V7r55jC9CQAAAAAAAAAAwSF4ge2OP16qW1fatUtatSpMb+J2S9OmSS6X9Npr0uefh+mNAAAAAAAAAAAIHMELbBcXJ/3tb+Zx2Oa8SNIJJ0gjR5rH110nHTwYxjcDAAAAAAAAAKBiBC8Ii7DPebE88ICUlCT98ov03/+G+c0AAAAAAAAAAChfrNMLQPVkzXnJzpa8XtMRLCwaNpT+/W9p507p//4vTG8CAAAAAAAAAEBgCF4QFn/7mxQbK23aJK1fL6WlhfHNMjLCeHIAAAAAAAAAAAJHqzGERd26Uvfu5nFY57yUtnev9OOPEXxDAAAAAAAAAACKELwgbCI258WyYoXUoYN0xhnS7NnSzJlSVpbk8URoAQAAAAAAAACAmo7gBWFTfM5LRBx1lJSfL23ZIp15pnTRRVK/fqbPWWZmhBYBAAAAAAAAAKjJCF4QNn36mO0vv0jbt0fgDT/+WNq2rez+zZul4cMJXwAAAAAAAAAAYUfwgrA54gipUyfzOOxVLx6PNGqU7+e8XrMdPZq2YwAAAAAAAACAsCJ4QVhFbM5Ldra0aZP/571eaePGCPY9AwAAAAAAAADURAQvCCtrzkvYZ93n5Nh7HAAAAAAAAAAAISB4QVj9+afZ/vZbmGfdJyfbexwAAAAAAAAAACEgeEHYZGZKf/972f1hmXWfni6lpEgul+/nXS4pNbWoBAcAAAAAAAAAgDAgeEFYWLPurbn2xYVl1r3bLU2dah6XDl+s76dMMccBAAAAAAAAABAmBC8IC0dm3WdkSLNmSS1bltzfvLl06qnS3/5m45sBAAAAAAAAAFAWwQvCwrFZ9xkZ0rp10vz50owZZtuunTRvnnTPPTa/GQAAAAAAAAAAJUVF8PL0008rLS1NtWvXVs+ePbVo0SK/x7700ktyuVwlvmrXrl3imCuuuKLMMYMGDQr3j4FiHJ1173ZLfftKI0aY7b33mv3PPy/9/HMY3hAAAAAAAAAAAMPx4OWNN97QmDFjNH78eC1ZskRdu3bVwIEDtX37dr+vSUhIUE5OTuHX+vXryxwzaNCgEsfMnDkznD8GSomqWffp6dKQIWagzF13ReANAQAAAAAAAAA1lePBy+TJk3XNNdfoyiuvVIcOHTRt2jTVrVtXL7zwgt/XuFwuNW/evPArKSmpzDHx8fEljmnUqFE4fwyUEnWz7idOlGJipMxM6ZtvIvSmAAAAAAAAAICaxtHgJT8/X4sXL9aAAQMK98XExGjAgAH6+uuv/b5u7969atWqlVJTUzV06FCtWLGizDFZWVlq1qyZ2rVrp5EjR2rnzp1+z3fw4EHl5eWV+ELl+Zt1n5Ji9mdkRHAxnTpJl19uHt92m+T1RvDNAQAAAAAAAAA1haPBy44dO+TxeMpUrCQlJWnr1q0+X9OuXTu98MILeu+99/Tqq6+qoKBAvXv31qZNmwqPGTRokF555RXNmzdPDz30kBYsWKDBgwfL4/H4POekSZOUmJhY+JWammrfD1nDWbPuH37YfJ+SIq1dG+HQxXLPPVJ8vJSdLc2e7cACAAAAAAAAAADVncvrde7W/y1btqhly5b66quv1KtXr8L9t912mxYsWKBvv/22wnMcOnRI7du314gRI3Tffff5POb333/X0Ucfrblz56p///5lnj948KAOHjxY+H1eXp5SU1OVm5urhISEEH4ylLZtm9S8uWkztnevVLeuQwuZMEE6dMhUvSQmOrQIAAAAAAAAAEBVkpeXp8TExIByg9gIrcmnJk2ayO12a9u2bSX2b9u2Tc2bNw/oHHFxcerWrZtWr17t95ijjjpKTZo00erVq30GL/Hx8YqPjw9u8QhKUpLUrJm0fbv0009Sjx4OLWTCBIfeGAAAAAAAAABQEzjaaqxWrVrq3r275s2bV7ivoKBA8+bNK1EBUx6Px6Ply5crOTnZ7zGbNm3Szp07yz0G4de1q9n++KOz6yjk9UqHDzu9CgAAAAAAAABANeJo8CJJY8aM0fTp0/Xyyy9r1apVGjlypPbt26crr7xSknTZZZdp7Nixhcffe++9+vTTT/X7779ryZIluuSSS7R+/XpdffXVkqS9e/fq1ltv1TfffKN169Zp3rx5Gjp0qI455hgNHDjQkZ8RRlQFL998I/XpI02Z4vRKAAAAAAAAAADViKOtxiTpggsu0B9//KFx48Zp69atOu644zRnzhwlJSVJkjZs2KCYmKJ8aPfu3brmmmu0detWNWrUSN27d9dXX32lDh06SJLcbreWLVuml19+WXv27FGLFi10+umn67777qOdmMOs4GXZMmfXIUlatUr6+mvp55+lq66SGjWq3Pk8Hik7W8rJkZKTpfR0ye22Z60AAAAAAAAAgCrD5fV6vU4vItoEMyQHgVu2zIQviYnS7t2Sy+XgYjwes5gVK6Tbb5cefDD0c2VmSqNGSZs2Fe1LSZGmTpUyMiq/VgAAAAAAAACAo4LJDRxvNYaa49hjpbg4KTdXWr/e4cW43dKkSebx1KklQ5NgZGZKw4eXff3mzWZ/Zmbl1gkAAAAAAAAAqFIIXhAxtWpJ/+sIFx3txs46SzrpJOnAAWnChOBf7/GYShdfRWPWvtGjzXEAAAAAAAAAgBqB4AUR1aWL2f74o7PrkGR6nT30kHn84ovSypXBvT47u/xKGa9X2rjRHAcAAAAAAAAAqBEIXhBRXbuabVQEL5LUu7d0zjlSQYH07LOBv27/fmnmzMCOzckJbW0AAAAAAAAAgCon1ukFoGaxgpeoaDVmmTTJhC8XXCBlZZmgJDlZSk83s2BKe/VV6cYbpT17Ajt/crKdqwUAAAAAAAAARDGCF0SUFbysXi3t2yfVq+fseiRJ7dpJK1ZIRx9dsnVYSoo0dao0ZIipcElMNPtTU03o0qqV2ebl+Z7z4nKZc6SnR+KnAAAAAAAAAABEAVqNIaKaNpWaNzc5xfLlTq/mfzIzpeHDy85r2bxZOvdcqVkzady4ov0nnyzNnSutWSO98ILZ53KVfK31/ZQpvqtmAAAAAAAAAADVEsELIi6q5rx4PNKoUb4rVqx9e/ZIH39s5sBIJlTp398EKhkZ0qxZUsuWZV87aZJ5HgAAAAAAAABQYxC8IOKias5LdnbZShdfnn5aivHz1yUjQ1q3Tpo/X5oxQxo40Oz/5BPfgQ4AAAAAAAAAoNpixgsirksXs42KipecnMCO27Gj/OfdbqlvX/O4d2+pbVsTxHz6aVEQAwAAAAAAAACo9qh4QcQVr3ixunc5JjnZ3uMkqVUr6brrzOOxY6PghwQAAAAAAAAARArBCyKuXTupVi3pzz+l9esdXkx6upSSYua2+OJySamp5rhg3HWX1KCB9MMP0ptvVn6dAAAAAAAAAIAqgeAFERcXJ3XsaB473m7M7ZamTjWPS4cv1vdTppjjgtGkiXTrrebxv/4l5edXapkAAAAAAAAAgKqB4AWOiKo5LxkZ0qxZUsuWJfenpJj9GRmhnfef/5SSkqS9e6Vff638OgEAAAAAAAAAUS/W6QWgZio+5yUqZGRIQ4dK2dlSTo6Z6ZKeHnylS3H160sffigde6x5DAAAAAAAAACo9ghe4AgreImKiheL2y317WvvOU84wd7zAQAAAAAAAACiGq3G4Air1diaNdKffzq7logoKJDeeEPascPplQAAAAAAAAAAwojgBY5o0kRq0cI8Xr7c2bVExBVXSBdeKE2a5PRKAAAAAAAAAABhRPACx0TdnJdwGjHCbJ96Stqwwdm1AAAAAAAAAADChuAFjrHajUXVnJdwGTRIOuUUKT9fmjDB6dUAAAAAAAAAAMKE4AWOsSpeakTw4nJJDz5oHr/8srRypbPrAQAAAAAAAACEBcELHGMFL8uXm9nz1d7f/iadc475Ye+80+nVAAAAAAAAAADCgOAFjmnbVoqPl/buldaudXo1EfLAA1JMjPTee9JXXzm9GgAAAAAAAACAzQhe4JjYWKljR/O4RrQbk6T27aUrrpBOOMH8BwAAAAAAAAAAVCsEL3CU1W5s2TJn1xFRTzwhLVok9ejh9EoAAAAAAAAAADbjlns4ygpeakzFiyTVq1f02OORsrOlnBwpOVlKT5fcbufWBgAAAAAAAACoFIIXOKpLF7OtUcGL5bXXpOuuk/LyivalpEhTp0oZGc6tCwAAAAAAAAAQMlqNwVFWxcvatSXzh2ovM1O65JKyP/TmzdLw4eZ5AAAAAAAAAECVQ/ACRx1xhCnykKTly51dS8R4PNKoUb6f83rNdvRocxwAAAAAAAAAoEoheIHjatycl+xsadMm/897vdLGjeY4AAAAAAAAAECVQvACx9W4OS85OfYeBwAAAAAAAACIGgQvcJxV8bJsmbPriJjkZHuPAwAAAAAAAABEDYIXOM4KXpYvlwoKnF1LRKSnm8E2Lpfv510uKTXVHFcj/oMAAAAAAAAAQPVB8ALHHXOMVLu2tG+ftGaN06uJALdbmjrVPC4dvljfT5lijrv1VunKK6VduyK6RAAAAAAAAABAaAhe4LjYWKlTJ/O4xrQby8iQZs2SWrYsuT8lxezPyJA2bpSeeEJ66SWpQwfp7beLjvN4pKwsaeZMs/V4Irh4AAAAAAAAAIA/BC+ICla7sR9/dHYdEZWRIa1bJ82fL82YYbZr15r9kmk3tmCBdOyx0rZt0vDh5rnnn5fS0qR+/aSLLjLbtDQpM9PBHwYAAAAAAAAAIEmxTi8AkKQuXcy2RgUvkmkn1rev/+d795aWLpUeeECaNEl65x3zVdrmzSaYsaplAAAAAAAAAACOoOIFUcGqeKkxrcaCER8v3XuvtGiRFBfn+xiv12xHj6btGAAAAAAAAAA4iOAFUcGqeFm3TsrNdXQp0Ss3Vzp0yP/zXq+ZC5Od7f8YZsMAAAAAAAAAQFgRvCAqNGokHXmkeUzVix85OYEd9/e/S+PHm/kwBw8W7c/MZDYMAAAAAAAAAIQZwQuiRo2d8xKo5OTAjvv1V9OarG9fk2jNnm3CleHDpU2bSh5rzYYhfAEAAAAAAAAAWxC8IGow56UC6elSSorkcvl+3uUy4cyzz0oXXig1ayb99ZfUpo00alTRHJjimA0DAAAAAAAAALYieEHUsIIXKl78cLulqVPN49Lhi/X9U09J115rZrhs3SqtWGGqWkpXuhQXyGwYAAAAAAAAAEBACF4QNaxWY8uXU3zhV0aGNGuW1LJlyf0pKWZ/RkbRPpdL6tAh8NkwgR4HAAAAAAAAAPAr1ukFAJZjjpHq1DHdsdaskdq2dXpFUSojQxo61FSo5OSY9mLp6aYixpdAZ8MEehwAAAAAAAAAwC+CF0QNt1vq3FlatMi0GyN4KYfbLfXtG9ix1myYzZt9z3mRpFq1pBNOsG15AAAAAAAAAFBT0WoMUYU5L2FQ3mwYy6BBUr16kVsTAAAAAAAAAFRTBC+IKtacl2XLnF1HteNvNkxqqvTss9K77/oPZQAAAAAAAAAAAaPVGKIKFS9hFMhsmAMHpFGjpH/9y4QyAAAAAAAAAICgELwgqlgVLxs2SLt3S40aObueaqei2TCjR0vPPSd9+qn0+edS69aRWhkAAAAAAAAAVAu0GkNUSUyUWrUyj2k35oA775SOOUZat85Uw/z6q9MrAgAAAAAAAIAqheAFUcdqN0bw4oAjj5S++ELq0EHavFk6+WTpp5/Mcx6PlJUlzZxpth6PkysFAAAAAAAAgKhE8IKow5wXhyUnm2Cla1dp2zbTmuzRR6W0NKlfP+mii8w2LU3KzHR2rQAAAAAAAAAQZQheEHUIXqJA06bS/PlSjx7Szp3SrbdKmzaVPGbzZmn4cMIXAAAAAAAAACiG4AVRp0sXs/3pJ7pZOapRI2nOHKl2bd/Pe71mO3o0f1AAAAAAAAAA8D8EL4g6Rx8t1asnHTgg/fab06up4X780fxB+OP1Shs3StnZkVsTAAAAAAAAAEQxghdEnZgYqXNn85h2Yw7LyQnsuPXr/T/n8ZiZMTNnmi3VMQAAAAAAAACqMYIXRCWr3diyZc6uo8ZLTg7suH/8Q8rIKFv5kpkppaVJ/fpJF11ktmlpzIUBAAAAAAAAUG0RvCAqde1qtlS8OCw9XUpJkVwu/8fExJh2ZO+8I23fXrT/P/+Rhg+XNm0qefzmzWY/4QsAAAAAAACAaojgBVGJ4CVKuN3S1KnmcenwxeUyX2++KX3/vTRunHT66eY5j0e6+WYzA6Y0a9/o0eW3HaNFGQAAAAAAAIAqiOAFUcma8bJpk7Rrl7NrqfEyMqRZs6SWLUvuT0kx+889V+reXbrnHqlBA/NcdraUl+f/nF6vtHFj2dZkFlqUAQAAAAAAAKiiYp1eAOBLQoK5zr5unfT441L//qbrldvt9MpqqIwMaehQE5Tk5JjZL+X9geTkBHZe67gPPpCOOEI68UTpww9NK7LS1TJWi7JZs8x6wsHjCfxnBAAAAAAAAAAfXF6vr15ANVteXp4SExOVm5urhIQEp5dTI2VmShdfbEaHWFJSTNercF1zh42yskyVSkXmz5f69jUp2/r1Up06UkGBdPCg7+NdLvNBWLvW/kAkM1MaNarkTBo+dAAAAAAAAAAUXG5AqzFEncxMU9hQPHSRmMlepaSnm9Ci9FwYi8slpaaa4/76y1S6NGliHvsLXaSKW5SFyvrQFQ9dJD50AAAAAAAAAIJG8IKo4vGYooPKzGRHFHC7TaWIVDZ8sb6fMsUcV6eO9NZb0rZt0oMPBnb+QFuZBYIPHQAAAAAAAAAbEbwgqmRnly06KC5cBQ8Ig4wMM4+lZcuS+1NSfM9piYmRevYM7NzJyfasUeJDBwAAAAAAAMBWsU4vACgu2JnsiHIZGdLQoYEPrLdalG3e7LsCxZrxkp5u3xr50AEAAAAAAACwEcELokqghQx2FjwgzNxuqW/fwI+dOtXMVXG5yoYvXm9RizK7BPphKm/2DAAAAAAAAAD8D63GEFWCmcmOaspfizLJtCNr1cre9+vTp/wPnfW+w4b5f97jkbKypJkzzZZ5MAAAAAAAAECNRfCCqFLeTHaL3QUPiEIZGdK6ddL8+dKMGWY7fLhUUCBdfrl04IB973XDDVLXruZx6Q+dy2W+Jk6UGjY0+woKpDvukH77zXyfmSmlpUn9+kkXXWS2aWlmPwAAAAAAAIAax+X1+hqkULPl5eUpMTFRubm5SkhIcHo5NVJmpjRqVNmZ52efLb33njNrgsN27JA6dZK2bZNuu0166KHKn3PWLOm880y4ct990rRpJT90qakm6cvIKNr3+uvSiBFSbKw0YID0ySdlW6JZAc6sWSVfCwAAAAAAAKBKCiY3IHjxgeAlOng8RTPZ16+Xxo6V6tQxhRDNmjm9Ojji/feloUNNsPHll1Lv3qGfa/166bjjpD17zIdr4sSSH7rkZNPTrnR51YoVJviZPbv887tcpoXZ2rWUaAEAAAAAAABVHMFLJRG8RB+vV+rZU/ruO+mWW6RHHnF6RXDMFVdIL78sHXOMtHSpVK9e8Oc4fFjq21dauNB8sLKzpbi44M4xebJ0880VHzd/vnkvAAAAAAAAAFVWMLlBVMx4efrpp5WWlqbatWurZ8+eWrRokd9jX3rpJblcrhJftWvXLnGM1+vVuHHjlJycrDp16mjAgAH6zZrHgCrJ5ZImTDCPn35a2r7d0eXASVOmmEoSj0fauDG0c9x/vwldGjQwM2SCDV0kUxETiJyc4M8NAAAAAAAAoMpyPHh54403NGbMGI0fP15LlixR165dNXDgQG0v58p6QkKCcnJyCr/Wr19f4vmHH35YTzzxhKZNm6Zvv/1W9erV08CBA3XAzoHciLjBg6UePaS//qLipUZr2NC0+Vq2TDr22OBfn51t5rlIZqbLUUeFto5Ag5dAjwMAAAAAAABQLTgevEyePFnXXHONrrzySnXo0EHTpk1T3bp19cILL/h9jcvlUvPmzQu/kpKSCp/zer2aMmWK/vWvf2no0KHq0qWLXnnlFW3ZskXvvvtuBH4ihAtVLyjUubNUv35or92921S6XH65dNFFoa8hPd1U3rhc/o9p3NgcBwAAAAAAAKDGcDR4yc/P1+LFizVgwIDCfTExMRowYIC+/vprv6/bu3evWrVqpdTUVA0dOlQrVqwofG7t2rXaunVriXMmJiaqZ8+efs958OBB5eXllfhCdBo0iKoXFFNQID31lDRqVOCvOftsMxvmyScr995utzR1qnlcOnyxvt+508yByc+v3HsBAAAAAAAAqDIcDV527Nghj8dTomJFkpKSkrR161afr2nXrp1eeOEFvffee3r11VdVUFCg3r17a9OmTZJU+Lpgzjlp0iQlJiYWfqWmplb2R0OYUPWCEn78UbrpJumJJ0z7sfIcPlz0OC3NVL1UVkaGNGuW1LJlyf0tW0rDhpnHU6eaqpd16yr/fgAAAAAAAACinuOtxoLVq1cvXXbZZTruuON0yimnKDMzU02bNtWzzz4b8jnHjh2r3Nzcwq+NoQ7sRkRQ9YJC3bpJo0ebx1dfLe3a5fu4lSulNm2kOXPsX0NGhglV5s+XZsww23XrpHfekT74QGrUSFq0SDr+ePM9AAAAAAAAgGrN0eClSZMmcrvd2rZtW4n927ZtU/PmzQM6R1xcnLp166bVq1dLUuHrgjlnfHy8EhISSnwhepWuein1R42a5oEHpHbtpJwcU/1S2oED0ogRJgyZMkXyeu1fg9st9e1r3qdvX/O9JJ11lvTDD1LPnma2zNlnS3feWfQ6j0fKypJmzjRbj8f+tQEAAAAAAACIKEeDl1q1aql79+6aN29e4b6CggLNmzdPvXr1CugcHo9Hy5cvV3JysiSpdevWat68eYlz5uXl6dtvvw34nIh+VL2gUJ060iuvSDEx0muvSW++WTLMuOUWadkyqWlT6aWXys5jCbdWraQvviiqzGnSxGwzM03Ls379pIsuMtu0NLMfAAAAAAAAQJXl8nrDcft34N544w1dfvnlevbZZ9WjRw9NmTJFb775pn7++WclJSXpsssuU8uWLTVp0iRJ0r333qu//e1vOuaYY7Rnzx498sgjevfdd7V48WJ16NBBkvTQQw/pwQcf1Msvv6zWrVvr7rvv1rJly7Ry5UrVrl27wjXl5eUpMTFRubm5VL9EsY8/ls44w1x3X7tWKjXWBzXNXXdJEyeaAKagoOzzs2dLgwdHfl3FLVwo9e5t2pANH162+sYKhWbNMi3MAAAAAAAAAESFYHKD2Aitya8LLrhAf/zxh8aNG6etW7fquOOO05w5c5T0v6voGzZsUExMUWHO7t27dc0112jr1q1q1KiRunfvrq+++qowdJGk2267Tfv27dO1116rPXv26KSTTtKcOXMCCl1QdVhVL4sWmaqXRx91ekVwVJcuZusrdJFMeZTT+vQx7cRGjfLd8szrNeHL6NHS0KFFLcsAAAAAAAAAVBmOV7xEIypeqg6qXiDJhBlpadKmTb6fd7mklBTzIXE6zMjKMm3FKjJ/vpkXAwAAAAAAAMBxweQGjs54ASqLWS+QJGVn+w9dJFNJsnGjOc5pOTn2HgcAAAAAAAAgqgQdvBw6dEixsbH66aefwrEeICgulzRhgnn8739L27Y5uhw4pSqFGcnJ9h4XDI/HVNzMnGm2Ho/97wEAAAAAAADUcEEHL3FxcTryyCPl4YIdogRVL3A0zAhWerppe+Zy+T8mIcEcZ6fMTNOOrV8/6aKLzDYtzewHAAAAAAAAYJuQWo3ddddduvPOO7Vr1y671wMEjaoXVBhmuFxSaqr9YUYo3G5p6lTz2N96J0+2dxZNZqY0fHjZdmybN5v9hC8AAAAAAACAbVxer9cb7Iu6deum1atX69ChQ2rVqpXq1atX4vklS5bYtkAnBDMkB9HB65V69ZK+/Va6+Wbp0UedXhEizgoXJPOBsFjhxqxZUkZG5NflT2amNGpUyTAkNdWUbV1wQdG+7GzppJPKr5Apj8djKlv8zcBxuUxotXatvWEPAAAAAAAAUI0EkxvEhvIGw4YNC+VlQNhYVS+DB0tPPy317i0dPGg6S6Wncz25RsjIMOFK6TAjJUWaMiW6QhfJrGfoUBOs5OT4/rDOnGnagl18sTRtmlS/fvDvk53tP3SRTEi1caM5rm/f4M8PAAAAAAAAoISQgpfx48fbvQ6g0gYOlNq0kX77TTr33KL9KSmms1O0XXdHGAQSZkQTt7v8sGPnTnPMa69JS5ZIb78ttW9vqlgC+Rl37zbHBCLQ4wAAAAAAAACUK6TgxbJ48WKtWrVKktSxY0d169bNlkUBoXjnHRO6lGaNsYi2TlMIk4rCjKrkhhukrl1N67FVq6QTT5SuucZ8mEtX9Vjp4q5d0uuvSy+/bI555ZXA3is5OTw/AwAAAAAAAFDDhDTjZfv27brwwguVlZWlhg0bSpL27Nmjfv366fXXX1fTpk3tXmdEMeOl6mGMBaq1bdtMy7HPP/f9vMtVNOho8WIpP9/sd7ulr74yJWCbN5ecfVNcaip/OQAAAAAAAIByBJMbxITyBjfeeKP+/PNPrVixQrt27dKuXbv0008/KS8vTzfddFNIiwYqI5gxFv54PFJWlhmrkZVlvgeiQlKS9PHHUoMGvp+3ApWvvzahS9eu0uTJJmzp0cNUw0gmoPFlyhRCFwAAAAAAAMAmIbUamzNnjubOnav27dsX7uvQoYOefvppnX766bYtDghUoOMp/vMfcw372GNLXoPOzPQ9k53ZMIgaX30l/flnxcdNny5dfXXJfRkZpj1Z6Q95/fp8yAEAAAAAAACbhRS8FBQUKC4ursz+uLg4FRQUVHpRQLACHU/x2mvmq3Vr6YwzpDPPlPbskS6+uGwXJmbDIKoEmi7Wq+d7f0aGNHSoKfvKyTF/adLTqXQpzuPhvw8AAAAAAAAqLaQZL0OHDtWePXs0c+ZMtWjRQpK0efNmXXzxxWrUqJHeeecd2xcaScx4qXqsGS/+xli4XFJioplNvmBB0QiMijAbBlEjK0vq16/i4+bPl/r2Df78K1ZIHTsG/7rqgrI3AAAAAAAAlCPsM16eeuop5eXlKS0tTUcffbSOPvpotW7dWnl5eXryySdDWjRQGW63/zEW1vfPPy99+qm0c6f03nvStddKTZqUf95AZsMAEZGeboIAf3NaXC4pNdUcF4xDh6QhQ6TOnaUffqj8OquizExT3lZ6UJRV9paZ6cy6AAAAAAAAUCWFVPEiSV6vV3PnztXPP/8sSWrfvr0GDBhg6+KcQsVL1eXrpvXUVDM73NdN6zNmmDZjFZkxQxoxwrZlAqGxAgKpZGmXFcaE2hfvooukmTOlPn1Myugv3AlFtLfvssrlSocuFsreAAAAAAAAoOByg6CDl0OHDqlOnTpaunSpOnXqVKmFRiuCl6otmOu84e7eBNgu2HQxEJs2Se3aSfv325syVoX2XfwSAAAAAAAAQADC2mosLi5ORx55pDweT8gLBMLJ7TbXR0eMMNvyblKvqHuTFFr3JiBsMjKkdetMEDBjhtmuXVu5ICMlRbrzTvP41lulvXsrv86q0r4rJ8fe4wAAAAAAAFDjhTTj5a677tKdd96pXbt22b0eIKLKmw1jGTOGDkOIMsGki4G6+WapdWsTjEyaVLlzeTym0sVXQaW1b/Roc5zTkpPtPQ4AAAAAAAA1XkgzXrp166bVq1fr0KFDatWqlerVq1fi+SVLlti2QCfQaqzm8dURqXZt6cABqUMH6bvvpLp1nVtfadE+NgNV1LvvSuecI9WqJa1cKR19dGjnqUrtuzweqUULaft2388z4wUAAAAAAAAKLjeIDeUNhg0bFsrLgKiVkSENHVoyzGjXTjr+eHP9+aabpP/8x+lVGlVhbAaqqKFDpQEDpDVrpG3bQg9eqlL7rgMHyg9UvF4zP4fQBQAAAAAAAAEKOng5fPiwXC6X/u///k8pKSnhWBPgCKt7U3GvvWauQz//vHTqqdJFFzmytELW2IzSdWrW2IxZswhfUAkul/TKK1KjRqbkK1SBhhTR0L7r5ptNANS4sRQfL23ZUvL5mBgpKcmZtQEAAAAAAKBKCqnVWIMGDbR8+XKlpaWFYUnOo9UYihs3TrrvPql+fWnJEqlNG2fW4fFIaWllZ5Vb6IiEqLF1q/kw+pvhEi0f1g8/lIYMMY/nzjXJq1X21ry59O9/mzQzOdn85W/e3Lm1AgAAAAAAwFHB5AYxobzBqaeeqgULFoS0OKCqGTdOOvlkae9e6cILpYMHnVlHdrb/0EUyVTAbN5rjgEo7fFh6+mlp+vTAji+e4TdvLj38sAlYXK6Sx1nfR0P7rnfeMdt//lPq37+o7G3ECDOj5sUXzZCnnBzpggukQ4ccXS4AAAAAAACqhpBmvAwePFh33HGHli9fru7du6tevXolnj/77LNtWRwQDWJjpRkzpK5dzU3vt91m5qlEWlUam4FqYNYs6YYbpIQEM/ulWTP/x+7bJ119tXT66dKVV5p9Y8aYEi1fA4keeECKhllh//mPCVz89eerX9/09zvxROmLL6Q77pAeeyyyawQAAAAAAECVE1KrsZgY/4UyLpdLHn/tZaoIWo3Bl48+ks46yzx+553IXzfOyjI34Vdk/vyys2qAoHk8Uo8eJm28+mr/lS+//y6dc460bJkJKtavl444ouR5rPZdycnSL79IY8dKTz3l/NCkQL3zjnTxxab12BVXOL0aAAAAAAAAOCDsrcYKCgr8flX10AXw58wzzRxuSfq//5M2bIjcex84IL35ZvnHuFxSaqqUnh6ZNaGac7ulJ54wj59/Xlq0yKR/M2earccjffqpdMIJJnRp1kyaPbtk6GKdx2rf1bevtGOHtHu3qR7Zvz+yP5MkrV4tXXONlJsb+GvOOccETIQuAAAAAAAACEBQwcsZZ5yh3GIXqx588EHt2bOn8PudO3eqQ4cOti0OiDYTJ5oigN27zXXkSIx8WLNG6tNHeuaZon2lx2ZYomFsBqqRPn1MpYfXaxK9fv1MlUq/flLjxtKgQeYvQ48e0uLFgaV+Y8ZIRx5pBhJNnhz+n6G4w4elSy4xLcZuuCG41zZvXvR4xw4pL8/etQEAAAAAAKDaCCp4+eSTT3Sw2GTxiRMnateuXYXfHz58WL/88ot9qwOiTK1a0uuvS4mJ0ldfSePGhff93npLOv540+2pcWNTUPD221LLlmWPbdfO3JiP6s/jKVt8EjannGK2+fkl9+fmmkCmf38z/yQlJbDz1akjPfigefzgg9KWLfattSIPPCB9+635CzxxYmjnWLzY/KW84grz8wMAAAAAAAClBBW8lB4HE8J4GKDKa93a3DAvmevGn35q/4XwgwfNDfnnn29urO/TR1q6VBo82MwBX7fOzHKZMcO0IKtTR/r554rbkaHqy8w0M+uLF5+kpZn9tvN4pHvvLf+YX3+VYmODO++FF0p/+5u0b5/0r3+Fvr5gfPutdN995vEzz5i+fKHweKRt28zcl0cesW99AAAAAAAAqDZCmvEC1HTDh0sjR5rH551nruGGciHcV2CzZo3Uu7f09NPmmDvuMCFL8YKC4mMzzjvPzCqXpFtvdWZsBiIjM9N89jZtKrl/82az3/bwJTu77JuVtnGjOS4YLpf0+OPm8UsvmZKucNq717QY83jMX5oRI0I/V48eRbNvxo6VPv/cnjUCAAAAAACg2ggqeHG5XHKVGi5R+nugppg8WWrVylSk5OSUfC6QC+G+KheSkqTOnYtai330kTRpkhQXV/5abrmlaGwGN+FXTx6PNGqU7+5W1r7Ro21uO1b6g13Z44r7299MAOJyBR/cBOvmm6XVq016aSWalXHttabVWEGBqd5Zty6Cvd8AAAAAAAAQ7VzeIPqFxcTEaPDgwYqPj5ckffDBBzr11FNVr149SdLBgwc1Z84cear4Rae8vDwlJiYqNzdXCQkJTi8HUcrjMddxt271/bzLZZ5fu7bswHurcsHf37527aTPPguuG9Jbb5nWZFbbsSOPDPy1iH5ZWSacq8j8+aYaqkq86aZN0q5dUpcuwb82UDt3Sp06mfZg8+YF9vME4q+/TGna0qVm+FPxGTgpKdLUqaYvIAAAAAAAAKqFYHKDoBrzX3755SW+v+SSS8occ9lllwVzSqDKys72H7pIJlTZuFG66y7puONMIFK7trlGO3Jk+XO59+2TWrQIbj3Dh0snn2zmnN9+u7n5HtVHOItP/EpPNyHC5s2+P7BWupieHtr5U1JK9tCzg8dj/nLm5EjJyWZty5dLc+bYF7pI5i/0tddK111XMnSRikreZs0ifAEAAAAAABHn6/JI6RvDEV5BVbzUFFS8IBAzZ5oWYeESShHB0qXS8ceba+RffBH69XBEH0cqXqSi8iypZPhitZm0K1xYtUr69Vdp6NDQz5GZafqxFZ9LE67qE4/H9Ar0NwOnvJI3AAAAAACAMInk5ZGaJpjcIKgZLwCKJCcHdlzPnuaCea9epvIl0NeFUrlw3HHSNdeYx6NGMWqiOrGKT/yN1XK5TGs628O2jAwTrrRsWXJ/Sop9ocvXX5vhRpdfLu3YEdo5rICodBASyMClUGRn+w9dpKKSt/Lm13g8zIYBAAAAAAC2ifTlEfhHxYsPVLwgENYN7xV1YSp9w3u4Kxf++ENq00bKzZX+8x/pqquCPweiU0Wzgd5+O4x3LoSzRtXjkU44wZRsXX+99NRTwb8+0tUngZa8padLd94p9e8vxcUV7ef2EwAAAAAAYCOac4QfFS9ABLjd5hqpVLYKwfp+ypSyv8jCXbnQtKk0frx5fOedJoBB9ZCRIb34ou/nYmKkDh3C+OZut0kCR4wwWzv/F9rtliZPNo+nTZNWrgzu9XZUnwQr0NK17Gxp8GAztOm666Tdu7n9BAAAAAAA2M6JyyPwj+AFqIRQujCFGtgE4/rrpXbtpO3bpfvvD/08iD7165ttq1bSjBmmMuqss6SCAmnMGGfXVin9+pn5Lh6PdMstwb32558DOy6U/n3+BJKgNmsm/eMfJg3dsUN6802pTh1T6eKrbMnaN3o0bccQHrS3AwAAAIBqK9DLHnZeHoF/BC9AJWVkSOvWmQvg1oXwtWvL7xYU7rEZtWpJjz9uHk+dKv32W+XOh+gxf77ZDhlSVHwyebLpYvXxx9Ls2Y4ur3IeeaToB/nkk8BfN316YMcFWqUSiEAS1GeeMV9btpifZ/Jk6ZtvuP0EzsjMNDXn/fqZNnn9+pnvqbACAAAAgGoh0Msedl4egX/MePGBGS+IlHCOzZCkM84w17CHDJHef9++88I5HTpIq1aVnedy663So4+aSqdly0z4ViWNGWNSw/btpSefNGVbpf9y7NljHjdoYL7/5BPzIT90yPc5w9nE1NesltRUU7bmK0ENdDbMjBkmWQPs4G9AlBUS2pH4AwAAAAAcFeo8agQumNyA4MUHghdUFz//LHXuLB0+LM2ZIw0c6PSKUBlbt5oMwuWS/vhDaty46LncXKltW5NTTJ4s/fOfzq2zUnbvNqHLgQMlBxSlpEgPPmj+38Fjj0k33STdc495zus1F5bPO6/oe0skLiwHk6BmZZlKg4rMn2/KmewW7rQX0YfpigAAAABQY1j33UmRvzxSEwSTG9BqDKjGjj1WuvFG8/if//RfEICqISvLbLt2LRm6SFJiojRxonl8zz0mgKmS5s83iy8eukjmovEll0h3320qXj77rOj/Qbhc0rnnhrd/X3ncbhOSWL3fyrt4XdFsGEk64ghznN1oNVUzMV0RAAAAAGoMa7xB8+Yl90fi8ghKIngBqrlx46QmTUx7qmeecXo1qIzPPzfbU0/1/fwVV0jHH28yi7vvjtiy7OPx+B88b4mNlV591VwkLh1ehDJwKdLKmw1jSUmRYmz+n2frlpfSF+A3bzb7CV+qL6YrAgAAAECNkpEhvftu0fdHHBF9l0dqAoIXoJpr2FB64AHzeNw46b33zJiJrCxznRtVx/z5ZuuvU1Xxa/rTp0tLl0ZkWfap6M58yfTNa9nSf1VJMNUnTrFuP/FVnXPppSZhK68iJljlBVrWvtGj+YVQXTFdEQAAAABqnJ07ix7n5tp7mQGBIXgBaoCrrpJatTK/aIcNo8tQVbRxo7R6tSmEKK8L1UknSRdeaK6nV1Q8EnVq0p35vqpz1q2TXnmlZB+5CROkJUsq9160mqrZKmpv53JJqanhaW8HAAAAAHDEtm1Fjz2ekkEMIoPgBagB3ntPWr++7H66DFUdVrXLCSeYeS7leeghqU4d6YsvTGFFlVHT7syvqDpnxgwzsKdPH9NeLVQ1KdBCWVYpnL8U1uuVpkyJzuowAAAAAEBIigcvkrR1qzPrqMkIXoBqzuoy5AtdhqqOitqMFXfkkdLtt5vHt9wi/fVX+NZlK+7ML+mMM8zXgQOmBdk//ykdOmT+smZl+e8Z+Ndf0vPPS2PGmO9rWqCFsjIypC5d/D+/Z0/ElgIAAAAACL/t20t+T/ASeQQvQDVHl6Gqz+s1Yz8k6dRTA3vNrbeajGLDBumxx8K3NluVN3je+r4m3ZnfsKH0wQfS3Xeb76dMkbp1M3+w/fqV7Rm4fr10xx0mvLr6aunxx6Xffqs40JJqVqBVE+XlSatWmcf//W9Re7t77jH7rrtO+v5759YHAAAAALAVFS/OI3gBqrlAuwd98UX580Aquske4bN2rQlQ4uJM16lA1K0rPfyweTxpUsUz66NGeYPnZ80yz9ckMTHSvfeaYKV2bWnFirJ/qTdtks49V2rd2vSZ27XLDHV6+GGpadPyAy1rX00KtGqiTz811VJt20qXXFLU3u5f/5LOPls6eND83frjD6dXCgAAAACwgRW8xMaaLcFL5BG8ANVcoN2Dxo+XOnUyN8nv2FHyucxMc1O9r5vsEX5WtUvPnlK9eoG/7oILTFCzf78phKgyfA2eX7u25oUuxZ19dsXDfbxeUxL17rvSmjWm7KlhQ/Ocv0ArNbVkoFW6FhnVQ5Mm0pAh0vnnl9wfEyO98orUpo0pfbzwQunwYWfWCAAAAACwjfXP+2OPNdvSFTAIP5fXW9497jVTXl6eEhMTlZubq4SEBKeXA1SKx2NCks2b/Ve01K0rFRSYURKSqawYOtR0K/rzT3OtrvRrrRvna2IRQqRdfLHJH+6+2xQ/BGPxYunEE82f38KFUu/e4VkjwiwrK7ABP/Pnm0oGfzwe01cwJ8eksunpRZUujz9uPmCzZ0u9etmxalQVK1aYZPfUU80vm/r1nV4RAAAAAKASmjc3YcvFF0uvvWa2r77q9KqqvmByAypegGquorEZLpdp+b91q/TMM9IJJ5iONLNmSYMGmRugfQU21r7Ro2k7Fk7F57sEct29tO7dpf/7P/P4ppvMuWgXVwUF2jOwouPcbhPMWK2mrNDl8GHp7bfNkPXTTpPmzavEYlHldOwoffutqZYidAEAAACAKs3jKeok3aWL2dJqLPIIXoAaIJCxGYmJ0j/+IX33nbR0qXTjjaatVXkX571e050mOzusy6/RfvnF/I9jfHzoRQgPPGDGgyxeLPXvT7u4KinQnoGBHldabKz0ySfSgAHSvn3SmWdKH3wQ2rkQXb7+2rTuq0jHjqb1mGR+ue/cGdZlAQAAAADCY9cu09lGMmMFJIIXJxC8ADVEMGMzunaVnnhCevrpwM4d6M34CN78+Wbbu7cJT0KxcGFRG7niNm+Whg8nfKkS0tNNUlq6bM3icpl5Lenpob9HvXombBk61AxbP+ccUx4lmQQ2K4tyqaro73+XWrc21SyB2L9fuuwy80snNzesSwMAAAAA2M+a59K4sblUIBG8OIHgBahB/HUZ8qdVq8DOG+pN9qiY1Wbs1FNDe73HI40a5fs52sVVIRX1DJSkKVMq/ktdkdq1pbfeki65xHwoLr7YlMKlpZkyKcqlqpb166Xly00lS6Ch3P790hdfSL/+Kl1+edFtUgAAAACAKsEKXpKSzKwXyTQ1OHTIuTXVRAQvAPyq6CZ7SWrRonI32dutOt2YX1BgfgYptPkukmkDt2mT/+dpF1eFBNIz0A5xcdLLL0sjR5oPyLPPlv0QUS5VNXz4odn27m1udQpEkyZm3k98vPTee9LEidXrFysAAAAAVHPbt5tts2bmn4LWPZrWfkQGwQsAv8q7yd6Sny/9+GPk1lSezMzqdWP+Tz9JO3aYDlAnnhjaOeyayY4oEUzPwMqIiTH9Bps08f085VJVgxW8DBkS3OtOOEF65hnz+O67zS1S1eUXKwAAAABUc8UrXmJiTAAj0W4s0gheAJTL3032zZubFmM7dkh9+kivvebM+iyZmeYG/Op0Y7413+Wkk6RatUI7R7hnssMBwfYMDNWXX5q/4P5QLhXd9u4t6lV41lnBv/7KK6XTTzePS38OqvIvVgAAAACo5ooHL1JRuzGCl8gieAFQIV832W/aJK1cKZ1xhhncfskl0s03S4cPR3591hwT6yb84qryjfnWNdNQ24xJkZnJjmoq0DKozZv9P0eLKufMm2dKElu3ltq3D/71Ho+0YoXv56ryL1YAAAAAqOaKtxqTCF6cQvACICC+brJv2FB6/33prrvMMZMnS4MGlX+TfDhUxzkmHo+0YIF5fOqpoZ8nkHZxdsxkRzUUaBnU6NHSrbdKf/5Zcn9V6/1X3UIiq83YWWeVP6jLn+zs8kO1qviLFQAAAABqAH8VL9Z+RAbBC4BKcbul++837cjq1TM3WZ94YtHcl0hcy6yOc0yWLpVyc6WEBKlbt8qdy1+7uDp17J3JjmqmonIpyTy3Y4f06qtS3bpF+//736rV+6+qhUSBeOwx8xf86qtDe311/MUKAAAAADUArcaiA8ELAFuce670zTfS0UebtmS9ekljxkTmWmZ1nGNitRk75RQpNrby5yveLu6++8y+2NjgZ26jBimvXMrlMl8zZ0rvvCM9+GBR2VR+vnTFFVWn9191HBAlmdT23HOlLl1Ce32gvzAPHQrt/AAAAACAsCjdaswKYAheIovgBYBtOnWSvvtOGjhQ+usv6fHHI3Mt07oxvzz160u9e9v3nuE2f77ZVma+S2lWu7g775SaNDGdob7+2r7zoxryVy6VkmL2X3CBNGyYdPnlRc+99JJUUOD/nNHUoqq6DoiyQyAVT5L5s7/4Yunnn8s+V93atwEAAABAlPN6qXiJFgQvAGzVqJGZ+9Kgge/nw3Et0+2uOKDYu1c67zxp/3573jOcDh2SvvjCPK7MfBd/YmJMOCZJc+bYf35UM8XLpWbMMNu1a/33qPP3l7+0aGhRVR0HREkmDJkwQfrjj9DPEUjFU48e5vsZM6QOHUwAY/0//OrYvg0AAAAAolxennTwoHlsVbwQvDiD4AWA7b76quyc7eLsvpb5/ffmhmrJBD/FpaZKt9wixcebQOjUUyt3LTISvv9e2rdPatxY6tw5PO8xaJDZErwgIFa51IgRZmu1FfOlKvX+q45zTDZvll55Rbr33vIrjwJRUcXTt99KS5aYqievV/r0UzPsq7q2bwMAAACAKGe1Gatfv2gUqxW8WPfJITJsmBwAACVF8lrm3r3mWvDhw6aiZcYM6csvzbmTk023HLfbXBc8+2xznbB3b+njj6Vjjqn8+4eD1WbslFNMdUo4nH662f7wg7njwfofYaDSrBZVmzf7buHlcpnn09Mjv7bSqlJIFKjZs822R4+iuvLKyMiQhg41SXnpX6yS1K2bmfPzww/S+vVSnTrlt29zuUzJ49Ch5Qd4AAAAAICglW4zJhVd88nLM51grEAG4UXFCwDbRfJa5k03SatXm8qWZ581A+N93Zjfp4+0cKHpdLN6tdSrlwlhotHnn5ttONqMWZo1k7p3N48/+SR874MaqKIWVZI0ZYq0c6fzQ4Z69JBq1fL/vMtlfrlEQ0gUqA8+MNshQ+w7ZyAVT926mYS7urZvAwAAAIAqwFfwkpBgOsEUfx7hR/ACwHaBzGS244b3N96QXnzRVIW89lrZNmOlHXusuc57/PHSjh1m5MD771duDXY7eNAERFLFc2sqa/Bgs6XdGGxXUYuqoUPNPJCTT5YefdR3dUQkvPiilJ9vHvsKibxeqX//qlOZ8ddf0ty55vFZZzmzhurYvg0AAAAAqgir1Zg130Uy/7xlzkvkEbwAsF15N7xbmjev3PiBdeukv//dPL7rrsBDnObNpQULzIyTv/6SzjlHmjbNPOfxSFlZZl5MVpb5PtK++UY6cMDcmdC+fXjfy5rz8umnzvysqOYyMsxf1PnzTQ/A+fOltWvN/vx8qUkT0yPw1ltNpcTu3ZFf48iR0t13S//6V9mQqGlTs33pJZPsVgXz55tfbKmpUpcuzqyhOrZvAwAAAIAqwlfFi0Tw4gSCFwBh4e+G96ZNpbg4M0D+iitCu+B/+LB0ySVSbq5pGTZuXHCvr1/fVLpcdZUJf0aOlM4917Qh69dPuugis01Li/wMaGu+S79+5VcM2aFnT6lhQ2nXLum778L7Xqih/LWoqlPHhDHPPGNafb3/vmlVtWiReT5SKWhMjBlCf999ZUOizZvNrBLJ/LL69NPwrMFOVpuxs84K/y8QfyoqeayK7dsAAAAAoIqoKHih1VjkELwACBtfN7zn5Jg5zLGxZt/IkcF3GXrgAdOOKyHB3IgeGxv82uLipOnTpXvuMd9nZpYdS7B5szR8eGTDl0jMd7HExkqnnWYe024MEedySf/4hynzOvpoM5j9pJNMIhrOFHTjRhOo/PVXyf2lQ6LYWGnyZOnCC03ae+650uLF9qwhXGrXlhITnWszJgVW8jhlStVp3wYAAAAAVYivVmMSFS9OIHgBEFa+bng/80wTusTEmPDjllsCD18WLjQ3qEumRVjr1qGvzeUybcr8zYax1jR6dGRace3fb65BS+Gf72Kx2o0RvMAx3bqZQGP4cOnQIemFF8KXglr9BZ94QrruuoqPj4kxrcb695f27jWDkVavrtwawunxx6U//pBOP93ZdfgreaxTx+zPyHBmXQAAAABQzfmreLG+J3iJHIIXAI447zzpP/8xjydPLgpTyrNnj7kBvqBAuuwyE+ZUVnZ2+aMlvF5zg3x2duXfqyJffWWuO6ekmAKASBg40GwXLZJ27IjMewJlJCaatmING/p+3o4U1Os1FTaLF0uNG0vjxwf2uvh4E/h062ZCjYEDTZ/DaBUXF1oZoN2KlzxOnWrKC//4g9AFAAAAAMKIGS/RIyqCl6efflppaWmqXbu2evbsqUVWj/cKvP7663K5XBo2bFiJ/VdccYVcLleJr0HWbd0AosaVV5obzyVpwgQTwPjj9Up//7u0YYN01FHSU0/Zs4acHHuPq4zibcYiNZ6hZUupc2fz3/ezzyLznoBPX35p0lV/KpuCTp0qvfKKKbt7803TvixQCQnS7Nnml89VV5nvo83vvwfftzHcrJLHm24yw7jq1XN6RQAAAABQrdFqLHo4Hry88cYbGjNmjMaPH68lS5aoa9euGjhwoLZbnxI/1q1bp1tuuUXpfoazDho0SDk5OYVfM2fODMfyAVTSjTeamS2SdPPN0nPP+T7u5ZfNtdLYWHNjfIMG9rx/crK9x1XG/PlmG6k2Y5bBg82WdmPwJyKz7sOZgs6bZ3oaStJjj4U2RKl5c+nHH6U773RucL0/27ZJxxxjvvbtc3o15Yu2cAgAAAAAqokDB6S8PPPYX8WLVRGD8HM8eJk8ebKuueYaXXnllerQoYOmTZumunXr6oUXXvD7Go/Ho4svvlj33HOPjjrqKJ/HxMfHq3nz5oVfjfwNcQDguDvvlO64wzz+xz/M/JfiF3pffVW6/nrz/L33Sj162Pfe6emmtVd511GTk81x4fTnn9J335nHkQ5eis95KSiI7Hsj+mVmhnfWfaFA080FC8yslkCtXStdcIH5pXL55ab6IlT16xc93rtXeuQR0x8w7KlUBWbPNoFGo0bRXVUycaLUrp30yy9OrwQAAAAAqh0rVKlVy3T0Lq54xQv3w0WGo8FLfn6+Fi9erAEDBhTui4mJ0YABA/T111/7fd29996rZs2a6aqrrvJ7TFZWlpo1a6Z27dpp5MiR2rlzp99jDx48qLy8vBJfACJr4kQTrni90qWXmmTeutB76aVm8HzHjtJtt9n7vm636UAk+Q9fDh82owrCKTvbXK896iipVavwvldpffqY68nbt0tLl0b2vRHdMjPNTPtwzbovIZAUVJKefVZq00b64ouyz/kqzbEqZE48UZo2zZ5qlYICM+vlttukI46IQCpVgQ8/NNuzzors+wZr4ULpt9+k115zeiUAAAAAUO0UbzNW+p++VgVM8aoYhJejwcuOHTvk8XiUVKr2KSkpSVv9NJz78ssv9fzzz2v69Ol+zzto0CC98sormjdvnh566CEtWLBAgwcPlsfPXaiTJk1SYmJi4VdqamroPxSAkLhcZt5L377mmqavrHTlSum99+x/74wMadYsM++kuBYtzE34f/whnXxyeG/SttqMhdIBqbJq1ZL69zePaTcGi8cjjRrl+04YO2bdl1FeCupyma/rr5eOPFLascOklMX5K83ZulX6/nvzfO3a9qw1JsYEOZKpfCkuLKlUOQ4elD791DweMiQy7xmqSy4x21df5RYrhEVE2iICAAAAUcqqeCndZkyS6tYtatvPnJfIcLzVWDD+/PNPXXrppZo+fbqaNGni97gLL7xQZ599tjp37qxhw4bpww8/1HfffaesrCyfx48dO1a5ubmFXxs3bgzTTwCgPF6vtHp1+cfYeqG3mIwMU9Uyf75pdTZ/vrRhg7R4sdShg7Rli3TKKdJPP9n/3pL0+edmG+k2Y5bi7cYAyVRhla50Ka6ys+598peCpqSY/U89Jf36q/TZZ2af5ZxzpHPP9V+as2RJyeMry+OR3n7b93OBplJ2XSFesMCEP8nJUrduoZ0jUoYONeV1a9dK5VQ2A6GIWFtEAAAAIEqVF7xIJduNIfwcDV6aNGkit9utbaWm+mzbtk3NrU9CMWvWrNG6des0ZMgQxcbGKjY2Vq+88oref/99xcbGas2aNT7f56ijjlKTJk202s8V3fj4eCUkJJT4AhB5jlzoLcbtNhU3I0aYrdttrmVmZUnHHWf+B6xvX3MN1067d0s//GAeOx28fPWVtGePM2tAdAnnrPty+UpB1641+yUpPr7k0KWFC6V33/V9rrCU5qjyv6zsvEJstRk780xTiRPN6tYt+nOk3RhsFNG2iAAAAECUKt5qzBfrcnupS/EIE0f/hV6rVi11795d8+bNK9xXUFCgefPmqVevXmWOP/bYY7V8+XItXbq08Ovss89Wv379tHTpUr8twjZt2qSdO3cqOdDBvQAc4diF3go0bWoqUk480bRAO/VU6dtv7Tv/ggXmOu2xxwY+X9xuaWnm/T0eqdivZNRggX4Ww/KZ9ZWC+rNjR/nnCkdiG+gvoS1bTJ/CkSOlN94wtxXZeYXY65U++MA8jvY2Y5aLLzbbN96Q8vOdXQuqhYi3RQQAAACiFBUv0cXxWyPHjBmj6dOn6+WXX9aqVas0cuRI7du3T1deeaUk6bLLLtPYsWMlSbVr11anTp1KfDVs2FANGjRQp06dVKtWLe3du1e33nqrvvnmG61bt07z5s3T0KFDdcwxx2jgwIFO/qgAKuDohd4KNGokzZ1rBtHn5koDBhRdx61sxyBrvotT1S4W2o1FVrTPIkhJKT/vcLmk1NSSxSeO2L8/sOPsTGwD/SV06JD5RTFtmnThheZ1F1xg3xVir1d65hkz+8Ya1BTtTj3V/L/9nTulTz5xejWoBpyulgUAAACiRUXBi7Wf4CUyYp1ewAUXXKA//vhD48aN09atW3Xcccdpzpw5SvrfJ2HDhg2KCaJ1htvt1rJly/Tyyy9rz549atGihU4//XTdd999io+PD9ePAcAG6enmYu/mzb6vS7pc5nmnLvQmJJhQ4uyzTVgyaJB0663S88+XvOiTkmJmhFsddfzxeMyFIOsG91NOCd/aAzFokDRlivTxx+a/f+n55rBPZqa5QzuUz00k/PqruT5uXf93uUr+nbQ+G1OmlB/ORIQTiW2gv6zOOcektllZ5uuHH6TDh/2ft/gV4r59K15HTIz5i2ulplVBbKx07bXm5zzySKdXg2ogWqtlAQAAgEgLtNUYwUtkuLxeX1cMara8vDwlJiYqNzeXeS9AhFldeCTfF3pnzXL+wvRff5k1+KsMCWStvi68t2ghPfmkcz/fX39JRxwhHTggLV8uderkzDqqO+szXvp/faPlM75ihSme2LZNat9eGjNGuuee6A2J5PGYXnkVhSBr19qbEoXyy2r6dBM6VGTGDNNmDUCFsrICqxidPz+wPBMAAACoqjp1Mv+m/+wz06mltOefl66+Who8WJo9O/Lrqw6CyQ0cbzUGAMVlZJjrlS1bltyfkuL8BWlLnTrS229LtWv7fr6ijkH+Rjzk5Dg7BLhOnaKLV7QbC49on0Xwww/mwuS2bVKXLuaC5tVXm1n3c+cWfeYzM6Pj76IkE6ZMnWoely7TCmdpTii/rNq0Cezc8+ebnobl2blTuu026YsvAjsnUE1ZBWj+qjSjpi0iAAAAEGaBznixjkN4EbwAiDoZGeZC7/z55sbv+fPNzepRc6FX0qJFpjLEH6tj0HPPSb//LuXlmX3RfuHd6lj08cfOvH91F82zCL791rQX27FDOuEE8/fOKk92u00VjHXHzIIFkV9fuZxKbIP9ZVXRFWLL9OlSq1bSXXcV1YpbrOFA48ZJjzwi3XCDHT9J5Hm90vffS08/7fRKUMUVz15Li6q2iAAAAEAYHT5s7s+TaDUWLQheAEQlt9vceT9ihNlG2wWTQHvFX3eddPTRUmKiFB8vNW0avRfepaLgJTtb2rvXmTVUZ9E6i+DLL6XTTpP27JF69zbVLUccUfa4U081288/j+jyAuNUYhvML6uKqnNcLunGG02Pt9xcaeJE00btnnvMMZmZ5vt+/aR//9vsW7fOuTK5yti4UTrxRPPzbt7s9GpQxWVkSJMnl90fTdWyAAAAQDjt2GGuKcXESE2a+D7GqoTZtk0qKIjc2moqghcACEGgc7qbNZPq1jWPDx2Sdu8O7HVODQFu00Y66iiz1vnznVlDdebEHPiKzJsnDRwo/fmnuZ7/yScmKPTFCl6++MJ8RqJOtCe2UsXVOU88If30k/TOO6b06K+/pLg4/z0K//zT2R6FoTrySOmkk8y/DGbOdHo1qAasf0QW/6v1/feELgAAAKgZrPZhTZr4/6ewVQnj8RRVxyB8CF4AIASB9pTfskXat898bdhgOggFIpIX3otzuYqqXpjzYi+vt+IWXeGeRWB1qZo502w//FA680xp/34Tvnz0kVS/vv/Xd+4sNW5sPs/ffReeNdYIFVXnxMRIw4aZnoaffSb9/e/+exRanOxRGKpLLjHbV191dh2oFn780WyHDJE6dDCPv/rKufUAAAAAkWR1qfbXZkySatUy/6aXaDcWCQQvABCCYOd5161rLqhfeWX0DwEuPuelvOu8CJzXK40dK02YULTP32cgXLMIinepuugisx0yRDp4UBo6VHrvPalOnfLPERNjXidFabuxqiSQ6hyXywzWWb48unsUhuq880w1z48/miofO5ROFwMNo0J9HaLGsmVm26WLdMop5nHUzaMCAAAAwsSqeLEqwf2x5rxYxyN8CF4AIEShzPMONrBxQr9+5i6ItWul335zbh3VRUGBKVZ46CHz/eOPS2+/XfZzU7du+GYR+OtSZbnoIjODKBBRPeeluorW4UCVdcQR0hlnmMevvVb58/lKF9PSKm7DFurrEFWsipeuXYuCl6wsx5YDAAAARFSwwQsVL+FH8AIAlRDKPO9QAptIql+/qOKmsu3GqtJN5OFYq8cjXXut9OSTJlibNs10hCr+uRk3zhwbHy+dfXbl39PXGsrrUuVySbfcEvjPawUvX31lxo8gAqJxOJBdrHZjr71WuemO/tLFzZvLn4ET6usQVXbsMK09JdMS0Qpefvwx8NlqAAAAQFUWSKsxqSiYIXgJP4IXAKikUOZ5hxLYRFLxdmOhqko3kYdjrYcOSZddJj3/vGnR9fLLZlSHxfrc3H23ufF/927p668r+YP4kJ1tb5eqtm2lFi1Mi7JwrBc+BDpUyskehaE66ywpIcH8hVm/PrRzlJcuer3ma+RI07Jt82bpzz/NvopeJ1XN2Tk10PLlZnvUUVKDBuYuvnbtzB/jl186uzYAAAAgEqh4iT4ELwDgkFACm0ixgpesrNCqGqrSTeThWOvBg9IFF5hQLTZWev116dJLfR8bGysNHmwef/hh8O9VEbu7VLlcUv/+5jHtxiLEph6FUVmBVru2SfA2bZJatw7tHBWli5K5/atLFxNgJSSYv3jPPFM9Z+fUQFabsS5divbRbgwAAAA1CcFL9CF4AQCU0bGjuT554ID0xRfBvbYq3URux1pLX8zeu1caNkx65x0zKycz08wQL8+QIWYbjuAlHF2qmPPigEr2KIzqCrQOHSqXPAeaGtavbwIXybQ127fP3vPDMcuWma2v4GXBgsivBwAAAIg0K3ipqNWYFbxYxyN8CF4AAGW4XKG3G7OjtVWk7syv7Fp9Xcxu0sTMxqlTxwQpVqhSnoEDzXXnlSul338P6Ufxy+pS5U8oXar69TPbRYukvLzKrQ9BCLFHYZWpQPN4zLCOYAU6G+aDD6T8fBO45ORI3bsH9rqqODunhrEqXrp2LdpnBS8//CDl5kZ+TQAAAEAkWTNeqHiJHgQvAACfrOBlzpzgXhfozeE//eR7fyTvzA90rddeK916q6lisf7PjL+L2QcPmu2dd0qnnRbY+Rs2LAo+7K56Kd6lqrQgulSV0KqVdPTR5jo5XZgiLMgehVWmAu2jj6QjjzR/2YKxYoV0yy3lH1M8XXS5pLp1zb82+vUrf3aOJMXFmdI1RK3Dh83HQCpZ8dKypXTMMSaXY84LAAAAqjOvN/DgxXqe4CX8CF4AAD4NGGCu6f7yi7mpPlDW3RMVufFG09JszBjpk0/MLJlI35kf6I3sv/0mPfqoKSxISjIX8y691PfFbMtzzwV3MduqjPngg8BfE6jiFyOLC7BLlU+0G6sa7KhAi4gjj5S2bDEBzO7dgb1m8WJT1rB1q3m9yxXcDJzyZudYDh2Svvsu4B8DkffbbybwrldPOuqoks/RbgwAAAA1we7d5p8uktS0afnHWtdsduwoeg3Cg+AFAOBTYqLUu7d5/Mkngb0mL0969tmKj6tVy1znXLlSevxxU13TqJGpcInknfmBtOFKTpZefNHciN+xo9m/Zo20f3/55w72YvZZZ5ntggX2t++aMcNsTz896C5VfhG8VA2BVnU5Psakc2eTEObnmzSwItnZ5kO4c6fUo4fpJxXKDBx/s3NSU81f/AkTpBtuKNr/559B/VgIP6vNWOfOUkypf9n07Wu2BC8AAACozqxql8REqXbt8o9t3LjonjTrdQgPghcAgF/BzHn57jvp+OOlN94ouvjl6+Zzl8vMbtm5U3rzTemqq8y10YMHi9p0+RKOO/PdbtMSzBdr7U89JV1xhQmUfvpJ2rXLtB0LRDAXs9u2NV+HDkmffhr46yri9UqvvWYeX3JJUF2qymXNeVm61PxZIjoFWtUVFWNMLr7YbF99teJjt26V9u41JQ1z50pHHBHyDBy/r7viCmn8+KK/KPv3m19yV18t7dlj9kVqIFVNEuR/02XLzNZXZZ9V8bJ4MZkZoge/NgAAgN22bTPbitqMSeafN82alXwdwoPgBQDglxW8zJtnbkT3paBAeuwxUx2zZo2Z/5GdLb39dvk3nzdqJJ13nvSf/0gbNkgPPxzYmuy+M//XX802Pt7/Wotr1Eg644zAzh3sxWyr6sXOOS9LlpifsU4dadgw+86blFRUAZSVZd95Ya/09PLb/xUff+K4ESPMgr74Qlq/vvxjzzvPtCWbPVtq0KBof5AzcIJ63aefSqtXS88/L3XoIN1+e+QGUtUUIQz5sipeunYt+1xqqtS6tbmwvXBhWFYMBCWSc+wAAEDNYQUoVqBSEevfiMx5CS+CFwCAX8cdZy6w79vn+6LV9u3SmWea2daHD0vnnmsqIHr3Du7mc5dLOvHEwNZk5535ubkm+JGkd94J/EZ5q0WZv7EQoV7MtoKX2bPtuwPWqnY5++yS16ftQLux6OfxmNCtPL7GnzgiNbWoN5TVH6+4WbNM2Ztl0CCpbt2ILE2SSS6zs6V27UwC/PDDkRtIVROEOOSrvIoXiXZjiB6RnmMHAABqDqtlWCAVLxLBS6QQvAAA/IqJkQYONI+ffbZkW4x588wdxnPmmB6i06ZJb70lNWxY9Ppgbj4PV5hRnv/8x3Qr6tjRXMMNdK3lzeQub5Z3RU46yfRk/eMPadGi4F7ri8cjvf66eWx1cbITwUv0u/tuEyLWr+87tLzvvtDn/ITFJZeY7bPPmvDF+oUzbZp0/vnSaaeZyZFOOekk07fKX4oZroFU1Z3HI40aFfSQr127ii5id+7s+9RWuzEq8+CkED/iAAAAAQmm1Vjx4whewovgBQBQriOOMNs33ihqi9GokTRggPkf6Q4dzHyXv//df2gSiPLCDIudd+YfOlT0fmPGBL92fzO5K5rlXZ64uKL2bna0G8vKMjfmH3FEUYBmp1NOMf/dfv5Z2rLF/vOjcrKypEceMY//+19TLGJVdZ15ptmfmRllF/ri4kxKtH69SQv79TPTH0eONFcnTzvNpJNO+u678geGhGMgVXWXnV22DKA4P/9NrWqXtDT/HwsrePn+e1O9CTghxI84AABAQEJtNcaMl/AieAEA+JWZWRROFGddczztNHMNslMne97PX5ghSZdeau+d+W+/bS5yNGtmAqVQhDrLuzxDhpjtBx+Efg6L1WbsvPOkWrUqf77SGjUys8Yl87Mjeuzebf7OeL1mFvywYSUr0F54wVyoXrJEevFFp1f7P5mZ0uWXmzK04nJzzTYjQ3riCVOK56RAB03ZPZCqOgvxv6kVvPia72JJSzOzxw4flr76KrTlAZXFrw0AABBOtBqLTgQvAACfymuLYfn557JD6SurdJhx221m/0cflX+TeTC8Xumxx8zj6683rdJCFeosb38GDTLXlZcvr3i+eHkOHDDhkhSeNmMW2o1FH69X+sc/zN3VbdpIjz9e9phmzaTx483jO+8syjYcE8gvnO++kwoKIrcmfwIdNGXnQKrqLsT/pj/+aLb+5rtYaDcGp/FrAwAAhFOwrcYIXiKD4AUA4FNFbTGk8LXFKB5mPPCA1LattHOnaTVmh4ULTduZ+HjTwSiaNG4s9e5tHn/0Uejn+egjKS/PzMXp08eetflC8BJ9/vtf6c03pdhYU/VUv77v466/3syJ/+MP6d57I7vGMpz8hROsigZSSeZfPHYOpKrurP+m/rhcZoDYSSeV2G1VvAQavCxYEPoSgcpwYo4dAACoOQheohPBCwDAp2hpixEbK91zj3n86KNmmHJlTZ5stpddJjVtWvnz2c2OdmNWm7GLLgpvZ6aTTjJ/RuvWmTZrcNbatdINN5jHEyZIJ57o/9hatYrCzCeekH75JdyrK0e0/MIJRCADqf76S/r998itqapzu6VJk3w/53KZSqg9e0wp16FDkkyR1E8/mUPKazUmmSBfkhYtkvbvt2XFQFCK/9rwxeu1d44dAACoWaxWY4HOeLECGoKX8CJ4AQD4FE1tMc4/X+rc2VRwPPpo5c61Zo307rvm8T//WemlhcVZZ5nt55+XHXcRiD17iqplQp1fE6j69aWePc1jql6cdfiwdMklpiXfSSdJd9xR8WsGDTKft8OHHf77EE2/cALhbyBVy5ZmqEhennT66dKWLY4sr0qyBkWVvvKckmIGFcXESM8/L515ppSbq99+My0V69aVjjqq/FO3bm1Oc+iQ9PXX4Vk+UBHr14avmWsdOtg7xw4AANQc+/aZLyn4ipe8PHPPGMKD4AUA4FM0tcWIiZHuu888njq16G6OUEyZYu4sPeMMqX17W5Znu/btzYXE/Hxp7tzgX//22+a1nTpV3ILHDrQbiw4TJ5rh4QkJpt1YoHdOT54sxcVJH38szZ4d3jX6FU2/cAJVeiDV/PlmMNM330hHH22eGzTIJKEo36ZN0uuvmz/nuXNL/jddu1aaPl167z2pXj3ps8+kk07Sss93SDKhfEWfdZeLdmOIDv36FRZt6amnTFvIuDhp5cro6KQIAACqHqvNWJ06/ttMl5aYWDSv13o97EfwAgDwqbxuOtb3kWyLcfbZUo8epk2Mv440Fdm9W3rhBfN4zBj71mY3l6uo6uXDD4N/vdVm7OKL7VtTeYoHL+XNRkf4fPNN0ZyWf//bFF0Eqk0bafRo8/if/zShXcRF2y+cQBUfSNW3r/k+KUn69FNzG9ny5aZ3oJ23kXk8Zkr8zJlm6/HYd26npKRIP/5oPrx9+5b9byqZX4pffGGqnn76ST/eZn7RBRouW+3GsrLsXToQDOt/Jzt0MHO2zjtPuvJK89wDDzi7NgAAUDUVbzNW3hjK4lwu5rxEAsELAMAvf910UlLM/ki2xXC5pPvvN4+feabiOdy+PPecCW66dCkKC6KVNeflww+lgoLAX7d5c9GFxREjbF+WT716SbVrm//D9vPPlT9fdbyuHE5//mlajHk85s88lMDtX/8yecGvv5p5L46Ipl84lXXUUdInn5hbyb780lRr2CEz06Rq/fqZPoL9+pnvMzPtOb+TjjnGzHApz/HHS99+K3XurGX7TH+xLp0D+wVpVbx8+y3tFOCczz4z2wEDivbdfrvJFz/5RPr+e2fWBQAAqi6rYiXQNmMWgpfwI3gBAJTLVzedtWuduQY6YIB08snSwYNFIUyg8vOLLiiPGRP4nSBOOflkUya8bZu0eHHgr3v9dXM37UknSa1ahW99xcXHm/eTKt9urDpfVw6XUaPM7KIjjzQFA6FISCiqJLv3XgfLzaPpF05ldekiffCB+UO58MLKny8zUxo+vGzqvHmz2V8V/5J89JGpYglGaqr05ZdaVvdvkqSuxxX750w5qe0xx5himfx8E74ATrDah552WtG+o44qmsc2cWLk1wQAAKq2UIMX63iCl/AheAEAVMhXNx0nFK96ef556fffA3/tW2+ZOdfNm9tzDTTcatWSBg40j4NpN2a1GbMu4kSKHXNequN15XB7+23pxRfN343//ldq2DD0c11+uXTiiaaC5s47bVti8KLlF44d0tOlkSOLvg+1j5vHYxI2X738rH2jR1et8rCtW82Hrm9fM2AoCLs9Cdqwv6kkM+NFkrliXU5q63LRbgzOWrvWhOSxsUUVWJaxY81n9J13pJ9+cmZ9AACgaireaiwYVsULM17Ch+AFAFClpKebQOLwYemeewJ7jddrBohL0g03FA2Ri3ZWu7EPPgjs+FWrpB9+MBd1zjsvfOvyxQpe5s8P7dpvVb2uHOm2aMXf7623pKuvNvvvuMNUSVVGTEzRmJUXX6Tlje127TJXWx99NPgPzptvlt9f0euVNm6sOtO5vV7p73+Xdu6UunaV+vcP6uXLl5ttq1b/Cxvvvlu6664KU1vrYveCBZVbPhAKq81Yr15SgwYln2vfvqiwL9Q5dgAAoGai1Vj0IngBAFQ5VtXLq6+asKEiCxZIS5ZIdepUPEIgmgwebO6A/eEHc/2wIjNmmO2gQVKTJuFdW2ndu5sLSbt3mxnZwcrOrnrXlSPdFq30+51/vrRnj2lTM2GCPe/Rq5eZF+P1+g/CEKLMTOmbb6Rbb5WaNi3/g+P1mh6Dd99tWpYFWsKWk2P/usORLr78svT++6a075VXzDYI1u+YLl3+tz5/PfZKpbZW8PLNN9KBAyGtHAiZ1Was+HyX4u66y2xff11avToyawIAAFUfwUv0IngBAFQ5J5wgDRtmhs6PH1/x8Va1yxVXSI0bh3Nl9mrWTPqbGWNQYbsxr7coeIl0mzGpZOuUUNqNBXq9OBzXlUMR6bZo/t5PMu1rgmlHV5EHH5Tq1ZO++qroMwUbXH21dPbZ5vHu3SWfsz44zzxjyvKOPNL8orv/flPeERPg/2VPTrZ3zeFIFzdsMKmeZAYKFfYKC9yyZWbbtatMGrtrl/+Di6W27dqZf5AeOCAtWhT80oFQeTzSvHnmcfH5LsV16yadcYb5/zYPPhi5taH6i3R1LgAgsirbaozgJXwIXgAAVdJ995lqkLfeMhUh/vz6a1GrrtGjI7I0W511ltlWdGH922/NzJt69Yqu7UZaZea8BHq92O7ryqGIdFu08t7PYuf7tWxZNOPlttukvXvtOW+N5/GY0jtfrD/ce++Vnn7aJGz16pneQ6+8YhLHlBTzS8+f+HjTr8gu4UgXCwqkq66S8vJMqnzLLSEtrUTFSxCprctFuzE444cfTD6YkGBmafljVb288orJC4HKinR1LgAg8kKteLGOJ3gJH4IXAECV1KmTmb0tSePG+T9uyhSzHTJEats27MuynRW8zJ0r7d/v/7jXXjPbc84x12udYAUvX3whHToU3GvT080Ff39cLik11RzntEi3RXOiDduYMaaF2ZYtZmY5d8raIJA/yK1bzS+2Dz+UduyQ3n5buvRSc/uaNYDHX/hy8KC5ovvtt5Vfa7jSxdmzzS+zOnVMuzG3O6SlWcPHu3RR0KktwQucYM136dfPVIj607u31Lev+d/QRx6JyNJQjUW6OhcA4IzKthrbto0W0+FC8AIAqLImTDDX7T780PTsL23nTumll8zjMWMiuTL7dO5sug4dOOC/kuTwYemNN8xjJ9qMWTp3Nq3c/r+9+45von7jAP5JU2hLgbKkjBZBVPZQUUAoFEVwMSx7w09BBGQpboYTFZUhCg5kqAUFynQgQpmCIAoyBJGh7A1llybf3x8PR5o2SS/JZbWf9+vV110vN75JLpf2+9z3eS5eBDZudG/bsDAplO3KuHEe9dMa6tgx4JNP9K1rVFq0QKRhi4wE3n9f5keP5p2yhtD7BrVoATzyiLwJmSUlAXPmZI9QxscDY8dKZPnAARmm5O1/Tr6K9j3yiARcJkzwOBK+Zw9w+bLEbm69FRKNdTUaKHPU9tw5JCbK4l9+AdLTPWoCkdu0+i7O0oxl9sorMv3sM1tHCpG7/D06l4iIAiM93ZbF2N1UY1qg5vJl4Px5Y9tFgoEXIiIKWbfdJnVbAFtHRWaffCJ/RNxxh+0u51BjMuWcbuznn4ETJ6Ret7Oivf4QFiYd84D76cbGj5eO0LAweR5Zvfqq9Dv7iqv851YrsGSJ3B0aFyeFj/UwKi1aoNKwOeuM4Z2yHjLijUxKAvbvB1JTpQBPaqoU+Rk8WKKdTzwBfPWV65RkOdmwwZbvKCcpKc6jF44+VCYT0L27tNNDWpqx6tWvB2LNZuejgbTfx42T/0hvvx1VPn4aN5Ww4vJl9wPE2bBwAulw6RKwZo3M6wm83HcfULeu3HCh1agjclcgRssSEZH/nTghU7MZKFbMvW2jo4FChWSe6cZ8g4EXIiIKacOHA/nySdHa1FTb8qtXgQ8/lPmhQ73rhwy0Fi1kunix4zsXtTRj7dvLaxFIntR5Wb0aGDZM5seNk4EBWr+y9txd1fHxlrP85599JrXNb7kFePBByfqUkQHccw9QtKi+G+yNkJAAlCnj/HFfpGGzWJzXROKdsh5yZ2SGK2az5CLq1Emm2jCwwoXlpI2Pt6374YfAzp0y7ypIcO2aRBTr15ce319+0fecPvxQjvfiixKh1Dj6UJUrZ0i07s8/ZVqrVqaFzkYDxcXJ8qQkYP584PhxmD6aiEYXfgCQKd2YJwEUFk4gnVavlvhkuXJyw0hOTCZb7PPjj6U2jNcYJMxzAjFaloiI/E8bHVuypNzE6C4t3RgDL77BwAsREYW0m28GnnxS5l95xdYpPGuW/PFQpowEJEJZYqLcjXLoELB5s/1jly4B8+bJfJcu/m5Zdlrg5ZdfZLRRTo4ckfcnI0P6kQcMsO9XfvttWW/BArnR32jO8p8fPAj06SOBvX//BYoUAZ5+Wjp9f/0V+PxzWc9RH7pSwHvvGZcWLSxMOuwcyXxDv5Fp2HinrA/oHZlh1Bu5aBEwcKDUfRk2zHGQYO5c4J13gAoV5AO3fj2QPz/Qtav89+YqSFS4sIzOOX5cttP+03P2oTp82JChUtqIl5o1szzgbDSQNlTuiSdkeGDFiki8IoGXFeM3y4fZ3QAKCyeQG7Q0Y02b6r8J5NFHJbh44YLtJhKPMUiYJwVqtCwREfnX8eMydTfNmEZLN8bAi28w8EJERCHvpZck3/8vv0gfYnIy8Npr8tjTT0s/YiiLjLSlJ1m0yP6xhQulpkqFCkC9ev5vW1a33y7BrqtXgXXrXK977RrQoYP8kVetmtysn7VTqmpV6ayyWoGPPjK2ra7yn2siIqRO0OHDUpaiRg1Z7uwGe639WQNk3pgxw9avnfUP6sw39BuJd8r6iJ6RGUa5+27JsXjhgkQCHQUJ2rWT4x46JP91jRoF/Pcf8OWXwKRJsp6zINHUqbLuvHnAyJGyzGKR6KmrD5WXQ6W0ES/ZAi+A89FAmvvvB7ZuReP/3QoA+OX4rbjW+yn3AigsnEBuWrpUpnrSjGlMJvnbBpB4rcd51xkkzLOMGmRJRETBTRvxogVQ3KWNeGFdOd9g4IWIiEJe6dJAs2Yy/+KLMvJj7175p9JViqZQ4qzOi5ZmrHPn4EinZjLpTzf2wgsyYqJQIen7iY52vN6gQTL9/HMJMhklp1EdgASQbr5ZAntZObrBfuZMeezttyWbi7f27ZPgISBpzw4fdn5Dv5F4p6wP5TQywyilSklxIi1xc1ZakODAAQmi/PuvBFC0/9r0BInCw4HWrXGjYv3q1a6jcV4OlTp3TpoJOAm86BEVhWqfDUaxmAxcREFswl2O26mUFBF76imgRw+gTRtbzkEOByOdjh2zjdK6/373tm3TBqhUScoTaXFQtzBImKdlHmSZla9GyxKR7zBjJDljVOCFI158g4EXIiIKeSkpMvIjK63fLDfc0PnwwzLduNH2R9GpU8CPP8p8MKQZ0+gJvHz7ra1o8PTpMlLGmYcfBipWBM6elRvxjWLEqI6sN9h36AA8/rice926eZeb32KRfZw/DzRsCDz3XM439BuFd8r6mL/eyHXrXN8qr5T8t1a+vAzvysrdIJGPh0ppo13i46XOkqfCwoDGNc8AAFYg0fmK588DkyfLsLOUFAlk7dmj7yAcDkaQ+nMAULs2cNNN7m1rNssNCoB8X+pJ32mHOSPzvKQk+Xsr63d5qVK+GS1LRL7BjJHkirepxhh48S0GXoiIKKTpSReVG27oLF1aMgcBwHffyXT2bKmNUrs2UKVKwJqWjRZ42bDBcZ/vX38B//ufzD/3HPDYY673FxZmG/UxYYLr99odvhrVMW6cFFDW6sR42t533gHWrpUBC19+6d+7Ul2VIwHkOfFO2RDgi+iiqzfdx0OltMBLrVoebW6ncfn/AAAr0dj1iklJ8mH86COJEletqu8AHA5GsNV3cSfNWGZdusioy2PHgClT3NyYOSMJMmpKKUlbe9ttsuzVVxl0IQoVzBhJOeGIl+DGwAsREYW0vHRDZ9Z0Y8nJMg2m0S6AdBLdcosEu7K+7ufPyz/7Fy/K3Vpvvqlvn716AQULStBG68jyVkKC6zuDPB3VUbCgvDfh4ZKV6Isv3G/bb7/ZymZMnCh3tfmbs0xTgCzTzkcKYv7OGefjoVJayiaP04xl0riRRETXoCEy4CKY9PTTEiHu1w/o3l1OfFfPEZDchHc5SGFGeYpSntV3ySxfPuD552X+3XeB9HQ3NmbOSALw668yrV9fynoBclMHEQU/ZowkPbwNvGjbMfDiGwy8EBFRSMtLN3S2aCHTH36QtCNaUKNjx8C1yRlH6caUkpEuO3dKx/3MmRKc0KNwYQm+AM5zlrvLZJL9OnsM8HxUR506tqDSwIHArl36t710CejaVUYztWsn6cYCJWumqUWLJFh16JAtVRwFMX/njHM1VMqAogJGjnip2eMOFDWdxQUUwu+4M/sKzl6bnIaDAcC1a8Dff3vfSAppu3bJjSEREZIu0lO9ekls5MABN9NtNmgAFCjg/HHmjMwT1q+Xad26trc6N9yMRJQX5KUbDMlzRqUa0wI4ZCwGXoiIKKTlpRs69+6VtFtXrwLPPCPLIiIkpVew0QIvCxbYikC+/76MoMiXT9KkuXtXjpZu7LvvgN27vW/jV18B//wj6Teynh+Z64d76tln5XW4dElGJem9U3nYMOmwK1NGyku4urHeHzJnmnr0UeC992T5668D//0X0KZRTnwcCHHI2VApLz9UViuwdavMGzHiJSyfGQl1LgEAVmat85LTa+PsOcbHywfj66854oVujM5s2FAGQXkqMlK+TwBg9GipG5NjcWWlZKTWpUuud86ckbmeNuKlXj3g3nvl78i9e+UGCiIKbnnpBkPynFGpxo4dk7+3yVgMvBARUUjLK0XAU1Jk9EPWP4auXg3O/L5aEeB//rEVgRw2TJZ98IGkvHDXbbcBDz8s8xMnete+8+dtRYtHjZK7xfTWD9crLExqchcrBmzaBAwfnvM2338PfPyxzE+fLtsGm65dgUaNpD9v8OBAt4Zy5KNASI7HzDxUyoAP1Z49cs5FRgK33mpMMxt3LAMAWBnZzP4BPa+Ns+f4yitA+/a29f74A5g1y5gGU0jxNs1YZk8+KWks9+wBmjbVUVz5jTckqALIXQtxcdnXmTSJhT5yufPnge3bZb5uXRnlW7u2/M475ImCX166wZA8Y7UCJ07IvKeBF22kTEYGcPq0Me0iGwZeiIgopAXihm5/c5XfVxNM+X1TUoAnnnD+uDf/HAwaJNOpU4G0NM/3M3q03B1WsaK8du7UD3dH2bLA55/L/JgxcqeyM8eP29KpDR4snWvByGSSOuNmMzBvnqS+oyDng0BIjgz+UGlpxqpX15+iMCeJiTJdnf9+WH724LXJ6TmePAm0bCmPDx/O2wjzkGvX5FQCjAm8LFkCXLiQfbnD4spKyV0PADBhgvxk/vzfcYc8tnOn9w2joLZxo5wO5crZ7mjWbkRatSpw7SIiffLKDYbkudOnbX0AN93k2T7y5weKF5d51nkxHgMvREQU8gJxQ7c/hVJ+35yCRCYTMGSI50GiBx4AqlSRuzinTfNsH3v2SNozQEbfRER4th+9HnsM6NNHXpPu3YFTp7KvoxTQu7cEX6pVk8BQMKte3Tba5emngStXAtoc0sNX0UU/2bJFpkakGdPUqgXExABpaSZsLpJo/GtTtKgMTQBkBEK7dsDFi3IBXLFCR74oB7zZlvxmwwb5nipe3DbCwFPa96ojDosrm0xyd8KPP9pydGb+/L/9tiz77DPHX0iUa2ROM6Zp1EimwfA3IxG5lvkGw6xyyw2GmeWFP3GMfo5amrFixSSdt6e00TIMvBiPgRciIsoVAnFDt7+EUn5fXweJTCZbP9KHH3p2A/mzz0q9lQceAFq08Kwd7vrgA6BSJeDwYRkNlDUwNWUKsHCh3HH09deSTinYjRwpdWj27AHefTfQraHcThvxUquWcfs0m21Fz1euNG6/dgd45x3pBM+fX4YlVK8ut6c2aaIjX1QWKSmyrifbkl9p9V3uv1/STnpD9/fqlL9tPThhYUDz5o43eOABGfVy8aIMX6RcSwu81K1rW6Zd87ZtY0oZolCQlGT73yez3HKDoSYv/Inji+fobX0XTeY6L2QsBl6IiCjXCPEbup0Kpfy+/ggSde8ud6n/84/7aa5+/hmYP1/OjbFj/Ve4Pjpa7mzKl0+O/8kntrudvvoKGDhQ1nvrLWM7ln2pUCF5DQEZobN3b2DbQ7mbFngxcsQLYEs3tmKFsfu107MnsHy5FFjYvz/7BdBhvqgsUlJknaw98Hq2Jb8zsr6L7u/V/m/IH0Dp6a5XNJmA556T+Q8/lOJJlOsoBaxfL/OZAy8lS8qNIACwZo3/20VE7tNGIbRrZ/vfZfXq3BV0ye1/4vjqOR4/LlOtTountMALR7wYj4EXIiKiIBdK+X39ESSKjrbVkHE2/N6RjAxbeqx+/SSllz/dcYcthdhTT9nudurWDbh8WW6EHzLEv23yVrt2UovmyhUJHrmqQ0TkqbQ0GcEIGB94adxYpsuXy2gzn6W2qFdPLl6OaB+cQYMcH9xVDkeHuaYokNLSbB3eRtTq0v29mvGffNHpGWLTti1QoYLUIZozx7sGUlD67z+5czk8HLjzTvvHmG6MKHSkp9tuNHvmGaBGDZnfvDlgTTJUXvgTx5fP0egRLwy8GI+BFyIioiCXOb9v1uBLsOX39VeQaMAA6VtauhTYsUPfNp98AmzfLjlwR43y7vieuvlm549t3y6jYUKJyQRMnCgjeb77TtKl5SV5IRd1MNi6VaZxcfL5NdL+/XIeX7wIdO3qw9QWq1e7HrqglNwGGREhI2NiY+WCUbkysGxZ6BT6ohvXgltvlXPJWzl+r8KKePyHhPvyS67V8PCcdxoeLn84fP+9RP8p19HSjNWqBURF2T+m/Q22apV/20RE7lu5UmqGxcYCd98N3HWXLN+0KbDtMkoo1TL1lC+fIwMvwY+BFyIiohCQlCQ3pZYta7882PL7+itIVL480KqVzH/4Yc7rnzoFDB8u86+/bnznrR4WS84jWkLxjq5KlaRuDiB3c+WVrDXe5GlmwMY9W7bI1OjRLikpQIcO2e9A9ElqC735oiwW6WE5flxuWd+1Czhxwthj+EMePsm1+i5GpBkDMn2vKgUTshY2k9/H3fYxzAtS3CsQ1rIl8NBD/su5SX7lKM2YRhvx8vvvEnQmouC1aJFMW7SQm85yW+AllGqZesqXz9GoVGNa4IaBF+Mx8EJERBQikpLk7uzUVLmpNTVV0u8ES9BF468gkVYXZcYM4MwZ1+uOHCnr1KgB9OljzPHdlZvv6Hr5ZaBcOeDff6VOTW7nTZ7mvFA81GhafRcj6x/5PbWF3nxR334L7N4tT3rDBrklPevF1Ntj+FpKCiw334IVTUZhZueFWNFkFCw335JnTnIj67tokpCCOWiLsjhktzw/0jEHbZH0chWgYEHPD3DhAnNF5jLaiJd69bI/dvPNMvo4I8MWoCGi4KOUbTR5ixYyzRx4yQ2X7VCqZeopXz5Ho0e8aPsj4zDwQkREFELMZikG3amTTIMhvZgj/ggSNW4sd8BfugRMmeJ8va1bgUmTZH7cOH1ZWHwhN9/RFR1tG+k0Zgzw99+BbY8vedNhnxeKh/qCFngxcsSL3wOhevMwJiVJjqoaNSSnSEJCaBX6SklBSpuvUf7QGjTBCnTGTDTBCpQ/tAYpbb7O9Sf5wYPAzp1yV3KTJgbt9PpFJwkp2I/ySEUiPsJTCEMG0hGJSvhbhnR6GiV8912JnP/4o0ENpkBLT5fRLIDjES8A040RhYJt2+SmpshIW82wWrXk/79jx4DDhwPbPiNof+I4E0x/4njKl3/GMdVY8GPghYiIiHzC10Eik8k26mXiRLlzMyulpBPcapX+zPvuM7YN7sjtd3S1agU8/LB0+AwYkDvuwnNEb4d9o0Zy7j/xhARqXngB6NUrdxcP9QWr1TcjXvweCPUmD6OrbQE5gZ57LvCReIsFKX1+QFvMxkHYj9I5hLJoi9lI6fNjrj7JtTRjd98NFCli0E4zXXTMsCIRK9EPk9EaCwAAn6K3d1HCY8dkSOg77xjUYAq0P/8ErlwBihYFbrvN8TpaurFQHGVLlFdoo12aNgUKFJD5qCigalWZzw3pxjL/ieNMsNQy9ZSr5+htKm6jUo1pgZeTJ4Fr17zbF9lj4IWIiIhCVufOQPHicjeYlgM5swULgOXLpV71mDH+b19moXTTuidMJmDCBHmtly6VjEm5scSD3o74X34BZs2S0VgTJkifZlqa8/VDOdWcL+3bJzUIIiKcdyB6IiCBUG/yMDrbNl8+mQbBUDnLitUYdGoEJI5o/2+muv774FOvwLIiuE5yI8vR+CLNmLP3tg8+BQDMQHdcRqTn58CQIXIerVxpy08VQHm4PJBhtLexbl3nf3Nof2usWyc3TBBR8NECLy1b2i+vU0emv/3m3/b4SkKC44wE0dHBVcvUG0lJQI8e2ZeXLev5c1TKuBEvxYtL4Ecp/aUFSR8GXoiIiChkRUUBTz4p81nvJLpyBRg6VOafeQa45Rb/ti0rb254DxUVK8rIDsBWvyS31THR2xE/dCgwdizwxhvAiy8CzZvr2y4I+s+DypYtMq1Wzdg0gQELhHqTh9HRtv/+C4weLSdagK1eYcFBxMPZv5gKYTiAcli9Inh60o2suWS12ka8aClhDOHkovMAluJm7MdZFMUctPU8ShgXB3TtKvMBHvXCGljGyBx4caZKFaBECflbKTfcNU+U2xw9KqXeAODRR+0fy1znJTeYMkUyF9x1l/xpM3y4LA8Lk9H0uYWWxqtvX6BwYZn/+mvPA0vnz8s1HPB+xIvZDNx0k307yRgMvBAREVFIe+op+WNx5UpbJy0gnd779gFlykjHdzDw5ob3UFGpkkytVvvluaWOSdmyroNjWof9u+9K6rCXXwbeessWkMpJqKaa8xVfpBkD9GXv8lkg1Js8jFm3LV1aTi7tSWRkSNqoADiSVkDfeqk7pZh7gBldc2nrVkn5ER0N1K9vXDuRkCA95FmEQaE3PgMAfJr/ae+ihMOGyXT+fClSEwCsgWWc9etlWq+e83VMJqBhQ5nnSEui4LN4sUzvvjv734aZAy+hntrXYgEmT5b5AQPkT5tRo+Rv6fPnge++C2TrjHPlivyvCgD9+9tGxq5b5/k+tTRj0dHy4y0t3Zg2ioaMwcALERERhbS4OOmUASTAMnOmBDG0G8DfeQcoWDBw7cvKmxveg53FIqUmHMkNdUx27JB/CLX2uzNyKacRFoAEdUI11VxWRqUL0gIvNWsa1TIbZ4FQQDIv1a5t/DF96upVoH174P77gbNn/XvsxYtR+ou3dK1aeu1sidB+9VX2CK2fXK9Xb2jNJW20S+PGQP78XjfR5p9/JN+eA70wDWZkYE36Pdixy4soYZUqUqhLqYDk5fTF+5FXnToF7N4t8/fc43pd7ftm1SrftokCi+n7QpOWQjlrmjFAbkYxm6WD/PBh/7bLaN99J4N3ixUDOnSQZWFhcn8JICNCcoO1a4HLlyWIVq2aLfC9Zo3n+zQqzZhGC7xwxIuxGHghIiKikKd1yv7wg6QnadcOuHQJuP12+T3YeHPDezDTW3g+FO+u3bRJihEfPiz/MH3+uXsjl3IaYQF4nyYgWBiZLkgbxWb0iBdN1kDo8uXymbx2Te68DKk7SQ8flv/s//gDeOghuVXU19LTJZdjixZIuPA9SpuPAXD8oplgRXyhs0iocEja2q2bDA3Lyg+9hL64Vmn1XQxNM3bunARELl8GKlfOdtEpE29Gy7rS8/Lpp14e6/nnZZqc7PfAXW7+7vA3LTXRbbdJR6YrjRrJdO3agMVAyceYvi80Xbpk+05p0SL741FRQNWqMh/q6cY++kimjz8uz0vTpYtMv/vO//eS+MJPP8m0WTP7EYfeXH8ZeAkNDLwQERFRSEtJAV55xfFjf/8tmVPIP/TWJwm1OiarVwP33Sd3EtepI6kCHn/c/ZFLzkZYxMbKHfJ//BE8afE8ZWS6oPPngb17Zb5GDePamFXmQGiTJpLyIn9+CeSGVOdUhQrSU1O0qOQaatlSOux9xWqVKMMHHwAA1MDBKHrbTQBMcBx8MWHctCIw79gqdWni4+WDlJmfegmNvlZduWIbNaClD/Ga1Sq1V3btkqjuihVyW3CWi06fUXJBmTHDy7e7fn0ZIrplC1CkiBHPQLfc+t0RCFp9F1dpxjS1a8uI4LNngW3bfNkqCgSm7wtdy5bJ9bxcOecjfnNDnZe//5aAhMkkqaMzq1kTqF5d7u+YOzcw7TNS5sALINff6GjJDvvXX57tU0s1ZtSNWwy8+AYDL0RERBSyXKUnAeQPeaYn8R+99UkWLJB/thwJtpQYS5YAzZsDaWlyd/CyZUDx4vKYJyOXHKWaO3QI+PJLeXzMGNt8qDE6XdDWrTItU8ZhiQufqVTJdvP/wIH+GThimJo15aQtVEg+QG3aSAoyTz9YrrYLC5O8IEWKAPPnY0T0+9ixMwyRkUApB3df1qtvksBkZKTUpfnnH1slVwBo0EDa6+NeQqUkbaAeeq9p69bZpxAxxIQJkuQ/MhKYN08itA4uOg88ANx8s3TeeN059dxzMlTUz/S+zqyBlTMt8FK3bs7rhofb6hEx3VjuwvR9oS1zmjFno6RzQ+Bl0iSZPvyw3DuSlZa1IDnZf23yhWPHgM2bZV4bFRsebguQe5puzOgRL9p+GHgxFgMvREREFLKYniS46KljAgDffCOd24mJkrv5yhVZHmwpMebOlRQPly9L5qYffgAKF/Z+v44CNu3b20Zu9e5t6zwLJUZ/HrX6Lr5KM+bKiy8CFStKRqwRI/x/fK/cfbfk5oiKkpO2cWPPPliOPpA33wx8/LFtnX79gJ078X2+Vhg9WhZNnw4cPGS6EVz84gtZvn59loBr5kIoy5cDv/ziuB0G9hLu3i13m2o1wJylRQOAiHwWVKqkb7+Z04zldP3T7X//k163zz6ToXZOmM3AE0/IvNfpxjJzUlfGF7TvDmdMJhkglVtqYPmKUu4FXgBbujH+nZS78O/j0GW12gIvjtKMaUI98HLxIjB1qsz37+94Ha3Oi3aTUqjSasDdcYf96BRv67z4KtWYtl8yBgMvREREFLKYniS4uKpjYjLJz/PPA488IjfLr1wpmXTKlgUefdT7lBhGjpaZPl2CIdeuSc2g+fOBAgU8358er74q5RyuXgUeeyz0/sk0+vOo1XdxlmbDl6KibHnHJ0yQNHAhJSFBhpaFh0tPrLsfLFc5avr3tw3LMpnw39VYdOsmvw4YIJ+bzMHFXr2k80gpxyVddPGyl/DqVfl81aghHSCRkQqdoubBBAUTsiZXtwJQuHrNjLvvVli3Luf9+6S+S+HCcuHp2jXHVf/3P3nNV6/2PGXJDadOAR07ysgXX6aqy8Rsdh3gVAoYNy731EPzld27ZeRTZKT+66YWzFq1KsRqWpFL/Ps4dG3aJCMOChWS+yacqVVL/pY+elRuEgk1yclSxqxiRRlZ7kj58jIYVilg1iy/Ns9QS5bIVEszpvE28MJUY6GBgRciIiIKWUxPEnyc1THRCs+//bZkz9m/Hxg1Su5iPn1abtD3JiWGN6NlsgZsJkwAevaUuw7/9z9ZnvnmfF8JC5P+7OrVpTPkscf81u9pCKM/j4Ec8QJIR0CHDnIePPlkCKZkue8+ICbG8WOuPlg55XAEgGHDAIsF6enyGp0+LYMy3nvP8eovvSTTGTOc3IWt9/ZKF72EzgKvy5ZJJ/SoURKAadYM2Pb5r0i+nIQ5aIuysI9wxuMgxmEQKuMvHDpkQqNGwIcfOn85Tp2y3XHsdeDl1Cngk09sB9M5fKZMGdud0V6PeilcWHKnHT4MTJvm5c700wLNjq61UVH6R3C4LdjyW3pBG+1y5536v7PuuUfWPXoU2LPHd20j/9JSouaEfx8Hn4ULZfrgg0BEhPP1ChSwpbb87Tfft8tIStlubunXT/7+daZLF5mGaroxpbLXd9HUrSs3FOzf73qEmjO+GvHCwIvBFGVz7tw5BUCdO3cu0E0hIiIiFzIylIqLU8pkUkr+tLX/MZmUio+X9ci/MjKUSk1VKjlZps7eg4wMpd5+2/H7l/Vn8mSl0tKy72PuXMfngMkkP3PnOm/n3LlyDjk63uDBSlksRrwa7tm7V6nixaUNXbooZbX6vw2eyMhQKjbW9XtoMin13Xc578tiUapQIdlm2zbft92ZQ4eUKlxY2vHxx4Frh0dSU/V9sKpUUapePaV693Zvu9RUNWSIzBYpIuetK4mJts+Vx21NTXW4b0ef49KllUpIsP/9m2+UslqscmG6/kAGwlQqGqtkdFSpaKwyEKYUoNJQULWvt//G9p06KXX+fPZjz54tj1erpv+tcejaNaXuu0929sILbm/+/feyadGiSl2+7GVbxo+Xnd1yi7TLx65dU6pMGTnk11/bvjt+/lmpe+6R5a1a+eBa6OjEiYtz/aURxPr1k6cwZIh72zVoINt98YVxbdH7NwAZb/dupWrXzvm7mH8fB6eaNeU9mjEj53V79pR1R4zwfbuMtGaNtDsqSqnTp12ve+KEUuHhsv5ff/mnfUbassX2XK9cyf74XXfJ47Nmub/v226TbVeu9L6dSil15oztGnHpkjH7zK3ciRtwxAsRERGFrJxSWwFMTxIoegvPm81AuXL69tm3r9yMXbasjGjp2xd4/30ZjeDJaBln2ZQ0DRu6vgvPVypUkNFB4eFSA2fMGP+3wRPp6c7vstY+j0pJ2YrMZUIc2b9fitrnzw/ddTZ8oUwZ4M03Zf7FF0PsLkC9OWT++ksKsGzb5tZ28xaaMXaszE+f7rgwbmYvvijTTz8FTp7M8mBOBaK0Ih/33CPrfvKJDF9B5s+x/UXgyBF1IzPZ008Dfy36B+3XDYHpoQftbvM2w4pErEQnzEIiVsJ8PfVYIVzArLf2YexY+SzOnCl3p+7cKdtpAyW04sD33ef6+edo2DCpdRMdbbvF1w3Nmsm19MwZqU/llccfl1vm9+4FXnvN56NBFi2SATY33STvpfbdcf/9wJQp8vovWGDA88rMVTo9vfktg4w24kUr2KxX5nRjRgi2em15yZw5MuJp82ZJVQU4v6zy72Pv+GKw3L//ymjfsDApOJ+TUK3zMnGiTDt3BooWdb1uiRK2VGShOOpFG+2SmOh4BJM36caMTjUWE2P7O551Xgzkh0BQyOGIFyIiotDi6KbV+PiQvWk1z9F7s3uRIvrWc/TTsKFSHToo1bWrUv/7n1J9+ihVsGBw3w368ce2tixaFLh26DVggLQ3JkZGF2T9PM6apVSPHrZlAwc6f33nzZN17rjDj0/AiYwM2x2JnTsHujVu0PvBeuMNpRYsUGrVKt3b/YNbVEz0NQUo9eyz+ppjtSp1552yi+HDHaygDV3LOnwt89C1zz+3LS9TRmW8P07FlbUowOqkqVYVW+Syymja3P6BDRtcD5fUTtrrJ+jq1bZzumBBec5Zv3NKlPDiO2faNNuOvPjieu012UWjRh7vwqZ9++yviY9GgzRr5nqgz/Dh8nhsbM53R+uiDZcN5i8AN126ZLsrfP9+97bVRktVrOh9O7wZgUqeu3LF9h2s/c1z4IDjv4/DwpT69ttAtzi0+Wqw3Icfyr4SEvSt/8svsn6pUt4d15+OHFEqXz5p9++/69tGG6R6yy2hMwpc88AD0vaxYx0/ro2arV3bvf1evmw79wz5XryuXDnZ5/r1xu0zN3InbsDAiwMMvBAREYUeprUIXe6kjDt9Wv4ZmD5dqZdftqWh8dWPk8xGftO3r7SjUCGltm8P3vN80SLba/bDD87babUq9dZbtnUffthx+rhRo+TxHj38+CRc+O036awClFq6NNCt0cnTXIw5bHcZkerOfH8qQKl771UqPV1/k+bMkd0UKeL4fc8xin7xoqTBur5OKhrr+xyjsTyfFi3kBLVYnAd6tJ+PPrJr2pEjtnRpzl5OXR3LWT8cv/yiVESE7MTLfDEHDyplNsuuduzwYkd+7D3fvdu2a2fp6q5cUapyZVnv8ccNOKiXqe2CkZa6JzbW/Y7Js2dtb/ehQ563IRTjWRlXM1Tq2D9U8oC1KnXsHyrjahA1Tqe9e5WqU8f2Oj//vP11WbvkzJhhS52pJ+UnOebLy6MWhB4zRt/6Fy/a/jbx5rPrT9oNAvXr69/mwgWloqNDLyBw6ZLt6337dsfrHD5sC4iePat/3//+K9vly2dsMEr7v2r+fOP2mRsx8OIlBl6IiIiI/EvPze6O6O0/GzJE+mvfe086/tu00bddcrJfX4Zsrl5VqnFjW4eaVgdB+wmGcgSHD8vd/trrrMe33yoVGSnb1Kgh/0BmlpQkj33wgfHt9dTTT0ubbrvNgBoa/uLpB8vFdk/hYwVIHaIDB9xrjsWiVKVKOXQs6YkuXrmi1KefquSbBur7HD/6tVL79jl+nll7imNiJPLnwJUrttpDHnUsO7v9HJAiJgYUlWrVyr3PYjZ+7j0fNkx2+9BDrtfTAguAUsuWeXnQTDV+gvoLwA3vv287jTxxxx2yvSd1BjShFs+aO2ydijMfsv9ONR9Sc4etC3TTHHJ0aZw/3zYauFgxpRYvdr2PgQNl3Q4d/NHi3MeXl8dz52wjQXbt0r9d9eqyzcKF7h/T365dU6psWWnvV1+5t22XLrLd00/7pm2+sGSJtLlsWdfBkYoVZb0ff9S/740bbfs2UsuWst/Jk43db27DGi9EREREFFKSkiQ3edmy9svj4mR5UpLj7fSWhhgzBhg4EHjmGak1MWCAvnZlKgUREPnzA7NnS+2DY8ekDkJmessR+CIXOQBYrUDPnlKzo1YtYPRofdu1awesXAnExgJbt0rpjg0bbO1cu1bWq1bNmHYa4fXX5XzYvRt4551At0YnTz9YTrZLLtofk/AUTCapPxQX515zwsKA55+X+Q8+AK5ccbCSngJRERFA794o/Xx3XcctfX9VKTKRVVKSFBRKTZXk8ampwKlTwMiRDvezbp3UHnJGKeDAAdyoLWPHWU0Rq9SUQdu2hhSV6tNHptOnO3l9c7J6tfPCV0AOT9I9V64AX3wh80895XrdBg2Afv1kvk8f4NIlLw6s98Ie6C8AN2j1XerW9Wx7rc6LN2+r3rJSetfzpZTn1qPtmHtw0FLKbvkhSym0HXMPUp5bH6CWOeaobk6RIkDr1sDZs1LX548/gEcecb2f7tcvmfPny3Z5nTt/GykFfPml7y6PS5YA164Bt98uP3qFUp2XBQvk71atnpc7OneW6TffABkZxrfNF7T6Ls2aOf8/BfCszotWgyU21rO2OVPq+iUxpGoaBrmgCLx89NFHKF++PCIjI1G3bl1s2LBB13azZs2CyWRC69at7ZYrpTBixAiULl0aUVFRaNq0KXbv3u2DlhMRERGRURz1ge7b57xvGJA+2fHjZT7rPzXa744KyOoN2GidUYFUrJjz/lilZDp4sPMOA18WOh43Tv6xjIqS98xR4VBntGBLjRryD2TDhvIPZJMmtn8oe/UKnoLMMTHyfAHgrbckABMSPPlgXd/Osmc/Voz9AzMHrMWMF3ag95UJAIBXXrEVu3VXly7y2TpyRIID3mgYuxsFcNHp4yZYEY//kHDTTuc7cRXo+esvoFs34OpVAF50LFsswKBBtg9stoaagJdeMiQi2ry5vL6nT3v42fFj7/ncuRLnio/XV0h69GiJBe7ZA7z6qhcH1r4AnAmmLwCd1l+PE9Sr59n22lNdtcrzNoRKPMuSbsGgD8pBPo32X67q+u+DP4iHJd2gOxS85Cxme+GCTFu0kBsZypXLeV933glUrSqXtNmzjW9rKNHzt9GJExKU+d//5JLQq5e+fXtyeVy0SKYtW7q3XSgFXj76SKa9e7v39yIAPPAAUKKEFJRftsz4tvlC5sCLK8EYeNH2Twbw/QAc12bNmqXy58+vvvjiC7V9+3bVu3dvVaRIEXXs2DGX2+3bt0+VLVtWJSQkqFZZxtO+/fbbKiYmRs2fP19t2bJFtWzZUlWoUEFd1pkTgKnGiIiIiEJLTqUhnG3jSRYmf9ObvmX8eKX++88+xYUvc5H//rstLYY3KQnS0mxF133RTiNZrUo1v16nvWlTpZYvd7/eTrDW6cnK0WcKkLQm3rZ5/HjZ1y23SOoRT308eNf1dlkVYLE/d2BRJljUXDzmWW6j9HSlypeXnfXsqZTV6nkqJT/nYHr1Vdld48YebOzHtjZoILt6/XX92yxcKNuYzUpt2uTFwbWKxsF+0dFBqxFgMjmpnaTD0aO2fZw65dk+MjIk3ZUv0jAZKXXsH/pO8bF/BLahKufUVtrfOu68pm+/Lds1bOi7dgc7V38bAUo99pjjv0u0v3mMvjxeu2b77Kxc6d62a9fKdqVKubedv23fLu0MC8ueXlavfv1kH926Gds2X8h8XT5xwvW6f/0l60ZFSYphPUaPlm2MroX40Ue2zwA5F1I1Xu655x7Vv3//G79bLBZVpkwZNXr0aKfbZGRkqHvvvVd9/vnnqkePHnaBF6vVqkqVKqXGZEoafPbsWRUREaFmzpypq00MvBARERGFHk86tD0J2Pib3nIE2k94uFIVKkina4ECvukEu3DBVuy6VSvvCnuGWkHmf/6R1zhrO/XU23F0vgVDnZ6snHVKae+Ht+29eNFWF8jTMhpr1yqVL59VOmEwXcXhP/vPMf5Vc5Hk3cnz00+2GiwffHDjXHX12jg8nJ9rihw8aGv2X3+5uXFOT9KTnl4H/vzTdr06fNi9bTt0kG3vuMOLwN3MmfY9rdpPiRJSTCCEzJsnTa9e3bv93H677MfTWhEXL0rtJ1en+OzZ3rXRCMkD1ur7OA5YG+im+iQOevCg7bT/5x9ftTx46QlmZf6pWVOpZ5+Vr4Pz5z38DsjBqlWyfbFi7l/TLl60Xe8PHXJvW3/q31/a2Lq15/vQgkwFC8rzDmbTp0tb69TJeV2r1XbtXL9e3/4HD5b1n3vOu3ZmNXeu7Ld+fWP3m9uETI2X9PR0bNq0CU2bNr2xLCwsDE2bNsW6deucbvfaa6+hZMmSePzxx7M9tm/fPhw9etRunzExMahbt67TfV69ehVpaWl2P0REREQUWvSUhsjK0yxM/uRO+pZ8+ST39b59knrEVR0EpTzPRT50KLBzJ1CmDPD5565zV+fEjyUlDLFli+P84jnV23GWLkZvnR5/ySkrFuA6tZ0eBQrIMQBJH+XqWI4cOSKv2bVrJrSrfxDT0RP7UQGpSEQyOiEVidiHW5Bkmuc416BeDzwgxWgA4NlnYf55iUepDVGggL7jGZSDqWxZ4NFHZf6zz9zc2FX+Rs2bb3r+ml43ebJMW7d2/2mPHw8ULSo1LbS3x22lSkmew5EjbV8AkydLwapZs4Bduzzcsf9p9V08TTOm8bbOy/vvS+q4EiWyl5XSTqXt2z1vn1FKV9T3edS7ni/5IvNf2bKA1l321VfutynU5fQ3h+all+R13bJF6gQ+8ABQsKDry6NSnn3lLFwo00ceAcLD3du2QAFJHwcEb7qx8+eBGTNkvn9/z/dTv76kg7twwZaaLVjpTTMGyLnkbrox1ngJHQENvJw8eRIWiwWxWc6U2NhYHHXyLq9ZswZTpkzBZ07+gtS2c2efo0ePRkxMzI2f+Ph4d58KEREREYUoTwI2/qS3Hs2BA8DlyzJds8ZWiDon7uYiT0kBPv1UjjtjhnSyeSOUCjJrQQlHtODB008DZ87YB2dcBTO0Zd4GM4zir0BY//5AoULA1q3Ad9/p3y49HWjXTs6HatWAL36Kg2nuHJjjSiMRK9EJs5CIlTDHlwHmzPE+ijpwIPD444DVCnTogKTqf2POnOwdy3FxDg6nlFQC7t3b9TF8UFOkTx+ZTp8uRezdkpQEh09S6xH0MsH+hQtSpBoA+vZ1f/vYWFvAZeRI4J9/PGhEYqJEbl54wfYF8OSTUjDDYgFefNGDnQaGFnipW9e7/TRqJFNPPtuHDgFvvy3zEycC//5rf0PDF1/IY6++Cixf7l07vZXQrwbizIcBOI74mmBFvPkQEvrV8G/DHPBV3Zzu3WU6Y4b7ge9Qp/dvierVbZ3QmTm7PGoKFnS/TVrgpUUL97cFgr/Oy5dfSvClUiXg/vs934/JJPV4ALm2BCurFVi6VOb1BF4A9wMvx4/LtGRJ99qWk8yBl7x2bfCVgAZe3HX+/Hl069YNn332GUp4+x9eJi+++CLOnTt34+fAgQOG7ZuIiIiIyBuubkDPepe92SwdwA0aSOe0HocO6W/LwYPAE0/I/HPPefcPtCZUCjID+oIShw8DxYrJ6KOICLkzv1Sp0BnV469AWNGiwFNPyfxbb+n/B/+ZZ4C1a4HChYF58653cvly6JrJJBWBGzQAzp0DWrZE0n1ncz5cRoYM5+jYUSo0a8Xc3Roq47kHH5RDnjolr5PbHL2my5dLe6dP9+p24+Rk6YS7/Xbgvvs820ePHnLX/pUrEmTyqIPIbM5e4fntt4GwMHnR3Kl0HCAWC7Bxo8x7G3jR4n6//QZcvOjeti+9JCMs770XaN8++w0NPXtKgXKlpOM0kHdTm/Ob8XxP55WjFUwYN/QAzPkDfxeGduOFM57GbB97TK6de/fK9TQvMeJvDkeXRy3Y3b27e4XJd+0C/v5b/mZo3lz/dpkFc+BFKfkKBeSGIG9GSANAly4y/eEH4PRp7/blK3/+KYGR6GgZpaNH5sCLnu8zX4140fZ3+bLcJEHeC2jgpUSJEjCbzTiW5ap07NgxlHIQWt6zZw/279+PFi1aIDw8HOHh4ZgxYwYWLlyI8PBw7Nmz58Z2evcJABEREShcuLDdDxERERFRsHB2h6XDu+yvy2mkjGbYMOnA/O237I9ZLMCKFcDMmXKTe7duMprjrruA117z+Om41U4fDAbwmLvBhvR04OxZyV7ki/37gj8DYUOGSL/3unXAqlU5rz9jhtxND0iKnNtuy/SgL4euRUQAc+fKiRgbC2RkwAwLErECnTATiVgBM7IMVwoPl4hbvnzAqFEyLGPuXPc+xF4wm21B0k8/9WInmV/ThASJfAHSy3jqlNu7VMqWZuzJJz3vhDOZgE8+AaKipNPz889t16oVK1yMHnvnHQmuXL7s+PGqVWWEEyAXxyC/5XfHDukcK1jQlm7IU+XLy+mYkWEbRaPHxo22NELjxjl/Tz/8UEYRHDsmnaeBHOG3Ku0OACZEIut5oACYEH2/l3nbDGI2A6+84vgxb2K20dGSrhGwvXd5hVF/c2S9PI4bZzu/u3eXUQ96aDHsxES5ocATwRx4WblSrlPR0RIw91bVqkCtWsC1a8Ds2d7vzxe0NGNNmgD58+vb5s47gchI+Xvx779zXt9XgZfoaNuoLaYbM4gfas64dM8996gBAwbc+N1isaiyZcuq0aNHZ1v38uXLauvWrXY/rVq1Uvfdd5/aunWrunr1qrJarapUqVLqvffeu7HduXPnVEREhJo5c6auNrlTJIeIiIiIyF8yMqSIbnKyTHMq4KoVSc9aCFZb9uijSuXPb1verp1Sf/9t29ZRAdqICKV27TL2eeXUzmApPq+30PGSJUqdPKnUf/9JcfNPPjG+QLKvLFzouo2eFg925qmnZL/Nm7te7/fflYqMlHVHjDDm2G7bu1epq1cdfzji4pSaMEGp/ftt6585o9S2bfb7cPdD7IUDB2xFl6dPN+iQly8rVaWK7LR7d7c3X7/edh05dcqLdlz33nu28zLr25HtunHwoFJRUTlXkD98WKkCBXxWDd7IU+Czz6SZTZoY07ZOnWR/I0fqW99qVeree/WfDn/9pVR0tHvHMNqvv2rnjFX9Hn63SkVjldxlsUq95zk1CB8oQE7x9PTAtC8rrSh5RIT9OR4f79134/Llsp+YGKUuXTKsuSFBKyDu6PvNm785tm2zXWLeeUffNo0ayfoffujZMZWSQvPatf7wYc/34wtt20q7nnzSuH2++67ss1Ej4/ZppPvvl/ZNmODedo0by3aff+56vYwM23fekSMeN9OpW2+Vfa9aZfy+cwt34gYBD7zMmjVLRUREqGnTpqkdO3aoPn36qCJFiqijR48qpZTq1q2beuGFF5xu36NHD9WqVSu7ZW+//bYqUqSIWrBggfrzzz9Vq1atVIUKFdTly5d1tYmBFyIiIiLKLRz1EWfusNm3T6lu3Wz/xJnNSjVrlr0jM/OPLwIhObUzGGRkSBudvTbOghI5bQdIp8n33wfmeWm++84+EOePQNjevXLOAUpt2uR4nZMnlSpfXtZ5+GGlLBbjju82LUro7I2sWVN6o4NEnTrZm+gwKOGODRuUql9fqe3b3d60Z0+PYzYOffutGx2oTzwhD957b87v0YgRsm7Fiob2wDuL2Xn6fjz+uOzDRZeJWz7+WPZ333361p81S9YvUEDiWnp8+aXtPfr5Z8/b6gmr1da52eOREzJTtKg8sGOHOoMYVQLHFaDU+PH+bZsjhw/bAi4//2xszNZiUapcOdn3rFlGtDa0NGmS/bphxN8cn34q+woPl0CzKydP2gImmWP2nqhWTfazaJF3+zHSwYO27/c//zRuv//9Z/sa/vdf4/ZrhIsXbX9H7dzp3rYvvyzb9ezper2jR23X0GvXPG+rMw0byv6//db4fecWIRV4UUqpDz/8UJUrV07lz59f3XPPPWp9pqtT48aNVY8ePZxu6yjwYrVa1fDhw1VsbKyKiIhQ999/v9rlxm15DLwQERERUW6i5w7rLVuUeuSRnEdlGD3iwd12Bpqno3NcbZf59yFDlLpyxb/PSSkJ+midBW3aKPXNN/4LhHXpIvtv2zb7YxkZSj3wgK0P/PRp44+vmxZBc/UBiYxU6uzZADbSxld3dSulPAounTplG7W0bp0Xx74up7fD7lq1fbuth3Pt2px3fv68RB8WLzYskOYsZufN+1G9uuxj/nxDmqi2bZP9RUXJ4C5XLl2yddy/9pp7x9FiYLGxvrlj25nFi+W4ERFK/Ttyivzy0EO2FVq0UJ+gtwKUKlJEqRMn/Nc2R4YOlSY2aOCbeK7W0fvww8bvO5idP28beTVhgrF/c1itSrVvL/suX97118GMGbZ4vbe6d5d9jRrl/b68pf0tl5QkbUpIMP4YWgBV78gif/nhB2lXuXLuf2a1bStWdL3eli2y3k03ed5OV7RRSu6O2MlLQi7wEmwYeCEiIiKivGrcuJyDL0BwpMUKFE9H5zjbLjlZqX79bMtq1fJoMIHHsgZdtBv8/RUI27rV1gGd9Q7RF16w3VFv5B2zHtGbay4IPhxuBSW8tWuXrh6msWNt57cRnchuvR0tWsgvjz3m/YE94Iv3Iy3N+HQzFotSxYrpC4698YbtGnbxonvHuXRJqRo1ZPsmTfwTZM/IsAWqhg1TkvfsvfeUmjPHttKqVSoDYaq26Q8FKNW3r+/b5czx47aMdz/+6Jtj7Nwp+zeb/RsAC7Rp0+R533qrbwJaZ8/aRmm2b+/8GO3ayTqvvOL9McePl309+qj3+/KGo79zihUz/qYNLYWrEUErIw0ZIu164gn3tz171nZNd5UybulSWadaNc/b6cqAAbL/l1/2zf5zA3fiBmGBqy5DRERERETBpmRJfesFQyH4QElKAvbvl6Leycky3bcv5/rozrbr1An46CNgwQKgRAlgyxYpljt5snRb+NKPPwKPPQakp0v7Zs6UevCAb+vVZ1a9OtCypTzXt9+2FUl/9VX5HQCmTAFq1PDN8XXTe9IHwYdj9Wrg4EHnjysFHDgg63ll5EigShXg229drqaUnM8A0Lev88LW7tD9dqTulArWZjMwerRnB0tP92y763zxfmzcKNuVKweUKuVV824IC7MVFXfVlsOHbS/lO+8ABQq4d5yoKDlloqPlOvjaa5611x0zZgDbtgFFiwIvvgigcmXgmWeANm1sKzVsCHO9ezA+bAgA4NNP5XocCGPHApcuAXXqAM2a+eYYlSoBdesCFotcc/OKqVNl2rOnMdeirGJi5PUMD5fzfMqU7Oukp8v3LwC0aOH9MevUkemmTd7vy1MpKUDbttmvdWfOyPKUFOOO1bat/K3y55/yuQ4WP/0kU08+szExQM2aMr92rfP1jh2TaWys+8fQQ9vv0aO+2X9ew8ALERERERHdULq0sevlVp4GJVxt17KldCI88ABw5Qrw1FMSFDl5Uh63WGxBiRUr5HdvLFkCtG4NXL0qx5k1yxZ08bcXX5TptGlAkyZA587AqFGyrGVLoGPHwLTLTgh9OPwWIwoLA6xWoF8/l700K1YAu3YBBQsCXbp4eczr9L7MJVOuR3x695aeZndYLMCYMUD58hIZ8ZAv3o9ff5VpvXrut8cVLfCyaqVyesF5+WXg4kU5tqefzcqVgU8+kfnXXwd+/tmrZrt0+TIwYoTMv/SSBF8cMpmAyZPRaN8MtG8vp/bgwb4PgGd15gwwcaLMv/KKb4IDmu7dZTp9unH7NPq7ykh79wIrV8prqj13X6hXD3jzTZkfOBDYscP+8ZUrgfPnJWiqBU28Ubu2XI6PHAlM7N9iAQYNcvxZ0ZYNHmzcuVCsGPDwwzKfnGzMPr116BCwfbucW/ff79k+GjaU6Zo1ztfxdeBFC+Qz8GIMBl6IiIiIiOiGhAQgLs55R4/JBMTH2zrnyFilS8tdsO+/D+TPL6NgataUgQXly9uCEk2ayO+e3kG6ZAnQqpUEXVq3DmzQBZA76J1ZtMjYO2U9FkIfDr/FiF56CbjjDuD0aeDJJ532UE+aJNOuXYFChbw85nU5vR2a56xv488Hn5MPkbvCwuQEPHLE1nPvAS14mhN33g8t8FK3rvvtcUU7fdd+fw7WJvdlu+Bs2iQBUgAYN867oECXLhIPU0rmDx70TYf9hx/KvsuVAwYMgAxj+eor4N9/s69cqxYQH4933wUiI6Ud/r7+TJggnfI1axozGsKVjh3l2r9lizGje1JSjP2uMtqMGTJt2lQu17707LMy8uHyZaBDB5lqFi6UaYsWcpnxVoECMvgQMGbUi7vBs1Wr/DTKMpPOnWWanCxBUncZHSBculSmd98tgSFP6Am8HD8uU70j1N3FwIvB/JD6LOSwxgsRERER5WWeFpAnY/3xh1KVK7uuC6Hn/chaq+W776S4NKBUq1Y5F9H2Nb/WI/FWiHw4tNfUUTF3w1/TP/9UKl8+2fGMGdkePnJEqfBweXjzZgOOl4mrtwOwFdAOD1dqxAgPz/Vff7Xt1IMnsHCh7fPm6sed98NqlcL0gFJr1rjdJJfSv0lR0TivAKX+RHW7F9UKk2pY5YQClOra1ZjjXbokdRoApSLyW+1ek7g4q9cfqVOnlCpSRPY3ffr1hc89p6sQw4i+xxQg9TouXfKuHXqdO6dU0aLSvG++8c8xtSLozzzj3X60z6On31W+ZrEodfPN0qavv/bPMY8etX1Wn3pKPuPLlytVvLgsmzfPuGN17y77HDXKu/04qtMSF5f9/fv3X6WmTpVrgfYZy+knOdm7tmV26ZJSBQt6dh3U+xzd0amT9zV7DhyQfYSFSR0vR3r2lHXeesvz47iycaPsv2xZ3+w/N3AnbsDAiwMMvBARERFRXudpAXkyVlqarfPYkw50R++j9hMMQRelQqpmvQiRD4ezoIT2Y2hz33xTdhoTo9TBg3YPaUXY69c38HiZOH07vrqkjhxR6rHHbMurV1dqwwbbtlmDkk4DH+3byw6aN3erbVOmSOFyQKk779TeD6vD92PaNP373b/fFlAyNCBwPWLXFD8pQKmJ6GfXyG/RTgFKRUVZ1YEDxh12wgSlgOyviwkWZYJ3wZdnnpF91ayZ6f1t1EgWTpnifMNevdRFRKm4YhcUoNTrr3veBneMHi1Nq1zZoMCojpN8/nw5ZqlSSl275vlhgj2Avny5tKVwYf8F0pRSaskS2+ugBVy0n7JljbsWjx8v+2zRwvN95BQ8GzpUqT59lLr1Vn3f277+HteCTS1b6riO63yOnrwfFotSJUrIflat8uip3KAFB5cudfz4Qw/lfPnyhhb8yZdPnhdlx8CLlxh4ISIiIiJyo1OSfEZvUKJ9e6W++EJuztfuknTWuaD9+Otu6pwkJ+t7jkbeKeu1EPlwuAq8zZxp4IGuXVPq7rtlxw8+KD2cyckq4+dUVa6cdKg7GAxjmGxvx5k0pUqXVqpXL2U9fUZ9+61SN90kzQsLU2rYMFlX9x3Pe/bYRvX89FOO7bFa5W5kbb89eyqVnq7U3GHrVJz5kN0xw3Htxsumt5Nr1izZ9q673HmVdLh+wXkNryhAqQ6YeaOhlxGhbsY+BSg1qudeww6ZkaFUXPGLDgMvWvAlvvhFjz5i+/crlT+/7Of7768vTE9XKipKFu7Y4Xzj6xHDmeWeU4BSBQooQ4NNjly4YOu8NeTzovO2/qtXbQGBG6+Tm0IhgN6tm7ShTx//H7t1a+cBKaNGA61ZI/ssU8az7XMKnmX9MZuVqldPqZdeksti2bJ+GmWZyfDh2Y/lauSKrwKEmzbJ9gULyiXGG126yL5GjnT8+F13yeOLFnl3HGeuXrW9HidP+uYYoY6BFy8x8EJERERERMFAb1Ai6098vFKRkcZ3LvhCKHTYhbKsQYmXX5bXs1AhpXbvNvBA27dLr1OmnDOL8IgClCpW8Iq6fNnAY+VkxAhpw6233ugFO3HC1qHl6nPhtBN08GBZqVYtlxESi0WpgQNt+3zhBQnEaJHQDISpVDRWyeioUtFYbcIdKhKXFKDUq6/qe3pDhsi++/d3+5Vx7foFJxWNFaBUaRxS1utP5C28IJ2a+E9dnGpc1Db15wx9n/+f3b9YaR3tTZpcfw+UUuq332RhkSKuI12nTikVHa2sgGpQ7bQC5PzxpQ8+kKbdcovnI09ucPO2/qefloc7dvTscMEeQD93zhZvW7fOv8fOyJCghK+/jy9ckMAyoNThw+5vr/e7uE0b6fQ/e9Z+e39n4nTnFE9Lk/ddGwFn9N8b2ki1li29f16TJsm+7r/f8eNa4Cjz6E2jFSsmx9i2zXfHCGXuxA0MKOFEREREREREvqC32PZjjwH33WcrinrgAHDlivP1lTK+0K2nQqhmfUgym4HERKBTJ5mOGiWv5fnzUlj76lWDDrRzJ3DxIixn07ACjTETHfEmXgYA9LowEZHf+6m69tGjwPvvy/xbb0nlcAAlSkg99XnznBezVkqmgwc7KLT8yitATAywbRuwYYPD7a9elSLxEybI72PHAqNHAyarBRg0CFAKZliRiJXohFlIxErciT8wCf0AAKNGKfz0U85P8ddfZVq3bs7ruuX6BacufkU+pOMIymAvbsERlMJbeAkA8A6eR4HyxlV1PrJil6HrabZskfcbAN59N9P1Zf16mdat67qqebFiwBNPwARgfPTLMJmAr78GfvnFrWboduUKMGaMzL/4IhAe7sXOLLbzLRsnJ3n37jKdPx84d879Q+r9rtK7ntFmz5bi9pUq+eBzk4PVq4FDh5w/btT3cXQ0ULmyzG/a5P72R47oW69NG+DRR+VymFlSEjBnDlC2rP3yuDhZnpTkfpucyekUVwro2VPaWaECULgwUL++7ashJ3pfC4123W7WzL3tHGnYUKbr1wPXrtk/phRw/LjMx8Z6fyxntL8ljx713THyCgZeiIiIiIiIgpTeoMTs2cCyZdJZcPIkMGKEvv2727ngC2YzMH68zGd9ntrv48bJeuS98HAgORkoXlw65154wYCdXu8FS1GtUR770QQr0BkzsR71AQDlsc9JNMMHXn0VuHgRuOceoG3bbA8XKQJYrc43d9oJWrw4MG0asHUrLHfejRXjNmPm079gxbjNsKRbcP68dPLNmiWxnuRkecoAZGcHDzo9Zk9MQ298CqVM6NxZju9MerqtU9XwDuTrF5wo01XUwUYAwBg8i/9hCi6gEOphHTrFrTE0Cloa+i5CetfTPP+8vJcdOgB16mR6YN06mdavn/NOhgwBzGbctWES/tfqJADp7HV1/njqiy/kehwfbwuCeCyH883RSX7XXUCVKhIAmjPHvcMpJd8/rgQ6gD5tmkx79XL+feorer9njfg+vusumXoSeDEieJaUBOzfD6SmyjUwNRXYt8/YoAuQ8ykOyM0F330n7QGk3XbXAhfcCRBevAisWSPzRgReqlaV76mLFyWAnNm5c/IdAAAljYt/Z8PAi3EYeCEiIiIiIgpSngQlihcHmjTRt/9A3X2clT/vlCV5XbWOyHHjgIULvdzh6tVIOXg32mIODiLLmwiFgZiAlAN1fDfEymIBVqyQ25k//VSWjRnjsIdVb+fmN99IENNO69ZImXoO5QscQ5MhtdF54r1oMqQ24qNOoPZtF/Dzz3LX+eLFMsIIO3bIkJe+fXM83gQMxJ3lT+PUKaBdO1vnWlZ//ikja4oWBW67Td9z0e36BSdFPYZtqAEA+ARP4Uc8DABohQUwjR9naBQ0IdGMOByACa6jGV//UQWXLunb57JlwJIlEgB7880sD7oTeLn5ZoncAHgTr6BwYeC334AZM/S1Q6/0dOCdd2T++eeB/Pm93KEHPf0mky3gM326/kMpBQwbBrzxhm2Zo8CGUhITDUQA/Z9/pGM8LAzo2tX/x/fnaCAtsOBJ4EW70cMZvcGzrKMsffGe6z3Fe/YEVq6Ua/nhwzKKxNXNLABQsKBt1IkeK1fKyJTy5YFbb9W/nTNhYUCDBjKvBXQ0x47JtHBhIDLS+2M5owVetOOR5xh4ISIiIiIiCmKeBCVCMX2Xv+6UJfHoo3JDPyB3gbsaZZETy6GjGITxkKwvWbsZ5CQcjHGwHPLB7bMpKdLj1aQJ8OyzMhwhMtJB1ETo7dycPFlSuSQkSAzn77+BlOfWo+2Ye3DQUspu3SPWWOw9VhCFItOxYpnFdtfzxInASy8Bu3JOkxWJq5jz1t8oUkRSiT37rOP1MqcZ88Wd+ylIQlvMwXkUyvKIwksYjRQY+4E0JyZgfPHXACBb8EV+VwCs+Py7MrjrLmDzZtf7s1oleAFIvKtixSwrrFkj54yewAsgUQUAsb8uxIgXJBr2/PNyJ/3MmRLv83Yg15dfAv/9J52djz/u3b4AeNzT37WrnFOrVwN79+a8udUK9O9vS980fjwwd2727yqt4/3zz12nwPQVLcjcrFn2tvmDP7+PvRnxYjZLIN6RYBt9qvcU79EDaNRIbkgBXN/MorlwQb4bHaUxcyRzmjGjrsla4MdZ4MWXacYy758jXgzgh5ozIcedIjlERERERET+kLVIek6FeP1d6JZCz9WrStWpI+dFgwaeF/ROHfuHvoLF9V5Q6uBB456AmwXElZLPTVyc4820n8KFlapdO/vycKQrwOpkO6sqhUMqY9kK28GWLFGqRQulJk5UqlQp5wfNVF170SLb4pkzsz/lrl3lsZEjjXsZ7V6bslYFWJw002pIEfBs5s5Vc5Gk4vCf3fHi8a+aiyT188hVqnRpWZY/v1JjxyplsTje1cyZsl6hQkodP25Q+xYsUOrSJXX1qrrRjsw/cXGeX0+vXVOqYkXZz/vvG9RePSe5kzeyaVN5+NVXc2539+620/fzz+0Pn/m7ats2pYoUkXU7dVLKajXoeeqgvRSAUt9847/jZuWv7+MLF5QKC5N9HzniWTudnS7B9DdDTqd4pkuqQ3Pn2s6LzM+xXz/bPgcM0HeuVqki68+ZY9zzW71a9hkba9+G2bNt39e+9M47cpxu3Xx7nFDlTtyAgRcHGHghIiIiIqLcwFnnQjB1oFBg7dkjgQZAqZdf9mwfyV857qjP+pOMjkoVL67UpUuOd+ROdDFzj6qbPW96O0H//VdiJs2aKWUO0/ccUx8Z47i9bvS8vviiPBQdrdSOHfa7ue02eeyHH5y/NJ5KTc35+QGynuHmzlUZZcupVDRWyeioUtFYZZQtd+N1OXFCqZYtbW148EGljh6VTbXTZsYMW2Dk9dd90kSnp5qnnedffin7KFFCOs0NbayrwMtbbzncbMYMebhiReedzlevKtWunaxnNiv19dc5N2fZMqXCw3MIGrp7d4EOP/0kxyxSRKnLl73enVf89X1ctarse/Fi97a7elWpW2+1fRcY/FYYzttglrPTbcoU2z7793cdfPnvP1kvLEypM2cMemJKztX8+WXfu3fblk+cKMuSkow7liPTp8txHnjAt8cJVQy8eImBFyIiIiIiyi180JdFucw339g6rJYudX/7xYt1dthXH5A9uqP1hjrqlXQ2lODQIaWef96rKIG7naCf916vL7g0YK3zF8rRQW+6KdtBr11TqkkTebhKFaXOn5flJ0/aNjt1yvlhPJWcrO8lTU42+MA7dij1yy/yxF1crKxWpT7+WKnISGlHyZJyOmV9ScPCnAQDnntOhnIcOOB2EyXOZ1XORjzldIe9IxaLUpUru4yDeGfKlOwNjYqyfbZOnMi2yfnzEvADlFrr4FS+fFmpRx+Vx/PnV2rePP3N+fxzWzO+/DLLg+58/t3QubPsql8/r3ZjGH98H3frJs85p1FLWU2YYPtcpaUZ3y5f8FUw64sv9AVftI9Y/freHc+RBg1k31On2pYNHy7LnnrK+ONltmSJHKdGDd8eJ1Qx8OIlBl6IiIiIiIgoL+nTRzpaYmNtIwlykpGh1KefSuzAVUe9CddTVF2zKpWebtvBihVywMcfd50ybOZMuXX9mWekJ0hPdEBHlMCdTlDd6dTG/pHzi5aaKinIAJk6cPSobfRGp04Sk3j7bfm9bFnfdNimzjrqTSzLc088ITseNkzX6tu25XwaZLvj/do1pQoUkAe3b3e7ianJhw1/bb79VrYpUkQpn3Q/aRHVihVtJ/mZM0rdfrssf+ghhznbevSQh/v0sV9+4YItFVlkpFI//uh+k7R4af78Sq1adX2hBykD9Th71hak27DBo12EpHHj5Dm3bKl/m7NnZTAioNSkSb5rmy/4Kpg1darttOzXz3HwpUMHedwXqR+1z8rjj9uWad/TvjheZlu22IJwlJ07cYOsVe+IiIiIiIiIKI8ZNw6oXl2K93brJoWzXUlNBe68E+jTBzhxAihTRpabTMpuPZNJASaTFGUONwH58tkenDxZDjhlinS1ZqUtGzBAKhe//z6wdatUMK5USd8Tc1GF2WwGEhOBTp1k6qpodEK/GogzH85WAF5jghXx5kNI6FfDdXu0g77zjvz+/fe2ismZxMYC334rq8+cCdx0E/DCC/LYoUNA+fJSI95ICdsnIw4HXDxHhfgSlwwpAn7DtWu2J/Lgg7blf/wBvPoqcORItk2qVQPWrQMKFnS968GDMxW+//NP4NIlICYGqFzZ7WYesZTUtd7Bg/r2pxTwxhsyP2gQULiw203K2cqVMn3kEdtJXqQIMHs2EB0N3Huvw826d5fp118D06cDK1YAp08DzZsDP/8sr/uPP8rv7nrrLaBNGyA9HXjsMeCfXRZ5AVx9/u3eSP2++Qa4cgWoWhWoU8f9toaqu+6S6aZN+rd5+23g1Cn5aDzxhG/a5SvuXMfd0bMn8MUX8nXz8cfyNZT5NLVYgKVLZb5ZM2OOmVnDhjJds8a27PhxmcbGGn+8zLT9nzgBZGT49li5HQMvRERERERERHlcVJR0VBYoIJ1Jo0dLh+vMmTLV+j3/+Uc6TO+7T/qyixYFxo8H9u8H5s4FypY12e03Ls6EOXOApCQHB50+XTpVXVFKegRLlgR69QJmzZLep+3bgbg46RVzxGQC4uNhVJTAnN+M8UP/k11nCUxov48begDm/Dp7/apUAe65R17Y5GSHqzRsCHTtKvNnz9o/dugQ0LatgcEXiwXmaVMwHoMAmLK9rCYoAArj4t43rGMTgPTknz4tPX2NG9uW9+0LjBoFfPedw802bgQuXHC+W6WAAweA1auvL1i/XqZ16wJh7neFlY7T96QHD5ZmHzrk+HGLRT5Pw4bJ5yc6Ghg40O3m6LNihUwTE+2X16wJ7N0LvPKKw9fi9GnpvL54UTqfmzQBSpUC1q6VuM3SpfZvlTvCwoAZM4C775aP9SMPXMWZg+68kfpNmybTXr2cXyZyo9q15fkeOgQcPZrz+v/9B4wdK/PvvguEh/u0eSGlZ09g6lRb8KV/fzklLRbg88/ls1KggC3YZSQtLrprlwRAAFuM3teBlxIl5LOqlO3Y5BkGXoiIiIiIiIgIVasCEyfK/CuvSIdr584yLVcOaN1a1pk/XzpmBwwAdu+WjuN8+SS4sn+/jIZJTpbpvn1Ogi4AkD+/BB/0GDtWbj/u0EF6hcxmifgA2XtVtd/HjTPu9mcASe/Ww5xhG1DWbN+bGWc+gjnDNiDp3Xru7bBnT5lOnerwjn+LBVi2zPGmXg4GyO7SJeCxx5B061bMmXkNZcvaPxxXKgNz0BZJ214Dzpwx4IDXffONTNu2tX+vHn1UposXO9zMwUAY1+utWyfT+vXdbyMkfhdX1up0NBAgHZWnTslAnZtvlvN+6VLb6LGUFBmp1KSJDN7SttHiI4Y6fhzYsUPmGzXK/njJTCN4Ll+Whl9vY/v22c+pa9dk+vLLQD03T/OsChQAFi6Ua8rfBwqgDeYiHflcb6T3Db9u5055y81mW/AyryhY0DaoS8+ol1deAa5elWCa9rEjmx49bMGXSZOAhx6Sz3HfvvL4pUvArbcaPwKxWDEZ3QdI0BOwBV5K6huA5zGzWUZZAhLAzHzzBbmHgRciIiIiIiIiAuA85dHhw8CCBdIB++CDcrf+hx8CxYvbr+d22hcXqcDsaLnMMktKAubMQfYoQRycD7PxTtK79bD/UixSx25G8oBfkDp2M/ZdKuV+0AUAOnaU4NPWrcDmzdkeXr3adeoqLwYDZFeokASydu1CUsf82QNoB/MhqfpuyTuzcKEBB4T09s6fL/Pt29s/1qKFTJculXxRWeg9bW6s52XgxWwGxk8IA2ByOOLJBIWZM2WEWKNG0kk5b56kIKpUSWJsbdtmfz8vXDB45JLmxAkZMlWnTvYPaWb//CORlHbtYEm3OM36BUjH84QJxnTAliolMbVCBTKQivvwFCYhA2FYgcaYiY5YgcawZO6y1PuGXzd9ukwfekiOlddoqdVyCrz8/jvw1Vcy/957eWtkkDu04AsALFmS/XNs+AjE67KmG/NXqrGUFBnNAwAvvSTBYl+kt8wLGHghIiIiIiIiIlgsOWf+KlFCOkyrVjXooAkJ3qUMc3uYjffM+c1IHFwbnT68F4mDa+tPL5ZV0aIyjAiw5UXKxO1RHUa4nnrKYQCtbVtZZ+5cY47100/AuXMSVNN6GDW1aklA7dIleU+zcOu0OX4c2LNHHtA7wsqBJKRgDtqiLOzziMXhIOagLdqHp6BjRymtsnWrpCUqVEhiG9On+6SMiXPVqklE7tdfXa+XkSGvTWoqVj8x3X+BPgA1agDffGNCGCz4Ao+jBE6hCVagM2aiCVagPPYjBUlupwy0WCSdGWAbVJbX6KnzopSkvFNKRjbmpTo4nujaVS7Zjvjqc5w58HLpki29oi8DLykpcqnXRrlpfBVcyu0YeCEiIiIiIiKiHEdYAMDJk8Z1vAIwJmWYr6or+0PPntI7HxGR7SG3R3V4auFCySXjbKiDpk0bmS5ZAqSleXlQAD/8INN27bLXGjGZXKYbc+u02b1biqlUqeK85zQnFikCn4QU7Ed5pCIRyeiEVCRiHyogyTTPrte1enVJ23f4MDB0qOtdGx3QsJNTPZvKlYFPPwUAHPnyZ127NDLQ99CjZvzvfqmddA5F7B47hLJoi9lI6fitW5/ppUvldS9e3DZwKq/RE3j54Qdg+XIZdPfmm/5pVyhbvdp1lkVffI61wMumTXI/ASBfFYUKGXeMzK5f5vwbJM7lGHghIiIiIiIiosCMsAACkjIsaDRrJi/ou+9me8jbwUC6WK3Sk9akia3eijPVqknerPR0p7VX3DJxogwP6d/f8eNar/nixQ57AnWfNg0aAGfPSo+8pzJFJc2wIhEr0QmzkIiVMMPqtNe1YEH9IwkM+1xdvizPV6/OnYG+fVEah3Wt7nWgLxOLBfhxVwUA2d9fBUntNnhWPbc6erXBY507S1AhL6pdW64Phw7Z6oJklpEho10A6WgvX96frQtNgfh+vPlmub5lZNguubGxvksJ59f0lnkEAy9ERERERERE5L8RFo4EIGVYUDCbZTSGk4e8HQyUo+XL5XWOiQFatnS9rskkuWaiomRIgbfCwqQgym23OX78vvvkWGfPOu0N1H3ahIdnj9C4w4teV79/rr7/Xipzd+qkf5uxY5FQ+wLicCBbDRuNIYG+LGwdvY57khVMbnX0njljKxuUV9OMARLwq1xZ5h2Nepk6FdixQ06Tl17yb9tCVSC+H00m26gXLcOjL9OMBezmi1yMgRciIiIiIiIi8s8IC1dCOWWYt5QCNm4ETp2yW+zzwUCffSbTLl2AAgVyXv+ZZ6Rw+7PPenfcnNKaARJ0Wb9e8tvFxztdzS+njRe9rn7/XGlp40qU0L9NZCTMc77B+KgXpU1ZRqCYTPK714G+LIzu6J01C7h6FahZE7jjDs/blRs4Szd24QIwYoTMDx8OFCni12aFrEB9P2qBl40bZVqypLH7zyygN1/kUgy8EBEREREREZF/RliQY127SuH3L7/M9pDPBgOdOAHMmyfzvXvr26ZoUacjdHS7eBG49VZgwACpGO1KzZpAvnyeH2vLFkmP9vTTnu8D8KrX1e+fq5UrZZqY6N52FSsi6askzCk9EGXD7CMdcWGHMefZ9YYPQNPbgfvHH/rqSkydKtOePX2XjilUaIGX336zX/7ee8DRo0DFikC/fv5vV6gK1PejFnjR+HLES8BvvsiFGHghIiIiIiIiIgB5u9xKQGm9a1OnOhwN4pNRHdOnA9euSRGS2rXd3/7oUc+Ou3gxsHevVPeOitK3jVKeVXRetw74+29g5073t83MVa+rxkWvq98+VydPAlu3ynyjRh7tIunox9hvjUcqEpGMTkhFIvZZbkbSe/cCKSkGNVTk1NGr1X4ZMwaoVQv47rvsHw+LRQb5jBkjowLMZhnAldc5GvFy5Ii8TgAwenTerYHjqUB8P9aoARQqZPv9yhXfFbfnzRfGY+CFiIiIiIiIiG7Iq+VWAqpjR+kF/fNPYPNm3x9PKeDzz2Ve72gXzdGj0ht4661SyN1d33wj0w4d9A1LGDtWjvX11+4fa906mdav7/62WTnrdQWAO+/M8QPil8/VqlUyrVYNuOkm97a1WKTSulIww4pErEQnzEIiVsKM6z29gwcb2uvrsqMXVpggo1eKFgW2bwcefRRo0sSWdiklRQrDN2kCPPecLMuXD1izxrAmhqw77pDX9NAh4NgxWTZypAwyq1dPyjWR+/z9/bhgAZCebvs9OVnOeYNjoDfw5gtjMfBCRERERERERHbycrmVgChaFGjdWuanTfP98Y4ckQ706Gj3irADkuvm3DlJGfbTT+5tm5Ymxd8BCbzoceaMjJBZtMi9YwHGBl6A7L2u06dL7/amTcCvv+a4uc8/VytWyNTdNGNA5kr3jikFtyrd6zF8OJL+eRdzPjuTvaM3Pgxz5powdSqwZ48EViIiJJPaPfcADRpI8CBrk69ckeW+6pgOFQULApUry/ymTRK4mjJFfn/vPaZi84a/vh9TUuRcvnrVfvmhQ749x3nzhXEYeCEiIiIiIiIiCrSePWX69df2tzj7QpkykoLr99/t89joYTLZbpefM8e9bRculF7ESpWkfosejz4q0yVL3HtdTp4Edu+W+bp13WunK5l7Xbt3B3r0kOVaxfJA0gIvjRu7v63Rle5zcvky8MEHwPPPI+nO/S47eosWBd55R05Z7eX+5ReHWfluMHhwTki64w6ZTp0KPP44YLXKa9qgQWDbRTnLNAAtG22ZL89x3nxhDAZeiIiIiIiIiIgC7YEHgFKlgFOnpJiFr5lMwO23e7atFnjRAil6uZtmDJAaNLGxwPnz7o220EagVKoEFCumfzt3jRgBhIfL6J9A5rhSChg4EOjc2bP6Lnor3etdLydLl0req3LlgNq1HXf0ZmTYDWkpV04GhH32metd+2JwTqhJSbFdRubMsX0cmjQJXJtIv0AMQCPjMfBCRERERERERBRo4eFAt24yv2CB747z99+Sj8kb9erJqJm0NGDZMn3bnDkjo1YAoH17/ccKCwMeeUTm3Uk3ZnSaMWcqVAD+9z+JFPz2m2+P5YrJBDzxhIyYio11f/ucKt2bTEB8vKxnhPnzZdq6teNjLlsGFC/usBhJdLS+Qxg1OCfUaCmqzp3L/tjAgUzDFgr8PQCNfIOBFyIiIiIiIiKiYPDUU8CPP9qKMRhNKaBNG6mc7M3ojLAwWx4ovenGrl2T3DkPPijF392hpRtbtMh1fqnMiheX4/gjr9KoUcCuXZL7J1S5qnSvGTfOmJxDGRkyWgqw1TbKqlIlCext3AicPm33kL8H54QSVymqNEzDFvx4jucOJqX0fmPlHWlpaYiJicG5c+dQuHDhQDeHiIiIiIiIiMh769fLCJCoKODwYaBIEc/3tXKl5IQqWhQ4dgzIl8+oVmZ34YIEUtLTgb/+slUN10OpvFFJ/OuvgVq1gKpVJTDmqZQU6bnPmueoTRv3a/o4s2qV1KEpVkzOnfBwx+tVry5V4b/5xm6UlMUClC8vRcYd9WqaTDJ4Z9++vFebYsUKfenEUlPl40vBied48HInbsARL0REREREREREwSYjw/h9fvqpTNu18y7oAgANGwJ9+0rRDV8HNgoWBLp2Bfr1cz/A4++gy9at/k85dvq0pKmrUQM4ccK7fSUlwa7S/ahRsnzHDv2jjXIyb55MW7RwHnQBgObNZaqlqLvO1eAc7XejBueEGqaoyh14jucODLwQEREREREREQWTESOknsbmzcbtMy3NVty+d2/v92c2A5MmAS1buu48B2R0zJIlkm7MU1OmAB99BFSsmPO6p055dyxPffWVjDp56injghR6rFolx6tSxbP6LlllrnT/0kvAjBnApk3GBbEsFhl15SzNmCZz4CXL65mUJANwypa13yQuTpZrmfDyGqaoyj14joc+Bl6IiIiIiIiIiILJrl3A0aMymsQoycnApUvSOe+PuieZvfaa1HaZMME/x3vmGaBwYdsIH39p1gwoUEBGvGg1TPxh5UqZNm5s/L7z5ZPRNFFRxu1zwgQJjj38sOv1EhKAyEjJt7RjR7aHsw7OSU2V1Et5uUM6IUE65p3FyEwmiekmJPi3XeQZnuOhjYEXIiIiIiIiIqJg0qOHTL/+WuqaGOGzz2Tau7ex6be2bgVeeQX49VfHjx87JoUnAO97CzMygNWrgT/+cL3eunXAlSvSA+1PJUsCTz8t8yNGAFarf46rvb6+LtqhFHDxojH7iooC8ufPeZ1GjWQ+S7oxTebBOYmJTL3EFFW5D8/x0MXACxERERERERFRMGnWDChVCjh5Evj+e8/3Y7FIp/z77wO//24bvWCkceOAN9+UdFSOzJkjAYi77wYqVPDuWK+9Jh3x773nfJ1Tp4C//5b5unW9O54nhg2T0TZ//gnMnev74505A2zZIvO+GPGi+eEHoGpVeX7e+Pdf99Z/4gng1VdlxBTpwhRVRMGBgRciIiIiIiIiomASHm4LkHiabiwlBShfHmjSBHj2WVlWqJDUAzFS27a24zka4fHttzLt0MH7Y2k1P374QUa/OKKNvLn9dqB4ce+P6a5ixYAhQ2R+5EgJfvnS6tUyEqVyZQnW+UpkJLBzJ/Dll1IvyBP79sk5WauW/ho87drJ6KGqVT07Zh7FFFVEgcfACxERERERERFRsNHSjX33HXD8uHvbpqRIQOTgQfvlZ87I8pQUY9oIAPffD8TESE2aX36xf+zwYQkMAED79t4fq149CaacOZP9WJp162Rav773x/PUkCFA0aLAX38Bs2b59lhamjFfjnYBJMdR5crAhQsSfPHEggUyLVZMRl+RTzFFFVFgMfBCRERERERERBRsqlWT9FwZGXLLul4WCzBokIyCyEpbNniwcSMx8ucHWraU+Tlz7B+bPVuOee+9UtHbW2azrSD74sWO11m/XqaBDLzExMgooxIlnI/MMcprr0n9k759fXsckwno10/mP/7Y8fmVk3nzZNq6tXvbnT0r55Y2eoqIKAQw8EJEREREREREFIyefhoYOBBo2lT/NqtXZx/pkplSwIEDtpEoRtDSjc2da59uTAuCGJFmTPPoozJdtCj7YxaLLdVYvXrGHdMTgwdLbidt5JKvFCwoNYFq1/btcQCge3egQAFgxw73U9adOAGsWSPzrVq5t+1PP0nKsVdfdW87IqIAYuCFiIiIiIiIiCgYdesGjB8PVKkiKaVmzpSps9Eq164B33+vb99HjhjVSun4L1hQAj4bN9qWJycDf/wBdOli3LGaN5caODt3Av/8Y/9Yejrw4otAmzZA9erGHdMTBQrIa5KbxMQAXbvK/Ecfubft4sUSlLvjDqnz4o6mTYGwMAn4HDjg3rZERAHCwAsRERERERERUbBKSZGO6iZNgM6dZVq+fPY6LV99JcXVx4zRt9/SpY1rY2Qk0KKF1O7491/bcpNJRmIYWeQ+JgZo1EjmlyyxfywqSgIvc+YET0ELpeS9mjvX+H1PnAg89xzw55/G79sZLd3YvHnuBe88TTMGyHl1990y/9NP7m9PRBQADLwQEREREREREQWjlBRJ45U1ddihQzKqY+JE27K4OOD0aeCmm4DoaOf7NJmk3kpCgrFtHT8eOHoUaN9efk9PN3b/mb3+uqQUe+op3x3DKF99Je/VoEHAlSvG7nvaNAm0bd9u7H5dqVULePllCXqVKqVvmwsXbAETTwIvgIx0ArIH24iIgpRJKU+qYeVuaWlpiImJwblz51C4cOFAN4eIiIiIiIiI8hqLRUa2uKrXUrCgFB43m2X9VatkNMiCBba6K5m7fUwmmc6ZAyQl+a7d33wDPPGEtGXRIiBfPt8cK6vFi4GaNSWwpD3XQLtyBbjtNnkfx46VEUBHjsiIo4QEz0fmnD0rI4msVgnElSljZKuNlZ4O/PADsHIl8P77nr03v/wCNGgAFC0q9WKCZUQT5T0Wi9TIMuJzTCHHnbgBAy8OMPBCRERERERERAG1YoWkFctJaiqQmJh9eUqKjLLIHLiJjwfGjfNd0MXRMePiZDSMr46pOX3altLsxAmgRAnfHs8dn3wC9O0rdUqsVttyb16bxYslvdtttwF//21cW4NVRoa8p+fOAevWAfXqBbpFlBcF8hpHQcGduAFTjRERERERERERBRu99TOcrZeUBOzfL4GZ5GSZ7tvn26CLs7Robdtmr0njrT//lFE1w4bJ7xs2yPTWW4Mr6ALIKA3APugCePfarFghU0dBN384cgR45hlbajlfCw8HmjaVee29JtcsFjlPZs6UqcUS6BaFNn9f4yjkMfBCRERERERERBRsSpf2fj2zWTrmO3WSqa/S4Vgsche4o6Qq2rLBg43t+D11CpgyBZg+Xfa7bp0sr1/fuGMYwWKRAIUj3rw2K1fKtHFjj5vmlfR0GT01ezawa5fz9X79FRg+HNiyxftjvvEG8O+/wMCB3u8rEPwZCElJkVSFTZoAnTvLtHx5Bgc8FYhrHIU8Bl6IiIiIiIiIiIJNQoKksHFWD8NkktRhCQn+bZcjq1e7rkWjFHDggKxnlIYNgZgYSSu2cWPwBl588dqcOwf8/rvMByrwcvPNwKOPyvykSc7X++orCZh8+KH3x6xcGShXzvv9BII/AyEcmWG8QFzjKOQx8EJEREREREREFGzMZqkbAGQPvmi/jxsXHEWdvU2L5ol8+YAHH5T5BQtkZAUQfLU/fPHa7NkD3HSTpFWLi/OsXUbo10+m06YBFy9mf1wpYP58mW/d2k+NCkL+DIRwZIZvBOIaRyGPgRciIiIiIiIiomCUlATMmQOULWu/PC5OlgdLMWcj0qJ5Qhtx8e67QFoaEBkJVK1q7DG85YvX5s47pYN37VrP2mSUBx4AKlaUETgzZ2Z//PffJdgQHQ3cf78xx1y9Wt73IUOM2Z+v+TsQwpEZvhGoaxyFNAZeiIiIiIiIiIiCVVISsH8/kJoKJCfLdN++4Am6AIFLi6Z1VmtF669ckVEgwZRKyVevjckElCzpffu8ERYGPPWUzH/0Ufbgwrx5Mn3wQSAqyphjXrgAfPedvMeOghnBxt+BEI7M8I2cPseAPB4MqR8paDDwQkREREREREQUzMxmIDER6NRJpsGQXiyzQKRFS0kBevXKvjzY6li4em0A6Xh/+WX9r43VGlwBh549ZaTR5s22dG8aX6QZa9wYiIgA/vsP2LXLuP36ir8DIRyZ4Rs5fY5NJnncbA6uzycFFAMvRERERERERETkHX+mRQu1OhbOXpt8+WQ6dixw6pS+ff34o3SaDx1qbBs9Vbw40LevtKdMGdvy3buB7duB8HDgkUeMO16BArZRBUuWGLdfX/F3ICRQo8/yghYtgNmzs3+O4+Ptr3H9+wNvvskADCE80A0gIiIiIiIiIqJcICkJaNVK0iYdOSKdyQkJxo/QcSd9U2Kiscf2lKPXpkIFeX127ZJO3Z9/lsCCKytXAseOSV2VYDF2bPZlu3cDxYpJPZqiRY09XvPm8lotWSIBuGCWkCAp4Y4fd/y4yWRsiiptZEabNo6PBQDPPitp+aKjjTlmXvHpp8CYMcDzzwPVqzu+xq1ZA0yaJPN//AFMmwYULBiwJlNgMfBCRERERERERETG0NKi+VKo1rFw9Nr8+CPQsCGwbh3QsaOkSAt30V23YoVMgyWg5MzDD0uA6MQJ4/fdvDkwbJi8FleuSKqzYFawoPPAC2B8Gr7mzSWAd+mS/fK4OAlUvfaa1N/57rucA31kM2cO8O+/QHq6889fw4YSoOnfH5g7V4Kq8+cDFSvKCDxfB6UpqDDVGBERERERERERhY7cVMeialVg0SIJHixaJMXqnaUoOn8e2LRJ5hs39l8b9VAKSE0FnnwSWLYMmDlT7v4vWdL4Y1WvLmnNLl+WYwSziROBvXslwOHofOzf39g0fICMZFmzBnj6aXkvkpPlvdm3TwID6ekStGrRIntwhhw7dgxYtUrmHY0myqx3b3l9S5UCtm0D7r4bGDkSKF8eaNIE6NxZpuXLB08tKvIJk1JMOJdVWloaYmJicO7cORQuXDjQzSEiIiIiIiIiIo3FIp2Whw45DlJo6Zv27QudO8rnz5cO3a5dgSlTHI96+fFH4KGHJEXZ3r1+b6JLly8DN90EXLxovzwuTlJfGR1cePJJSWc2fLh0YgejAwcksHbhAjB5MvDEE7YRDytWyMiIW24Bdu601fvxh19+kVExFy4ATZsCCxcCUVH+O34o+uQTqWV0993Ahg36tjl0SM57Z+trqd+MroFFPuVO3IAjXoiIiIiIiIiIKHRodSyA7EXEtd+NTt/ka61bS4f4tGnOU41pacaCbbQLAPzwQ/agCyCdz23bGn9n/+TJwPLlwRt0AYB33pHgRv36MgpCSzXXqRPwwQcyGmjvXnnPjeLoPcjq3nvl/YqOllo5rVtLyjZybs4cmbZtq3+bsmXlHHWWzk0LGg8eLMFkynUYeCEiIiIiIiIiotCSlCSdoWXL2i+PiwvdO8jr1rUFjiwWKc6dWbDWd7FYnBe591XnctaAWzB67z1gxAgZLRGWpQs2Ohp48UWZf/114OpV74938iRQrpykq8sphVjDhsD330tQ4Kef5PNiRBtyo5MnJVUbkHOasaw2bnT9XiglI6NWr/a8fRS0GHghIiIiIiIiIqLQk5QE7N8vnaKZ61iEYtAlsytXgHbtZGTCmjUSsEhNBWJjgUqVpNM8mKxeDRw86PxxX3YuHz8efGnXNJGRwKuvAjVqOH78ySeB2rUlaGVEJYj33gNOnwZ+/VVf6rBGjYDvvpN1L1wArl3zvg250fz58hm84w6gYkX3tj1yxNj1KKQ4GbtIREREREREREQU5LT0TblJeDiQkSEBmObNgUKFpLi3JjHRN3VTPBWozuWJE6WAfMeOwMyZxu7bGytXAg0aOE8Zp4mKAn7/3ZjROydOyOsBAKNG6d9nYqKMpKpaFShYUJZZLLZaNKVLAwkJoZW2z2h16gD9+wPVqrm/benSxq5HIYUjXoiIiIiIiIiIiIJFeDgwaxZw++2Spihz0AXwXd0UTwWqc7lWLZl+/z3w9dcSQAh0rYzffwfuuw+45x4ZRZITo1Kmvfee1He56y6gRQv3tr3nHlvQJSVFRlY1aQJ07izT8uWD51wLhNq1Jaj11FPub5uQIOkPnb3PJhMQHy/rUa7DwAsREREREREREVEwiYhw3nEfbEW5A9W5fOSI7DstDejaNfBBgowMoE8fwGoFKle2BTNyopS0uWlTCZ646/hxz0a7ZJWSIjVMTp2yXx5sgb5QYjbL6DTA+fsyblzeHlGUizHwQkREREREREREFExWrwYOH3b+eDAV5XbVuaz9bnTnckqKpBjLWhslkEGCjz4CNm0CihQBPvhA/3YZGcCzzwLLlgEff+z+cceMkZFRd98NPPKI+9sDEsAbNMjxY8EW6POnyZOBVau8e95JScCcOUDZsvbLIyNlebCkDCTDMfBCREREREREREQUTEKtKLezzuW4OOM7l7UggaOC9IEKEhw4ALzyisy/8w5QqpT+bfPlA0aMsG17/rz+ba9cAWbMkHlvRrusXg0cPOj88WAK9PnLuXPAwIFA48bArl3e7SspCdi/H0hNBd59V5alp0stIMq1GHghIiIiIiIiIiIKJqFYlDtz53Jyskz37TP+jv5gDBIMHCip4e69F3jiCfe379pVavqcOgV8+KH+7SIjgW3bZETRQw+5f1xNqAX6/GHRIuDaNaBKFaBqVe/3ZzYDiYnAsGFSV8dqBb76yvv9UtBi4IWIiIiIiIiIiCiYhGpRbq1zuVMnmfqidkWwBQnmz5ef8HDgk0+AMA+6W8PDgZEjZf6992S0hV433SQjgDwd7QKEZqDP1+bMkWnbtsbvu1cvmWqjlShXYuCFiIiIiIiIiIgomASibkqo0Nv5X6iQb9uhue02GekybBhQvbrn++nQQUZWnDkDjB2b8/r793t+rKxCNdDnK+fPAz/+KPO+CLx07Ai88QaweLHx+6agwcALERERERERERFRsPFn3ZRQoidIkC+fpPz65hvHtWCMVK2apDUbNcq7/ZjNtn2MHQucPet83SNHJAXW/fe7NzrG1bGdBfo0eSnQ9913wNWrElSrUcP4/RcpArz8sgSzgpHFAqxYAcycKVN/1kvKRRh4ISIiIiIiIiIiCkb+qpsSSvSMBipRAjh2TEYWPPQQsGePccfXOqW//NLWKR0WBuTP7/2+27SRei/JyUBMjPP13n4buHIFuHwZKFzY++MCzgN98fF5L9CnpRlr1867FG6hKCUFKF8eaNIE6NxZpuXLy3Jyi0kpX4d9Q09aWhpiYmJw7tw5FDbq4kVERERERERERETGSEmR2iYHD9qWxcfLyIyHHwbefRd46y0ZuRAZKSMMhg0DIiIkWLJ6tYwcKV1aRtHoGc3h6JhlywITJvgvMHHoEFCxojyvn34CHnjA2P07e20OHgRKljQmwBTMrFZJ+bZrF/D778Add/juWN9/L+fO449LkCfQUlIktVrWcIEWfMprATgH3IkbBMWIl48++gjly5dHZGQk6tatiw0bNjhdNyUlBXXq1EGRIkUQHR2N2rVr48svv7Rbp2fPnjCZTHY/Dz74oK+fBhEREREREREREfmDq9FAkZHAiBHA1q1A06YyOmT4cKBWLeDzzz27o1/rlM4cdAGAw4dluS9GBGRkZF/29tsSdGnQQJ6b0cxmIDER6NRJpmazpD67/XZg8mTjjxdswsKAHTuAjRuB2rV9e6xffgGWLAGmTPHtcfSwWCSo6GiMhrZs8GCmHXNDwEe8fPPNN+jevTsmT56MunXrYty4cZg9ezZ27dqFkiVLZlt/xYoVOHPmDCpXroz8+fNj8eLFeOaZZ/Ddd9+hefPmACTwcuzYMUydOvXGdhEREShatKiuNnHECxERERERERERUS6gFDBrFjBkCFC0qIxkcPeO/t27gXvvBU6edHwMk0nqzuzbZ0wdlIwMGbEzaZIEAEqVkuUHD8pol/R04OefpcaLP3zyCdC3L1CsGPDPP/I6kvf++UfqyISFAf/9lz3Nmz+tWCFByJykpkpALo8KqREvH3zwAXr37o1evXqhatWqmDx5MgoUKIAvvvjC4fqJiYl47LHHUKVKFVSsWBGDBg1CzZo1sWbNGrv1IiIiUKpUqRs/eoMuRERERERERERElEuYTDJ6Y/t2KUTv7I5+pYAuXYB69SS4cfSo7fEXX3QedNG2P3BAUnQZwWwGFi6UQMvbb9uWjx4tQZeEBOC++4w5lh6PPy7pt06fBt5803/H9beMDMejjHzl1lvlvbRagRkz/HdcR44cMXY9CmzgJT09HZs2bULTTMPiwsLC0LRpU6xbty7H7ZVSWLZsGXbt2oVGjRrZPbZixQqULFkSlSpVwlNPPYVTp0453c/Vq1eRlpZm90NERERERERERES5xNatOXcaX7kC/PorsHcvcPy4bXmhQvqOYVSntMkEvP66zE+aBMyeDXz5JfDdd7Ls1Vf9W/Q9PBx47z2Z//BDYM8e/x3bnxYvlro2w4f775i9esl06lTHQUF/KV3a2PUosIGXkydPwmKxIDY21m55bGwsjmaOKmdx7tw5FCxYEPnz58cjjzyCDz/8EA9kKiT14IMPYsaMGVi2bBneeecdrFy5Eg89wATmEwAAIDdJREFU9BAsTnLQjR49GjExMTd+4uPjjXmCREREREREREREFHh6gyJDhwJr1sioF02PHvq2NbJTumlToEoVGeHSvj3QvTvw779A8eLAmTPGHUevBx8EHnhA2vPCC/4/vj/MmSMjmy5d8t8x27UDoqMlnd0vv/jvuFklJEi6PGcBPZMJiI+X9UiXgKca80ShQoWwefNmbNy4EW+++SaGDh2KFStW3Hi8Y8eOaNmyJWrUqIHWrVtj8eLF2Lhxo906mb344os4d+7cjZ8DBw7454kQERERERERERGR7+kNirRoIYXro6NtywLRKT1vHvDXX9mXnz4NtG0LpKQYdyw9TCYZ9WIySYBi7Vr/Ht/Xrl6V9G6AvL7+UrCgBF8AGfUSKGYzMH68zGc9z7Xfx40zpoZRHhHQwEuJEiVgNptx7Ngxu+XHjh1DKa1olANhYWG49dZbUbt2bTzzzDNo27YtRo8e7XT9W265BSVKlMA///zj8PGIiAgULlzY7oeIiIiIiIiIiIhyCW+CJ/7ulLZYgEGDHD+mpaMaPFjW86eaNaXeS0QEsGOHf4/tiMUiReFnzpSpN6/H0qXA+fNS4L5uXaNaqE+vXsC99wa+aH1SkgTVypa1Xx4XJ8uTkgLTrhAV0MBL/vz5cdddd2HZsmU3llmtVixbtgz169fXvR+r1YqrV686ffzgwYM4deoUSjMHHRERERERERERUd7jbfDEn53Sq1cDBw86f1wp4MABWc/f3nwT2LUL6N3b/8fOLCUFKF8eaNIE6NxZpuXLez4SaM4cmbZpA4T5ucu8USMZQdS1q3+Pm1VGBlC0KPD330BqKpCcLNN9+xh08UB4oBswdOhQ9OjRA3Xq1ME999yDcePG4eLFi+h1vbBQ9+7dUbZs2RsjWkaPHo06deqgYsWKuHr1Kr7//nt8+eWXmDRpEgDgwoULePXVV9GmTRuUKlUKe/bswXPPPYdbb70VzZs3D9jzJCIiIiIiIiIiogDSgieDBtkHNuLiJOiSU+dyUhLQqpUEPI4ckfRlCQnGp1/SW49G73pGKlnS/8fMKiVF0oFlLUZ/6JAsdzcQlp4OLFgg8/5MMxZsNm4E7rtPRn79+6/z0WGkS8ADLx06dMCJEycwYsQIHD16FLVr18aPP/6I2NhYAMB///2HsExRxosXL6Jfv344ePAgoqKiULlyZXz11Vfo0KEDAMBsNuPPP//E9OnTcfbsWZQpUwbNmjXD66+/joiIiIA8RyIiIiIiIiIiIgoC3gZPzGbfp4TSm7Un0Nl91q8H9u8HOnb03zG1NGxZgy6ALDOZJA1bq1b639Ply4GzZ4FSpSTlV6CcOiWjTFq2BG6+2f/HX7JEpvXqMehiAJNSjs7SvC0tLQ0xMTE4d+4c670QERERERERERGR/1gskjbr0CHHAQaTSUbp7NsXuGLnK1dKAKpwYeCff4CbbvLPcVeskLRiOUlN1R8g27MHmDJFCt2/9JI3rfPOQw8BP/4IjBwJjBrl/+PXry/BtM8/l1o+lI07cYOA1nghIiIiIiIiIiIioky8rUfjDwkJwJ13Amlp/g0S+CINW8WKwFtvBTboAgDdusl02jTAavXvsc+cATZskHmW6zAEAy9EREREREREREREwUSrR1O2rP3yuDj3a5j4QlgY8P77Mv/JJ8Bff/nnuHrTq5086dt2+ELr1jKC6N9/ZWSPP/38swR7qlaVc4y8xsALERERERERERERUbBJSpIaKqmpUvsjNVXSiwU66KJJTJR6JBYL8Nxz/jlmQoIEBpzVIDGZ5GfgQKn1cvGi6/1Nnw4sXgxcvWp4U91WoICtXs60af49tlbfpVkz/x43F2ONFwdY44WIiIiIiIiIiIgoB7t2AdWrAxkZwE8/AfnySZqv0qUlSOKLdGgpKUCbNtmXa8GYxEQJUgHALbdIzRJHdWEsFmnniRPS9gceML6t7lq/XmqtREUBR4/KCBhfUwq4+WbgwAHghx+ABx/0/TFDFGu8EBEREREREREREZFvVaoEPPWUzD/yiAQ4OneWafnyEiQxWlKSBAqy0tKwLV8uRerj44G9e4H77pM2pqXJehaLpPIaOVKCLkWLSrAmGNStC1SuDFy+DHz7rf+Ou2ABMHo00KiR/46ZyzHwQkRERERERERERESeufNOmV67Zr/80CGgbVvfBF/mz5d0YgsXOk7D1rw5sG2bLSg0eTJQqxbw9dcSEGrSBHjzTXns6lVg0SLj2+gJkwno1QsID5c0c/465h13AC+8IOnOyBBMNeYAU40RERERERERERER5cBikUDGwYOOHzeZZCTKvn2+STumR2oq8MQTQJUqwPffS2qtrEwmGS0TDPVzzp4F0tOBkiUD3RLKgqnGiIiIiIiIiIiIiMi3Vq92HnQBJMhx4ICsFyhNmgB//AFs3uw46KIZPFgCSYFWpIj/gi5XrgD/+5+MGsrI8M8x8wgGXoiIiIiIiIiIiIjIfUeOGLteThYtArp3B375xb3tfv9dUp85EwwBIkf27PFtMGj1amDqVOC55wI3IimXYuCFiIiIiIiIiIiIiNxXurSx6+Xk00+BL790vyaLvwNERmjTBrj1VmDpUt8dY8kSmTZrJunWyDDhgW4AEREREREREREREYWghASp4XLokPPaKXFxsp63jh4FfvhB5nv2dG9bfweIjFC2rEzfeQc4c0balpBg7MiUn36SabNmxu2TAHDECxERERERERERERF5wmwGxo+XeWcjJsaNMyZY8NVXknarfn2gUiX3ttUCRM7aaDIB8fHGBIiMUq6cTFesADp3llo15csDKSnG7P/wYWDrVnnuDzxgzD7pBgZeiIiIiIiIiIiIiMgzSUnAnDm2ERqasDAJliQleX8MpaQWCeD+aBfAdYBI+92oAJERUlKk7kpWhw4BbdsaE3zRRrvUqQMUL+79/sgOAy9ERERERERERERE5LmkJGD/fiA1VWqwlCkDWK3G1Uz57Tdgxw4gMhLo0MHzNjoKEMXFyXIjAkRGsFiAQYMcp27Tlg0eLOt5Q6vv0ry5d/shh1jjhYiIiIiIiIiIiIi8YzYDiYkyn54OPP44MGYM0L+/BEy8oY12SUoCYmI8309SEtCqFbB6tQSFfFE3xVurVwMHDzp/XCngwAFZT3u9PXHsmEwZePEJBl6IiIiIiIiIiIiIyDjdugHLlwO9e3sfdAGAKlWA224DevXyfl+ZA0TBSO8oIW9HEy1fLnVeSpb0bj/kEAMvRERERERERERERGScfPmkvotRnn4aGDDAuP0Fs9KljV3PlTJlvN8HOcQaL0RERERERERERETkO1ar9/swmeQnt0tIkLozzp6ryQTEx8t6nkpP93xb0oWBFyIiIiIiIiIiIiIyXloa8NxzQM2annX2HzkCfPMNcOWK8W0LVmYzMH68zGcNvmi/jxvneV2ac+eA4sWBZs2AS5c8bia5xsALERERERERERERERkvXz5gxgxg+3bgyy/d337GDKBjR+Cxx4xvWzBLSgLmzAHKlrVfHhYGVKsmj3tq+XLgwgVg/36gQAGvmknOMfBCRERERERERERERMaLigKGDZP5t94CMjL0b6sUMHWqzLdta3zbgl1SkgRHUlOB5GRg1iwZ8bJtmyzz1E8/ybR5c0OaSY4x8EJEREREREREREREvtG3L1CiBLB3LzBzpv7tfv0V2LVLRmW0b++79gUzsxlITAQ6dQI6dAD69JHlI0dKYMpdSgFLlsg8Ay8+xcALEREREREREREREflGdDTwzDMy/+abgMWibztttEubNkChQr5pW6h58UUgIgJYvVpShrnrn3+AffskBVxiouHNIxsGXoiIiIiIiIiIiIjId/r3B4oWlREss2fnvP7ly5JaCwB69fJt20JJXBzw5JMyP2KE+6NetNEuDRsCBQsa2zayw8ALEREREREREREREflOoULA4MEyP2ZMzuvPnw+kpQHlywONG/uwYSHohReAyEjgl19s9Vr0Ypoxv2HghYiIiIiIiIiIiIh8a+BACb7Mn5/zuhs3yrRHDyCMXdh2SpcGnnpK5rVAil4PPww0awY89JDx7SI7JqU8qcKTu6WlpSEmJgbnzp1D4cKFA90cIiIiIiIiIiIiorxl1y6gSBEgNjbQLQk+x44Bf/8NJCQEuiV5ijtxg3A/tYmIiIiIiIiIiIiISFy+DERFOX+8UiX/tSXUxMYyIBXkOE6LiIiIiIiIiIiIiPxj3z6gVSsgMTF7cXilgNOnA9KskHX8OLB1a87rzZsHHDrk+/YQAAZeiIiIiIiIiIiIiMhfoqOBpUuBDRuyF4dfuxYoVQro1SswbQs1P/4IVKgAdO+ePYiV2bFjQFISEB8PnDrlv/blYQy8EBEREREREREREZF/lCwJ9O0r86+9Zh8wmDoVuHYNMJkC07ZQc/fdQFgYsHkzMH++8/WWLpVp7dpA8eJ+aBgx8EJERERERERERERE/jNsGBARAfzyC5CaKssuXgS+/VbmOeJFn+LFgUGDZH7UKMBqdbzekiUybd7cL80iBl6IiIiIiIiIiIiIyJ9KlwZ695b5114DVqyQYMyFC8AttwANGwa0eSFl6FCgcGHgzz+BlJTsj1uttpRuDLz4DQMvRERERERERERERORfzz0HmM3AypVAkybApEmy/ORJKQRP+hQrBgwZIvOORr1s2QIcPw4ULAjce6/fm5dXMfBCRERERERERERERP61cSNgsWRffv480Lat49Eb5NjgwUCRIsD27cDs2faPaWnGmjQB8uf3d8vyLAZeiIiIiIiIiIiIiMh/LBZbbZKslJLp4MGOAzOUXZEiknIsf37g33/tH9PSjDVr5vdm5WUmpbQzmTRpaWmIiYnBuXPnULhw4UA3h4iIiIiIiIiIiCj3WLFCRmDkJDUVSEz0dWtyh/PngbNngfh4++UnTgA//wwkJABxcQFpWm7hTtwg3E9tIiIiIiIiIiIiIiICjhwxdj0CChWSn6xuugno1Mn/7cnjGHghIiIiIiIiIiIiIv8pXdrY9cjehg3A6tVAmTLyGiYkAGZzoFuVpzDwQkRERERERERERET+o6W9OnTIVtMlM5NJHk9I8H/bQt2wYcB779kvK1MG+PBDICkpMG3Kg8IC3QAiIiIiIiIiIiIiykPMZmD8eJk3mewf034fN46jNNyVkgK8/3725UeOAG3byuPkFwy8EBEREREREREREZF/JSUBc+YAZcvaL4+Lk+UcneEeiwUYNMjxCCJt2eDBsh75HFONEREREREREREREZH/JSUBrVpJPZIjR1iPxBurVwMHDzp/XCngwAFZLzHRb83Kqxh4ISIiIiIiIiIiIqLAMJsZCDDCkSPGrkdeYaoxIiIiIiIiIiIiIqJQVrq0seuRVxh4ISIiIiIiIiIiIiIKZQkJUh/HZHL8uMkExMfLeuRzDLwQEREREREREREREYUysxkYP17mswZftN/HjWP9HD9h4IWIiIiIiIiIiIiIKNQlJQFz5gBly9ovj4uT5UlJgWlXHhQe6AYQEREREREREREREZEBkpKAVq2A1auBI0ekpktCAke6+BkDL0REREREREREREREuYXZDCQmBroVeRpTjRERERERERERERERERmEgRciIiIiIiIiIiIiIiKDMPBCRERERERERERERERkEAZeiIiIiIiIiIiIiIiIDMLACxERERERERERERERkUEYeCEiIiIiIiIiIiIiIjIIAy9EREREREREREREREQGYeCFiIiIiIiIiIiIiIjIIAy8EBERERERERERERERGYSBFyIiIiIiIiIiIiIiIoMw8EJERERERERERERERGQQBl6IiIiIiIiIiIiIiIgMwsALERERERERERERERGRQRh4ISIiIiIiIiIiIiIiMggDL0RERERERERERERERAZh4IWIiIiIiIiIiIiIiMggDLwQEREREREREREREREZhIEXIiIiIiIiIiIiIiIigzDwQkREREREREREREREZBAGXoiIiIiIiIiIiIiIiAwSHugGBCOlFAAgLS0twC0hIiIiIiIiIiIiIqJA0+IFWvzAFQZeHDh//jwAID4+PsAtISIiIiIiIiIiIiKiYHH+/HnExMS4XMek9IRn8hir1YrDhw+jUKFCMJlMgW6O4dLS0hAfH48DBw6gcOHCgW4OEeVyvOYQkT/xmkNE/sLrDRH5E685RORPvOY4ppTC+fPnUaZMGYSFua7iwhEvDoSFhSEuLi7QzfC5woUL84NDRH7Daw4R+ROvOUTkL7zeEJE/8ZpDRP7Ea052OY100bgOyxAREREREREREREREZFuDLwQEREREREREREREREZhIGXPCgiIgIjR45EREREoJtCRHkArzlE5E+85hCRv/B6Q0T+xGsOEfkTrzneMymlVKAbQURERERERERERERElBtwxAsREREREREREREREZFBGHghIiIiIiIiIiIiIiIyCAMvREREREREREREREREBmHghYiIiIiIiIiIiIiIyCAMvORBH330EcqXL4/IyEjUrVsXGzZsCHSTiCjEjR49GnfffTcKFSqEkiVLonXr1ti1a5fdOleuXEH//v1RvHhxFCxYEG3atMGxY8cC1GIiyk3efvttmEwmDB48+MYyXnOIyCiHDh1C165dUbx4cURFRaFGjRr47bffbjyulMKIESNQunRpREVFoWnTpti9e3cAW0xEocpisWD48OGoUKECoqKiULFiRbz++utQSt1Yh9ccIvLUqlWr0KJFC5QpUwYmkwnz58+3e1zP9eX06dPo0qULChcujCJFiuDxxx/HhQsX/PgsQgcDL3nMN998g6FDh2LkyJH4/fffUatWLTRv3hzHjx8PdNOIKIStXLkS/fv3x/r167F06VJcu3YNzZo1w8WLF2+sM2TIECxatAizZ8/GypUrcfjwYSQlJQWw1USUG2zcuBGffPIJatasabec1xwiMsKZM2fQoEED5MuXDz/88AN27NiB999/H0WLFr2xzrvvvosJEyZg8uTJ+PXXXxEdHY3mzZvjypUrAWw5EYWid955B5MmTcLEiRPx119/4Z133sG7776LDz/88MY6vOYQkacuXryIWrVq4aOPPnL4uJ7rS5cuXbB9+3YsXboUixcvxqpVq9CnTx9/PYWQYlKZw+aU69WtWxd33303Jk6cCACwWq2Ij4/H008/jRdeeCHArSOi3OLEiRMoWbIkVq5ciUaNGuHcuXO46aabkJycjLZt2wIAdu7ciSpVqmDdunWoV69egFtMRKHowoULuPPOO/Hxxx/jjTfeQO3atTFu3Dhec4jIMC+88ALWrl2L1atXO3xcKYUyZcrgmWeewbPPPgsAOHfuHGJjYzFt2jR07NjRn80lohD36KOPIjY2FlOmTLmxrE2bNoiKisJXX33Faw4RGcZkMmHevHlo3bo1AH1/0/z111+oWrUqNm7ciDp16gAAfvzxRzz88MM4ePAgypQpE6inE5Q44iUPSU9Px6ZNm9C0adMby8LCwtC0aVOsW7cugC0jotzm3LlzAIBixYoBADZt2oRr167ZXX8qV66McuXK8fpDRB7r378/HnnkEbtrC8BrDhEZZ+HChahTpw7atWuHkiVL4o477sBnn3124/F9+/bh6NGjdtebmJgY1K1bl9cbInLbvffei2XLluHvv/8GAGzZsgVr1qzBQw89BIDXHCLyHT3Xl3Xr1qFIkSI3gi4A0LRpU4SFheHXX3/1e5uDXXigG0D+c/LkSVgsFsTGxtotj42Nxc6dOwPUKiLKbaxWKwYPHowGDRqgevXqAICjR48if/78KFKkiN26sbGxOHr0aABaSUShbtasWfj999+xcePGbI/xmkNERtm7dy8mTZqEoUOH4qWXXsLGjRsxcOBA5M+fHz169LhxTXH0PxavN0TkrhdeeAFpaWmoXLkyzGYzLBYL3nzzTXTp0gUAeM0hIp/Rc305evQoSpYsafd4eHg4ihUrxmuQAwy8EBGRofr3749t27ZhzZo1gW4KEeVSBw4cwKBBg7B06VJERkYGujlElItZrVbUqVMHb731FgDgjjvuwLZt2zB58mT06NEjwK0jotzm22+/xddff43k5GRUq1YNmzdvxuDBg1GmTBlec4iIQgxTjeUhJUqUgNlsxrFjx+yWHzt2DKVKlQpQq4goNxkwYAAWL16M1NRUxMXF3VheqlQppKen4+zZs3br8/pDRJ7YtGkTjh8/jjvvvBPh4eEIDw/HypUrMWHCBISHhyM2NpbXHCIyROnSpVG1alW7ZVWqVMF///0HADeuKfwfi4iMMGzYMLzwwgvo2LEjatSogW7dumHIkCEYPXo0AF5ziMh39FxfSpUqhePHj9s9npGRgdOnT/Ma5AADL3lI/vz5cdddd2HZsmU3llmtVixbtgz169cPYMuIKNQppTBgwADMmzcPy5cvR4UKFewev+uuu5AvXz6768+uXbvw33//8fpDRG67//77sXXrVmzevPnGT506ddClS5cb87zmEJERGjRogF27dtkt+/vvv3HzzTcDACpUqIBSpUrZXW/S0tLw66+/8npDRG67dOkSwsLsu+rMZjOsVisAXnOIyHf0XF/q16+Ps2fPYtOmTTfWWb58OaxWK+rWrev3Ngc7phrLY4YOHYoePXqgTp06uOeeezBu3DhcvHgRvXr1CnTTiCiE9e/fH8nJyViwYAEKFSp0I7dnTEwMoqKiEBMTg8cffxxDhw5FsWLFULhwYTz99NOoX78+6tWrF+DWE1GoKVSo0I0aUpro6GgUL178xnJec4jICEOGDMG9996Lt956C+3bt8eGDRvw6aef4tNPPwUAmEwmDB48GG+88QZuu+02VKhQAcOHD0eZMmXQunXrwDaeiEJOixYt8Oabb6JcuXKoVq0a/vjjD3zwwQf43//+B4DXHCLyzoULF/DPP//c+H3fvn3YvHkzihUrhnLlyuV4falSpQoefPBB9O7dG5MnT8a1a9cwYMAAdOzYEWXKlAnQswpeJqWUCnQjyL8mTpyIMWPG4OjRo6hduzYmTJjAqCQRecVkMjlcPnXqVPTs2RMAcOXKFTzzzDOYOXMmrl69iubNm+Pjjz/mcFQiMkRiYiJq166NcePGAeA1h4iMs3jxYrz44ovYvXs3KlSogKFDh6J37943HldKYeTIkfj0009x9uxZNGzYEB9//DFuv/32ALaaiELR+fPnMXz4cMybNw/Hjx9HmTJl0KlTJ4wYMQL58+cHwGsOEXluxYoVaNKkSbblPXr0wLRp03RdX06fPo0BAwZg0aJFCAsLQ5s2bTBhwgQULFjQn08lJDDwQkREREREREREREREZBDWeCEiIiIiIiIiIiIiIjIIAy9EREREREREREREREQGYeCFiIiIiIiIiIiIiIjIIAy8EBERERERERERERERGYSBFyIiIiIiIiIiIiIiIoMw8EJERERERERERERERGQQBl6IiIiIiIiIiIiIiIgMwsALERERERGRwUwmE+bPnx/oZhARERERUQAw8EJERERERLlKz549YTKZsv08+OCDgW4aERERERHlAeGBbgAREREREZHRHnzwQUydOtVuWURERIBaQ0REREREeQlHvBARERERUa4TERGBUqVK2f0ULVoUgKQBmzRpEh566CFERUXhlltuwZw5c+y237p1K+677z5ERUWhePHi6NOnDy5cuGC3zhdffIFq1aohIiICpUuXxoABA+weP3nyJB577DEUKFAAt912GxYuXOjbJ01EREREREGBgRciIiIiIspzhg8fjjZt2mDLli3o0qULOnbsiL/++gsAcPHiRTRv3hxFixbFxo0bMXv2bPz88892gZVJkyahf//+6NOnD7Zu3YqFCxfi1ltvtTvGq6++ivbt2+PPP//Eww8/jC5duuD06dN+fZ5EREREROR/JqWUCnQjiIiIiIiIjNKzZ0989dVXiIyMtFv+0ksv4aWXXoLJZELfvn0xadKkG4/Vq1cPd955Jz7++GN89tlneP7553HgwAFER0cDAL7//nu0aNEChw8fRmxsLMqWLYtevXrhjTfecNgGk8mEV155Ba+//joACeYULFgQP/zwA2vNEBERERHlcqzxQkREREREuU6TJk3sAisAUKxYsRvz9evXt3usfv362Lx5MwDgr7/+Qq1atW4EXQCgQYMGsFqt2LVrF0wmEw4fPoz777/fZRtq1qx5Yz46OhqFCxfG8ePHPX1KREREREQUIhh4ISIiIiKiXCc6Ojpb6i+jREVF6VovX758dr+bTCZYrVZfNImIiIiIiIIIa7wQEREREVGes379+my/V6lSBQBQpUoVbNmyBRcvXrzx+Nq1axEWFoZKlSqhUKFCKF++PJYtW+bXNhMRERERUWjgiBciIiIiIsp1rl69iqNHj9otCw8PR4kSJQAAs2fPRp06ddCwYUN8/fXX2LBhA6ZMmQIA6NKlC0aOHIkePXpg1KhROHHiBJ5++ml069YNsbGxAIBRo0ahb9++KFmyJB566CGcP38ea9euxdNPP+3fJ0pEREREREGHgRciIiIiIsp1fvzxR5QuXdpuWaVKlbBz504AwKuvvopZs2ahX79+KF26NGbOnImqVasCAAoUKIAlS5Zg0KBBuPvuu1GgQAG0adMGH3zwwY199ejRA1euXMHYsWPx7LPPokSJEmjbtq3/niAREREREQUtk1JKBboRRERERERE/mIymTBv3jy0bt060E0hIiIiIqJciDVeiIiIiIiIiIiIiIiIDMLACxERERERERERERERkUFY44WIiIiIiPIUZlsmIiIiIiJf4ogXIiIiIiIiIiIiIiIigzDwQkREREREREREREREZBAGXoiIiIiIiIiIiIiIiAzCwAsREREREREREREREZFBGHghIiIiIiIiIiIiIiIyCAMvREREREREREREREREBmHghYiIiIiIiIiIiIiIyCAMvBARERERERERERERERmEgRciIiIiIiIiIiIiIiKD/B8pgAtD5nnoEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs_range, training_loss, 'r',marker=\"o\",linestyle=\"--\")\n",
    "plt.plot(epochs_range, test_loss, 'b',marker=\"o\",linestyle=\"-\")\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d79330e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.40328726172447205 val_accuracy: 0.8150064945220947\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m y_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_values)\n\u001b[0;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m true_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_values' is not defined"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"val_loss: {test_accuracy[0]}\", f\"val_accuracy: {test_accuracy[1]}\")\n",
    "\n",
    "y_i = len(y_values)\n",
    "i = 0\n",
    "true_values = 0\n",
    "while (i < y_i):\n",
    "    true_values += (1 if (y_test[i][0] == y_prediction[i][0] or y_test[i][1] == y_prediction[i][1]) else 0)\n",
    "    i = i + 1 \n",
    "    \n",
    "print(f\"El algoritmo acerto {true_values} veces sobre los {y_i} casos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03527f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nval_loss: 0.42675861716270447 val_accuracy: 0.7767857313156128\\nval_loss: 0.44326552748680115 val_accuracy: 0.7611607313156128\\nval_loss: 0.4475260078907013 val_accuracy: 0.7566964030265808\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "val_loss: 0.42675861716270447 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.44326552748680115 val_accuracy: 0.7611607313156128\n",
    "val_loss: 0.4475260078907013 val_accuracy: 0.7566964030265808\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f0a73",
   "metadata": {},
   "source": [
    "#### Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9995db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:(2236, 40) y:(2236, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(f\"X:{X.shape} y:{y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b478ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa77694",
   "metadata": {},
   "source": [
    "El bloque realizara el k fold cross validation dividiendolo en 5 folds, se uso los MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4fcbace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_5.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341515e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
