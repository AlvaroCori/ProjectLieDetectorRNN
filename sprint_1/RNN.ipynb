{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2afd9a5",
   "metadata": {},
   "source": [
    "## Antiguo proyecto de Redes Neuronales Recurrentes usando audios .wav, obteniendo su MFCC, aplicandolos a una RNN y validandolo con k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcf75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import soundfile as sf\n",
    "import json\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4830c4a",
   "metadata": {},
   "source": [
    "#### Conseguir los paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfde04d",
   "metadata": {},
   "source": [
    "La función consigue los caminos a los archivos de audio .wav pasándole una dirección del directorio y devolviendo el path de cada audio del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ffb974",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_files_from_path(directory):\n",
    "    path_files = []\n",
    "    dir_list = os.listdir(directory)\n",
    "    for path in dir_list:\n",
    "        path_files.append(directory+\"\\\\\"+path)\n",
    "    return path_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c67b2",
   "metadata": {},
   "source": [
    "Con la lista de paths a los audios se va a recoger las emociones que necesitemos buscándolo por el código de emoción en el nombre del archivo para su distinción.\n",
    "\n",
    "(Es necesario conocer en qué posición está el código en el path, varía según el nivel que se encuentre en el path completo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "364254b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths_for_emotions_keys(emotions_code, files_path, get_code):\n",
    "    paths = []\n",
    "    emotions_set = set(emotions_code)\n",
    "    for code_file in files_path:\n",
    "        if (get_code(code_file) in emotions_set):\n",
    "            paths.append(code_file)\n",
    "    return paths\n",
    "\n",
    "def get_code_crema_d(path):\n",
    "    return path[107:110]\n",
    "\n",
    "def get_code_savee(path):\n",
    "    return path[96]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78353db8",
   "metadata": {},
   "source": [
    "Esta función abre el archivo .wav y obtiene el mfcc escalado en un vector de 40 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe99354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 116\n",
      "-363.19 -414.01 -476.62 -475.43 -476.37 -476.93 -476.12 -475.94 -478.01 -478.31 -476.99 -477.34 -476.64 -476.62 -477.80 -477.81 -478.84 -478.01 -478.52 -476.89 -476.33 -466.54 -435.83 -412.28 -396.43 -366.37 -336.05 -312.91 -292.35 -267.67 -273.43 -277.80 -222.03 -198.07 -199.80 -198.53 -210.20 -215.38 -237.43 -263.72 -211.96 -148.65 -111.44 -102.54 -92.93 -96.31 -117.98 -141.85 -159.07 -183.06 -182.91 -177.43 -167.90 -163.05 -192.20 -234.59 -262.49 -280.98 -226.94 -152.27 -112.53 -104.43 -91.73 -74.93 -87.29 -111.80 -129.17 -147.03 -197.58 -238.17 -217.70 -188.33 -152.94 -90.41 -47.05 -53.63 -85.62 -83.11 -76.11 -76.32 -91.48 -133.28 -176.23 -211.73 -237.58 -218.68 -238.41 -281.33 -323.38 -373.03 -423.87 -464.05 -484.20 -489.87 -486.68 -486.84 -487.36 -488.04 -486.87 -490.70 -493.23 -491.08 -491.00 -489.17 -485.12 -486.27 -489.61 -489.90 -488.60 -480.82 -480.20 -483.12 -469.09 -457.49 -463.71 -440.86 \n",
      "\n",
      "110.95 93.13 38.92 40.37 39.02 38.27 39.44 39.77 37.05 36.72 38.43 38.03 39.00 38.96 37.36 37.37 36.08 37.27 36.40 38.48 39.20 34.44 10.57 -11.03 -21.02 14.35 60.48 81.84 87.79 123.23 178.86 206.04 223.49 216.23 203.56 200.65 196.73 193.27 178.76 163.75 190.17 203.34 192.14 172.16 170.40 190.26 203.65 207.22 206.30 200.71 211.87 214.61 225.21 238.32 227.67 197.64 177.64 171.14 205.70 205.11 183.27 158.92 135.91 148.38 177.25 182.04 177.68 181.84 179.01 161.01 169.93 177.99 190.79 199.37 182.51 158.80 151.66 175.15 191.25 195.99 194.61 172.28 144.76 125.02 108.44 119.53 121.58 111.64 92.32 76.41 66.56 48.38 29.25 21.76 26.00 25.71 24.81 23.54 24.58 19.39 16.33 19.53 19.61 21.47 26.89 25.41 21.43 21.19 23.09 33.49 34.44 29.84 38.52 48.56 50.32 73.91 \n",
      "\n",
      "20.50 35.64 33.77 34.64 33.24 32.64 33.82 34.35 32.13 32.08 33.41 33.21 34.12 33.89 32.47 32.55 31.71 32.93 31.67 33.16 33.74 25.30 15.93 9.23 2.38 27.88 56.34 45.47 13.94 -11.65 5.78 29.86 -2.04 -24.31 -33.68 -34.51 -29.47 -25.62 -16.00 0.83 -10.42 -27.81 -45.58 -62.57 -76.40 -81.51 -71.07 -44.89 -17.71 -4.86 -41.61 -67.61 -64.51 -47.13 -25.98 3.59 11.79 11.74 -5.40 -44.12 -67.13 -70.10 -70.80 -79.50 -84.69 -78.90 -71.71 -57.42 -29.47 -5.65 -3.49 -14.48 -33.26 -54.45 -65.68 -71.86 -75.64 -90.65 -92.07 -95.01 -88.75 -71.57 -44.25 -31.10 -17.57 -7.40 17.45 32.64 28.10 23.99 28.14 34.34 26.91 20.86 24.38 23.87 22.59 20.55 20.83 16.62 13.80 17.36 17.41 17.55 21.95 20.89 18.81 19.01 20.99 29.74 31.03 26.74 23.21 24.69 34.57 40.86 \n",
      "\n",
      "23.53 17.06 26.61 26.82 25.40 25.00 26.14 26.92 25.27 25.51 26.54 26.43 27.22 26.78 25.56 25.75 25.53 26.69 25.02 25.76 26.26 38.53 71.29 90.88 99.02 119.41 131.64 127.95 115.26 72.43 16.91 -11.98 -23.72 -14.39 0.38 3.54 2.97 6.73 13.63 18.45 16.35 8.73 -2.06 -2.13 -7.58 -16.52 -27.21 -32.60 -23.26 -19.09 -41.29 -46.76 -45.41 -47.09 -40.13 -13.33 1.90 11.32 -15.60 -17.33 -1.07 19.51 32.34 17.63 5.53 -0.35 0.50 5.76 20.73 29.31 31.71 28.66 18.99 7.51 2.66 3.15 5.48 -15.70 -18.35 -14.93 -5.14 10.14 39.71 61.88 74.31 76.41 84.40 99.87 95.64 72.36 45.39 31.13 23.49 19.43 21.88 21.05 19.35 16.56 16.61 13.82 10.72 14.58 14.75 13.06 15.76 15.37 15.50 16.15 18.08 24.45 26.30 23.85 21.73 22.02 22.77 20.67 \n",
      "\n",
      "16.52 19.18 19.04 18.78 17.43 17.23 18.24 19.22 17.99 18.39 19.43 19.11 19.75 19.18 18.09 18.42 18.83 19.76 17.82 17.92 18.47 18.00 -4.78 -17.35 -15.29 -0.90 3.18 17.73 35.78 46.79 36.22 4.29 10.55 22.61 32.29 34.73 33.21 27.59 21.36 13.73 -4.78 -24.43 -39.63 -44.05 -44.98 -37.77 -32.23 -40.12 -58.54 -62.55 -39.74 -14.14 2.42 8.68 10.20 -6.37 -13.44 -3.72 -16.73 -12.53 -9.21 -5.81 -10.63 -16.45 -15.26 -12.25 -9.99 -3.41 0.63 4.27 -7.62 -16.60 -22.73 -42.41 -52.60 -50.11 -42.61 -38.58 -29.33 -19.79 -12.78 -9.24 -10.91 -5.36 -0.11 -2.25 -12.56 -8.27 5.06 15.21 25.53 24.61 19.56 17.58 18.78 17.60 15.68 12.63 12.27 10.52 8.22 12.02 12.54 9.73 10.43 10.85 12.57 13.34 14.96 18.76 20.93 19.67 13.77 12.80 11.97 15.42 \n",
      "\n",
      "11.73 13.67 12.47 12.09 10.91 10.91 11.66 12.75 11.68 12.02 13.37 12.57 13.03 12.53 11.43 11.92 12.84 13.28 11.31 11.09 11.83 1.07 4.17 10.53 10.41 12.03 2.58 -13.01 -25.60 -8.13 -7.31 -12.67 -6.11 -6.15 -20.25 -28.35 -25.58 -18.88 -5.13 4.41 -23.31 -31.79 -24.18 -13.67 -14.39 -21.69 -26.73 -24.21 -34.48 -36.63 -48.74 -53.73 -43.68 -28.03 -11.49 7.91 10.11 -4.54 1.54 7.95 -6.48 -19.75 -20.35 -27.90 -37.32 -35.45 -29.72 -32.77 -29.79 6.83 9.29 4.72 -12.36 -28.59 -22.74 -13.30 -3.84 -5.67 -14.42 -24.87 -36.61 -41.33 -44.63 -44.87 -40.96 -40.95 -38.59 -39.42 -32.35 -22.24 -9.17 9.21 15.69 15.44 15.41 13.93 12.16 9.64 9.33 8.45 7.04 10.24 11.25 8.48 7.46 8.67 10.77 11.02 12.05 13.71 15.93 15.20 3.10 -1.73 4.35 13.05 \n",
      "\n",
      "11.27 9.24 7.76 7.54 6.64 6.79 7.20 8.34 7.25 7.28 8.99 7.66 7.94 7.80 6.56 7.16 8.38 8.07 6.34 6.14 7.07 15.55 26.24 24.44 17.57 6.19 -8.77 -26.66 -44.66 -37.89 -29.52 -12.04 -27.08 -38.45 -41.97 -46.31 -40.60 -30.91 -21.14 -19.78 -26.36 -41.06 -39.68 -30.11 -26.54 -29.91 -30.27 -31.33 -33.93 -25.11 -23.55 -28.44 -42.48 -56.08 -47.52 -17.09 8.88 14.76 5.19 -29.66 -57.44 -71.75 -66.95 -65.91 -65.11 -66.74 -65.08 -67.83 -49.80 -23.60 -15.69 -16.34 -24.33 -34.73 -33.41 -28.99 -31.65 -38.57 -44.38 -50.10 -59.76 -66.20 -68.39 -58.93 -53.81 -61.63 -51.37 -36.94 -22.52 -12.31 -7.40 2.67 12.29 13.13 12.09 10.44 9.25 7.96 8.75 8.51 7.22 9.34 10.77 9.00 7.14 8.92 10.19 9.26 9.59 10.01 12.14 12.69 9.41 5.94 6.31 10.30 \n",
      "\n",
      "7.52 9.83 5.05 5.11 4.53 4.80 4.84 6.00 4.89 4.49 6.26 4.65 4.80 5.28 3.82 4.43 5.73 4.55 3.24 3.28 4.24 8.27 0.87 5.07 11.53 0.27 -11.56 -2.93 7.79 1.75 2.91 9.65 1.11 -4.53 -8.95 -6.79 -3.39 -5.25 -10.62 -16.79 -16.66 -3.77 9.87 14.61 19.86 21.61 24.53 21.07 14.37 5.32 9.76 7.10 4.00 -3.41 -7.89 -14.03 -17.20 -12.59 -31.27 -32.30 -24.25 -7.30 7.06 14.61 16.08 18.30 8.55 -6.07 -31.62 -41.42 -43.20 -38.10 -24.35 -0.43 12.93 22.89 25.37 26.15 21.52 21.45 17.46 10.94 5.31 6.79 12.66 14.14 7.10 -1.05 7.55 17.20 15.95 8.14 9.53 10.79 9.12 7.51 7.22 7.46 8.96 8.65 8.20 8.97 10.57 10.04 8.57 10.56 10.34 7.94 7.67 7.85 9.58 10.53 15.19 16.39 9.99 9.16 \n",
      "\n",
      "8.21 8.11 3.98 4.21 3.91 4.30 4.04 5.20 4.25 3.45 4.79 3.32 3.43 4.66 2.99 3.45 4.66 2.73 1.90 2.16 2.95 -8.10 -6.64 -2.49 -3.94 -14.67 -16.93 -2.31 14.18 11.68 12.98 -4.11 -13.06 -22.26 -30.31 -29.44 -24.85 -22.00 -16.75 -11.11 -23.48 -23.79 -19.54 -11.04 -10.30 -12.16 -20.99 -24.24 -14.63 -13.01 -21.08 -27.52 -22.77 -20.72 -20.61 -6.01 -0.68 -6.41 -8.41 -14.58 -18.02 -11.32 -8.37 -4.93 -1.76 0.04 -5.16 -17.34 -20.11 -10.81 -9.19 -10.28 -20.59 -28.56 -21.22 -15.40 -16.01 -8.40 -9.13 -6.50 -0.58 1.64 -3.44 -4.89 -9.26 -10.00 -2.01 3.39 9.30 19.28 16.63 8.54 7.39 8.53 6.70 5.41 6.12 7.67 8.99 8.60 9.10 8.63 10.13 10.44 10.22 12.05 10.49 6.99 6.33 7.03 8.09 7.52 5.05 6.24 6.72 8.59 \n",
      "\n",
      "6.44 6.92 3.94 4.14 4.01 4.51 4.08 5.21 4.64 3.62 4.11 3.19 3.36 5.25 3.42 3.62 4.60 2.34 1.97 2.21 2.67 6.56 4.41 -0.03 -3.07 -18.84 -29.40 -31.04 -28.92 -30.05 -24.00 -28.56 -45.33 -48.52 -36.06 -30.01 -26.95 -26.84 -28.89 -26.09 -26.94 -24.93 -12.28 -7.85 -13.83 -24.70 -29.59 -32.19 -30.43 -23.79 -22.08 -11.77 -14.01 -19.89 -19.63 -17.67 -3.01 4.93 -4.32 -10.33 -6.14 -0.34 3.30 3.10 -4.52 -13.74 -16.86 -21.97 -20.06 -19.08 -21.41 -25.24 -32.16 -20.10 -10.66 -15.36 -26.65 -33.10 -27.44 -20.67 -20.57 -22.17 -33.90 -42.96 -46.61 -44.20 -32.54 -25.91 -29.90 -24.98 -9.74 0.80 5.70 6.43 4.96 4.30 5.85 8.05 9.11 8.99 9.29 7.96 9.21 9.77 10.76 12.15 10.06 6.53 5.69 7.08 7.60 5.92 -0.66 -2.11 2.74 6.99 \n",
      "\n",
      "6.92 7.72 4.36 4.35 4.25 4.84 4.44 5.43 5.35 4.38 3.93 3.71 4.03 6.24 4.36 4.24 4.93 2.90 2.93 2.82 2.98 10.78 18.52 23.06 18.85 5.38 -2.44 -6.89 -17.70 -21.67 -29.10 -19.61 -6.18 4.12 7.05 8.27 12.71 14.47 10.90 5.99 16.61 33.39 36.30 28.30 26.27 31.53 31.90 18.23 -1.53 -16.05 -1.81 16.06 19.65 13.65 7.22 -10.48 -24.00 -24.02 -26.67 -3.38 12.56 21.93 14.56 13.40 23.39 33.25 30.86 25.56 15.21 -5.67 -12.94 -5.60 14.95 32.84 29.42 25.08 33.11 44.24 41.58 30.91 20.28 11.88 5.61 3.43 8.88 6.90 0.46 -0.87 -7.72 -14.65 -8.99 -2.28 4.25 4.58 3.93 4.16 6.25 8.29 8.49 8.26 8.65 6.98 8.00 8.23 9.70 10.62 8.94 6.72 5.83 7.53 7.59 5.87 2.56 0.92 1.55 4.56 \n",
      "\n",
      "5.84 6.62 4.90 4.62 4.38 4.97 4.85 5.59 5.86 5.23 4.14 4.46 4.93 7.01 5.18 4.79 5.17 3.91 4.22 3.52 3.54 -5.79 -11.32 -9.87 -8.89 -15.10 -12.36 -9.79 -8.45 -6.72 -6.49 -2.64 9.36 3.54 1.25 5.11 9.08 9.90 7.67 7.90 7.23 -3.03 -2.00 -2.41 0.27 -4.56 -7.96 -11.55 -19.77 -19.43 -7.65 5.71 5.25 2.68 -3.92 -12.83 -15.06 -18.07 -2.58 -1.58 5.24 3.35 0.80 -4.86 -13.59 -22.81 -23.39 -18.76 -8.79 -3.90 -5.54 -2.32 4.32 -1.25 7.39 6.42 -3.01 -5.33 -5.42 -18.64 -25.54 -18.31 -18.37 -20.25 -13.30 -15.84 -18.29 -14.96 -7.66 -3.69 -0.79 0.61 2.92 3.02 3.54 4.86 7.08 8.37 7.00 6.74 7.61 6.06 6.86 6.55 7.61 8.24 7.54 7.57 6.68 8.06 7.64 5.33 -1.30 -2.90 -1.87 2.91 \n",
      "\n",
      "4.88 5.43 5.42 4.90 4.39 4.87 5.24 5.64 5.96 5.89 4.63 5.19 5.72 7.24 5.56 5.06 5.11 4.90 5.26 4.01 4.09 4.02 3.58 4.14 5.25 -1.23 -3.19 -8.91 -5.21 -4.58 -5.39 -1.57 -11.07 -19.76 -24.76 -32.25 -36.42 -33.98 -30.39 -29.63 -22.62 -18.46 -13.18 -19.44 -21.26 -17.30 -1.67 15.83 13.65 9.79 23.49 19.13 5.58 6.66 6.01 -4.40 0.55 -1.99 1.39 5.35 0.14 -13.67 -19.80 -21.38 -16.83 -17.14 -17.83 -17.98 -27.34 -27.85 -29.12 -34.76 -33.53 -20.49 -17.25 -23.09 -25.63 -32.45 -36.91 -34.84 -24.01 -12.53 -9.92 -13.15 -16.14 -20.30 -22.84 -20.49 -19.38 -19.79 -12.75 -1.03 1.69 1.79 3.67 6.15 8.16 8.49 6.37 6.50 6.88 5.66 6.13 5.49 5.72 6.35 6.48 8.79 7.98 8.59 7.85 5.34 -3.74 -6.86 -3.71 3.41 \n",
      "\n",
      "4.70 5.64 5.88 5.19 4.31 4.58 5.56 5.64 5.68 6.26 5.28 5.79 6.22 6.90 5.44 5.03 4.79 5.52 5.65 4.17 4.38 14.59 23.67 18.03 10.54 2.87 -3.29 -4.86 0.84 -2.37 -8.08 -2.35 -6.24 -8.23 -7.29 1.38 4.97 1.05 -6.60 -13.19 1.67 7.14 4.04 -7.54 -7.43 -5.03 -3.58 2.54 9.16 5.50 7.83 -0.46 -8.60 -11.03 -4.60 4.86 -0.15 -4.55 -5.07 -7.34 -9.11 0.50 3.61 8.93 12.49 21.46 25.57 26.12 16.97 6.75 4.86 -3.48 1.86 0.62 5.41 3.72 2.04 0.40 -1.76 4.15 14.23 12.91 11.92 17.15 10.71 2.80 -6.18 -10.40 -10.19 -9.44 -9.61 -7.12 0.65 0.88 4.15 7.72 9.28 8.87 6.95 6.88 6.97 6.06 5.93 5.33 5.22 6.08 6.26 9.89 9.31 9.15 8.29 7.00 4.81 2.33 1.23 4.24 \n",
      "\n",
      "4.28 5.13 6.25 5.39 4.17 4.18 5.68 5.60 5.19 6.33 5.88 6.21 6.44 6.19 4.99 4.87 4.39 5.61 5.23 3.98 4.27 -1.63 -9.97 -7.57 -4.68 -6.64 -6.53 -6.96 -3.86 0.84 -1.85 -1.33 3.96 2.33 11.05 24.74 27.40 19.43 13.70 13.70 10.34 0.83 -4.05 -6.77 -7.78 -11.87 -19.43 -21.82 -6.59 3.23 -11.12 -19.34 -16.04 -11.29 -7.01 4.98 4.49 2.87 0.43 -6.84 -0.15 6.80 1.74 0.36 -3.17 -6.83 -5.24 0.91 6.05 13.00 12.41 11.74 6.34 -2.85 -10.68 -13.51 -8.84 -7.96 -5.00 0.97 -6.31 -16.72 -15.05 -5.99 -11.16 -7.96 -1.07 -4.91 -8.72 -3.12 -5.52 -6.63 -0.06 0.32 4.79 9.23 10.27 9.55 7.88 7.28 7.89 7.09 6.13 6.03 6.61 7.69 6.93 10.44 10.33 9.78 8.71 8.05 8.16 9.25 4.99 3.11 \n",
      "\n",
      "4.01 4.70 6.49 5.40 3.95 3.71 5.48 5.44 4.66 6.16 6.19 6.45 6.45 5.43 4.46 4.71 4.10 5.19 4.18 3.55 3.78 1.10 3.54 5.11 6.51 1.58 6.63 5.09 -1.95 -1.55 -3.20 -4.62 -0.90 2.64 5.95 -3.18 -7.92 -2.58 2.45 -0.63 -7.23 -5.12 -3.12 -4.57 -1.67 0.01 -0.34 3.23 9.20 16.32 11.99 4.08 -0.05 -4.90 -11.08 -16.62 -11.69 -7.82 -15.60 -6.94 -4.70 -16.41 -23.75 -21.62 -25.92 -26.26 -26.04 -22.91 -13.65 -0.74 -2.73 -0.50 -11.47 -18.10 -15.07 -14.26 -15.46 -18.73 -17.93 -15.52 -17.65 -20.56 -10.62 -4.40 -15.09 -12.89 -2.27 2.25 -0.21 1.67 -1.34 0.38 -0.32 0.06 5.40 10.38 10.93 10.34 9.35 8.81 9.10 8.19 6.49 7.44 9.36 10.38 8.03 10.28 10.84 10.40 9.18 7.88 3.88 6.31 3.75 1.97 \n",
      "\n",
      "3.50 4.92 6.51 5.14 3.69 3.28 4.89 5.05 4.22 5.80 6.10 6.49 6.38 4.89 4.06 4.61 4.00 4.46 2.94 3.07 3.11 13.36 14.00 9.31 7.89 -2.09 -5.28 -2.14 1.63 -1.22 -4.21 -5.00 -5.87 -0.18 5.12 3.96 4.51 6.64 0.73 -3.77 -0.17 2.56 7.76 7.21 11.09 10.79 6.71 5.14 5.41 6.22 4.79 8.30 7.82 1.18 -4.01 -12.54 -18.20 -20.03 -9.95 3.45 2.42 -3.66 0.18 3.72 6.58 11.16 9.33 6.38 -2.49 -2.15 -0.05 3.02 7.51 13.86 10.57 5.26 5.44 10.24 14.32 12.14 8.59 11.50 8.89 6.02 -4.52 -6.79 -6.75 2.13 4.00 8.03 10.12 2.01 -0.14 0.10 5.84 10.95 11.09 10.85 10.81 10.02 9.76 8.64 6.73 9.13 12.18 12.72 8.81 9.55 10.83 10.79 9.65 8.40 7.46 7.46 6.10 2.84 \n",
      "\n",
      "3.85 4.52 6.25 4.70 3.51 2.98 4.04 4.40 3.86 5.27 5.63 6.31 6.29 4.71 3.91 4.54 4.07 3.65 2.01 2.70 2.55 -0.15 -3.82 -0.10 -2.40 -13.37 -19.36 -13.45 -4.53 -0.86 -1.02 1.32 4.58 7.36 0.51 -4.16 -2.85 -3.38 -0.04 1.97 -6.31 -5.77 -6.04 -1.87 -2.42 -6.55 -12.31 -13.43 -15.78 -18.76 -15.73 -9.74 -2.10 9.06 12.67 13.04 6.81 -0.97 6.82 0.01 -6.59 -7.03 3.45 7.29 3.52 -2.32 -3.40 -0.97 -0.32 -9.09 -5.97 0.18 5.61 5.15 6.20 10.39 10.47 10.35 8.52 2.05 -4.58 -11.89 -26.06 -26.37 -23.37 -24.12 -19.21 -11.08 -10.42 -6.65 -4.56 -1.80 0.37 0.36 6.01 10.80 10.63 10.70 10.57 9.24 9.21 7.92 6.61 10.53 13.58 13.27 8.61 8.48 10.37 10.61 9.53 8.77 13.17 12.05 11.61 3.64 \n",
      "\n",
      "3.19 4.28 5.74 4.26 3.57 2.90 3.20 3.59 3.54 4.65 4.92 5.87 6.12 4.86 4.00 4.42 4.20 2.98 1.70 2.55 2.33 -2.75 0.17 0.99 2.84 -7.59 -7.66 -5.82 -3.79 -5.04 -0.13 5.25 6.29 0.92 -1.79 -2.08 -6.66 -3.42 2.08 -0.02 -16.94 -12.45 -6.44 -5.22 -3.45 -3.22 -2.69 -2.53 -10.27 -8.68 -7.43 -2.97 -9.61 -8.30 -5.32 -3.78 3.19 8.64 -1.09 -19.01 -16.14 -5.80 4.61 0.55 -5.81 -11.98 -9.41 -7.30 -0.49 0.37 0.93 1.75 -6.09 -13.97 -10.31 -4.18 -0.12 -4.01 -5.34 -13.99 -16.96 -21.56 -23.59 -17.69 -14.73 -17.07 -18.00 -19.85 -18.41 -11.79 -11.73 -0.20 1.03 0.80 5.85 9.93 9.53 9.69 8.73 7.78 7.30 6.01 5.91 11.04 12.64 11.39 7.16 7.20 9.46 9.60 8.52 7.29 10.10 10.85 11.14 3.33 \n",
      "\n",
      "3.47 4.43 5.01 4.02 3.95 3.04 2.67 2.84 3.21 4.00 4.17 5.17 5.77 5.19 4.21 4.19 4.25 2.55 1.99 2.58 2.47 9.66 7.08 10.10 12.76 3.44 2.18 -1.30 -1.56 0.38 2.56 2.24 -4.93 -7.99 -5.15 -1.92 -7.58 -3.90 -1.98 -2.88 -7.36 -8.29 -9.25 -9.75 -12.94 -4.22 8.35 12.45 7.78 3.99 3.81 6.63 1.82 -2.14 -2.85 1.83 0.70 -1.13 -6.21 -7.11 -7.87 -7.89 -9.74 -11.12 -5.40 4.61 5.95 1.11 -5.89 1.07 2.57 -1.44 -6.26 -4.06 -2.93 0.47 -0.71 -2.67 -8.64 -8.24 -3.80 0.06 5.03 10.11 5.84 4.97 -2.11 -0.49 2.11 3.30 4.03 3.82 1.60 1.33 5.39 8.45 7.85 7.88 6.36 5.87 4.53 3.37 4.56 10.12 9.42 7.48 4.76 5.65 8.03 7.68 6.72 5.42 7.64 8.79 7.08 3.62 \n",
      "\n",
      "3.07 3.82 4.16 4.03 4.56 3.30 2.59 2.36 2.84 3.42 3.56 4.28 5.11 5.45 4.42 3.83 4.13 2.31 2.56 2.66 2.76 2.02 4.51 4.09 1.48 -13.53 -18.45 -10.49 -4.42 -1.82 0.49 -0.81 -3.40 -1.03 -3.50 -4.85 -4.35 -6.21 -4.72 -3.35 0.33 0.59 3.28 6.68 3.75 -5.86 -16.19 -13.08 -13.58 -10.70 -12.62 -6.71 0.81 -0.46 -1.83 4.42 3.85 -1.62 -5.89 1.97 1.85 2.39 4.22 8.67 5.66 -3.27 -7.68 -7.63 -2.39 4.20 9.65 6.90 6.07 4.36 7.19 11.06 10.05 -1.44 -6.40 -3.25 -0.31 -5.34 -11.10 -15.83 -18.05 -13.40 -10.50 -9.60 -3.66 -0.72 -2.06 -0.09 1.91 1.88 4.69 6.54 5.79 5.58 3.48 2.87 1.83 0.79 2.67 7.61 4.92 2.80 2.06 3.67 6.05 5.04 4.13 3.89 7.50 6.68 5.09 4.24 \n",
      "\n",
      "2.40 3.28 3.27 4.16 5.11 3.49 2.87 2.27 2.50 2.94 3.19 3.32 4.17 5.45 4.51 3.38 3.81 2.16 3.01 2.61 2.91 -3.35 -0.41 -1.63 -3.33 -14.34 -12.74 -8.12 -5.97 -1.99 0.08 0.72 7.05 4.01 -0.50 -5.96 -4.69 -2.86 2.12 4.82 8.36 7.20 3.33 1.80 4.67 9.07 11.61 -3.97 -12.36 -5.17 -4.84 0.25 3.47 3.34 4.10 -1.34 3.31 5.28 4.62 1.08 2.69 3.04 1.60 3.66 5.79 6.08 0.01 -5.99 -10.07 -5.44 1.94 7.00 7.07 3.20 -6.86 -9.66 -8.54 -8.42 -3.90 0.17 -5.06 -12.95 -13.49 -15.42 -16.76 -18.38 -11.11 -9.17 -7.89 -7.22 -10.56 -5.51 1.89 2.37 3.86 4.46 3.59 3.22 0.81 0.81 0.12 -0.97 0.58 4.17 0.67 -1.10 -0.17 1.28 3.65 2.14 1.02 1.16 -1.05 -2.56 0.77 3.94 \n",
      "\n",
      "2.74 3.32 2.41 4.18 5.29 3.41 3.23 2.55 2.27 2.57 3.08 2.46 3.12 5.06 4.42 2.90 3.35 2.01 3.08 2.32 2.72 6.87 5.18 9.38 10.83 6.27 8.72 7.45 6.72 5.05 3.27 0.11 1.97 3.19 0.72 -3.17 -0.94 0.67 -4.45 -8.05 -3.08 -3.12 -7.85 -6.38 -2.01 2.58 8.00 5.50 8.32 8.51 8.74 8.33 6.75 1.57 1.60 5.63 7.19 5.59 11.62 2.39 1.67 0.09 -0.99 -1.01 -0.84 0.47 -1.25 0.78 -1.89 -6.18 -3.62 -2.34 0.42 -0.72 -2.11 -4.25 -4.79 -6.33 -3.15 2.33 1.35 0.91 3.83 1.03 6.06 -5.32 -9.01 -4.90 2.22 -0.15 -2.88 -3.16 1.57 2.76 3.04 2.45 1.55 1.19 -0.31 0.56 -0.14 -1.57 -1.16 0.96 -2.03 -3.13 -1.42 -1.19 1.19 -0.45 -1.75 -1.92 -10.07 -10.72 -5.76 3.55 \n",
      "\n",
      "2.38 3.15 1.67 3.89 4.88 2.98 3.35 2.99 2.25 2.29 3.13 1.87 2.25 4.34 4.14 2.46 2.81 1.85 2.79 1.80 2.17 4.44 6.50 0.84 1.39 -3.96 -3.96 -3.13 -3.07 -4.77 -5.88 -7.23 -10.89 -5.34 -8.84 -6.01 -2.04 -5.10 -8.12 -7.07 -4.28 -3.76 -5.99 -7.15 -7.84 -9.94 -15.08 -11.87 -2.49 -3.28 -3.64 -3.86 -1.07 -7.81 -11.51 -8.73 -3.39 0.18 -0.07 5.73 6.27 6.11 2.01 -1.36 -4.84 -9.24 -9.27 0.48 3.65 -11.20 -11.53 -7.79 -2.65 0.35 -1.14 -3.04 -2.31 0.05 4.09 -1.09 -9.23 -11.14 -5.90 -6.10 -3.00 -17.37 -23.34 -22.06 -13.82 -4.36 -1.88 -0.79 1.11 3.00 2.35 0.76 -0.12 -0.23 -0.27 0.55 0.88 -1.14 -2.12 -0.93 -2.60 -3.10 -1.63 -3.21 -0.82 -2.21 -3.59 -3.18 -7.15 -7.00 -6.17 3.32 \n",
      "\n",
      "2.51 3.12 1.16 3.27 3.90 2.25 3.06 3.38 2.48 2.06 3.18 1.65 1.83 3.43 3.72 2.07 2.30 1.72 2.39 1.20 1.47 -2.41 1.33 1.69 0.62 -2.68 -4.27 -7.75 -8.81 -10.30 -11.75 -11.76 -9.80 -10.70 -12.56 -5.96 -3.11 -0.80 3.04 6.41 5.32 -2.12 -6.95 -9.07 -11.25 -8.77 -5.54 -10.69 -11.27 -17.51 -12.53 -13.00 -7.01 -0.82 0.45 -3.55 -1.84 -3.89 -0.01 5.87 3.91 -0.98 -9.22 -12.88 -15.62 -12.79 -8.32 -3.19 -2.74 -11.20 -13.45 -6.33 -5.92 -12.09 -9.51 -8.35 -4.50 -1.71 -1.35 -7.08 -12.40 -12.87 -6.48 1.47 10.87 -1.85 -9.63 -10.95 -8.17 -6.47 -7.31 -3.48 0.69 3.10 1.88 -0.45 -1.22 -0.94 0.09 1.18 2.49 -0.22 -2.11 -0.96 -1.40 -1.65 -1.11 -4.31 -1.96 -2.83 -4.36 -3.45 -1.58 -0.33 -1.53 2.89 \n",
      "\n",
      "2.79 3.39 0.96 2.51 2.63 1.39 2.41 3.54 2.92 1.91 3.04 1.79 1.93 2.54 3.23 1.74 1.91 1.71 2.17 0.71 0.91 4.34 2.45 0.81 -0.84 -2.54 -3.54 -8.58 -5.88 -6.76 -6.76 -6.00 -7.79 -7.40 1.50 5.62 5.19 2.42 -3.93 -7.78 -0.23 -5.02 -2.57 0.62 0.81 -1.50 -9.84 -4.89 -6.37 -12.43 -14.48 -12.83 -9.08 -8.76 -6.21 -4.10 -4.97 -6.48 -2.26 -2.52 -7.60 -9.51 -11.64 -9.93 -8.37 -3.81 -4.71 -3.40 -3.37 -3.92 -8.97 -10.42 -6.83 -6.68 -5.82 -8.97 -10.44 -3.78 -4.08 -5.32 -1.36 4.00 4.97 15.20 30.14 23.01 4.79 -2.46 -2.92 -5.71 -7.95 -2.10 0.50 3.07 1.70 -1.09 -1.68 -0.94 0.98 2.74 3.83 0.60 -1.38 0.46 0.54 0.10 -0.34 -4.30 -2.08 -2.31 -3.82 -3.75 -0.66 1.57 1.08 3.51 \n",
      "\n",
      "2.72 3.30 1.10 1.91 1.47 0.61 1.66 3.42 3.45 1.91 2.60 2.17 2.43 1.83 2.73 1.48 1.73 1.87 2.27 0.48 0.72 6.30 5.70 1.26 0.62 -0.94 -2.59 -7.52 -8.38 -4.39 -4.29 0.16 -1.68 0.06 -7.25 -12.04 -8.46 -11.06 -10.61 -9.96 3.31 5.01 7.50 7.73 3.60 0.23 -3.97 -0.56 -5.41 -8.98 -9.34 -6.83 -3.65 -9.04 -9.77 -9.13 -9.25 -4.78 -7.30 -6.57 -5.86 0.45 4.87 6.40 4.68 2.69 0.64 -7.20 -14.00 -15.97 -16.48 -18.29 -6.44 3.59 8.93 5.23 0.64 -2.25 -7.30 -11.18 -12.73 -6.89 6.82 24.75 34.51 36.55 21.94 2.44 -4.05 -4.97 -0.46 4.14 0.64 2.94 1.79 -1.17 -1.52 -0.32 2.04 3.26 4.33 0.95 -0.43 2.07 2.06 1.21 0.30 -3.40 -1.38 -0.94 -2.13 -2.94 3.29 5.33 2.49 4.24 \n",
      "\n",
      "2.37 3.09 1.50 1.70 0.79 0.08 1.13 3.13 3.91 2.17 1.95 2.62 3.01 1.40 2.28 1.33 1.80 2.19 2.63 0.60 0.99 0.13 4.48 6.61 8.35 8.31 4.83 5.52 1.78 -3.93 -3.55 -4.14 0.14 -0.88 -3.76 -1.99 5.19 6.27 0.80 -2.11 -0.58 4.75 3.53 0.95 -3.71 -6.93 -3.85 -0.69 -5.98 -3.11 -1.13 0.07 1.15 -2.80 -4.35 -3.47 -3.12 -4.14 -0.93 -1.08 -1.88 -0.03 2.28 1.25 -1.30 -4.12 -5.76 -7.96 -6.45 -6.38 -1.50 -2.37 1.41 2.90 1.66 1.42 1.71 -3.25 -6.69 -7.00 -2.86 8.33 23.62 32.74 28.83 32.14 23.43 1.10 -0.78 -0.72 3.49 5.71 1.12 2.77 2.09 -0.79 -0.86 0.77 2.47 2.97 3.93 0.86 0.26 2.68 2.39 1.29 0.69 -2.00 -0.22 0.81 -0.10 -1.32 6.85 7.96 2.78 1.73 \n",
      "\n",
      "2.27 2.92 1.99 1.93 0.74 -0.08 1.07 2.85 4.17 2.74 1.39 3.01 3.36 1.21 1.90 1.35 2.16 2.60 3.03 1.05 1.66 3.50 0.62 -4.11 -4.48 -3.32 -4.19 -2.77 -7.50 -12.71 -12.61 -12.59 -13.81 -11.07 -2.34 -3.58 -4.94 -4.54 -6.20 -3.38 -8.10 -9.28 -10.74 -10.63 -13.77 -8.25 -0.85 -3.11 -2.67 5.43 7.42 -0.90 -0.84 -2.79 -7.13 -13.49 -15.90 -14.90 -12.64 -8.37 -10.59 -14.57 -10.24 -10.98 -10.81 -9.65 -15.60 -13.31 -9.96 -6.43 -7.12 -8.08 -16.05 -8.65 -4.40 -5.64 -10.96 -13.47 -13.94 -11.66 -1.53 14.14 29.95 34.02 14.49 23.12 24.43 7.98 -1.99 -2.05 1.90 3.88 1.84 2.61 2.49 -0.08 0.13 2.13 2.82 3.33 3.08 0.71 0.50 1.91 1.47 0.63 0.98 -0.46 1.03 2.47 1.73 -0.60 0.18 -0.37 -2.23 -2.35 \n",
      "\n",
      "2.04 2.70 2.40 2.46 1.28 0.14 1.51 2.72 4.17 3.59 1.30 3.25 3.30 1.20 1.61 1.61 2.76 2.99 3.30 1.75 2.53 7.61 6.86 6.28 4.94 2.26 -1.72 -1.92 -4.96 -12.43 -15.06 -13.55 -11.92 -5.27 -7.97 -7.90 -7.72 -9.92 -10.94 -9.74 -12.70 -9.39 -6.31 -7.18 -10.18 -4.35 -2.46 -5.28 7.57 17.55 14.09 -3.66 -7.27 -7.28 -7.93 -11.64 -16.01 -16.41 -16.05 -14.02 -10.57 -7.04 -0.06 1.43 -0.90 -10.10 -5.70 -2.62 -0.15 -0.25 -3.93 -8.39 -17.69 -14.95 -6.14 -2.42 -4.41 -7.51 -8.18 -2.82 3.38 13.01 27.63 20.78 5.08 18.25 32.84 28.11 10.32 1.02 7.52 5.12 2.64 2.48 2.86 0.76 1.21 3.47 3.52 3.34 2.40 0.92 0.47 0.19 -0.13 -0.06 1.41 1.06 2.15 3.64 3.16 -0.07 -6.87 -9.09 -7.35 -2.77 \n",
      "\n",
      "1.99 2.83 2.59 2.99 2.19 0.69 2.30 2.80 3.91 4.53 1.95 3.38 2.90 1.28 1.44 2.11 3.46 3.27 3.37 2.54 3.36 2.19 5.15 4.24 2.13 1.44 -0.80 -0.09 0.93 0.47 -1.01 -8.04 -3.34 4.96 4.33 4.10 0.24 -2.77 -5.94 -6.96 -11.57 -5.19 -5.68 -5.61 -7.55 -7.15 -9.33 -4.83 16.37 32.30 27.63 9.67 -2.43 -2.53 -2.59 -1.84 -2.61 -0.90 -4.80 -10.53 -9.33 -1.81 6.31 0.62 -7.47 -4.99 7.08 15.27 22.03 12.29 4.72 5.45 -0.99 -8.61 -10.02 -7.98 -9.16 -5.50 -0.55 2.22 8.01 21.56 19.74 0.39 -10.93 -6.37 10.03 18.67 6.49 0.52 6.86 6.00 3.34 2.41 3.11 1.55 2.12 4.45 3.95 3.02 2.35 1.69 0.54 -1.43 -1.56 -0.12 2.13 2.50 3.08 4.19 3.84 1.50 -2.67 -4.63 -4.36 0.08 \n",
      "\n",
      "2.01 2.77 2.59 3.26 3.19 1.47 3.16 3.00 3.45 5.31 3.29 3.50 2.41 1.43 1.42 2.75 4.07 3.40 3.31 3.24 3.92 2.06 1.89 2.26 6.35 10.36 1.43 -5.32 -5.22 -0.27 -2.52 -4.05 -0.63 1.75 2.14 -0.05 -8.85 -10.20 -7.24 -1.30 -2.41 -3.61 -1.63 3.52 1.42 -5.43 -14.31 -1.58 23.63 35.38 28.39 11.14 -4.03 4.72 7.13 3.59 1.62 1.98 -13.99 -14.51 -5.38 -0.31 2.06 -10.45 -14.15 -5.61 16.75 29.11 28.59 21.79 4.67 0.33 -5.66 -13.14 -10.73 -7.61 -6.96 -2.23 -2.68 -3.76 8.40 26.75 14.92 4.21 5.05 3.41 11.70 16.36 6.31 -2.90 -1.71 3.22 3.76 2.41 3.14 2.12 2.65 4.73 3.97 3.79 2.93 2.85 0.97 -1.93 -2.11 0.69 3.08 3.77 3.78 4.16 3.77 3.00 3.51 2.76 2.14 1.88 \n",
      "\n",
      "1.39 2.22 2.53 3.18 4.02 2.33 3.84 3.18 2.87 5.70 4.92 3.70 2.11 1.66 1.57 3.38 4.36 3.40 3.21 3.69 4.08 6.13 7.20 4.08 4.97 10.78 1.81 -9.23 -12.94 -6.16 -4.60 -3.59 -0.89 -2.64 -3.82 -0.98 0.24 0.69 1.67 0.28 -2.29 -1.84 -1.06 0.49 -1.88 -7.24 -6.91 11.66 24.43 24.63 17.52 10.77 -8.88 -5.66 1.13 -2.75 -5.08 -8.73 -14.76 -11.86 -4.97 -4.78 -5.84 -11.53 -6.37 10.86 36.75 44.80 42.18 39.02 17.92 2.63 0.80 -8.29 -6.67 -0.94 -3.95 -1.33 -4.94 0.50 17.75 26.02 4.58 1.56 7.97 8.03 12.24 19.64 11.23 0.41 0.70 1.66 3.84 2.44 2.92 2.37 2.66 4.13 3.84 4.41 3.70 3.92 1.71 -1.14 -1.54 2.10 4.00 4.63 4.16 3.77 3.56 3.78 4.34 4.47 5.88 2.21 \n",
      "\n",
      "1.59 2.42 2.66 2.81 4.54 3.13 4.19 3.19 2.26 5.57 6.23 4.03 2.18 2.00 1.90 3.78 4.22 3.35 3.11 3.80 3.80 1.93 2.14 0.35 -1.36 4.82 3.70 -3.15 -10.90 -6.97 -1.46 -4.88 -8.14 -3.68 -0.60 -2.90 -5.31 -6.28 -2.33 1.27 -5.00 -8.84 -7.13 -6.86 -11.67 -8.03 14.12 30.72 26.67 15.92 21.81 24.29 0.00 -5.64 -5.35 -8.73 -13.61 -11.13 -3.80 -5.86 -4.60 -9.64 -13.64 -11.37 5.07 29.47 42.56 35.25 29.25 35.93 19.90 1.17 -2.57 -6.83 -5.84 -1.88 -6.46 -7.70 -8.67 -1.74 13.32 11.87 -5.14 -1.83 0.59 5.03 8.69 9.67 7.83 2.29 3.65 3.57 3.58 2.50 2.47 2.26 2.11 2.71 3.04 3.86 4.04 4.32 2.44 0.38 -0.14 3.45 4.55 4.75 4.08 3.27 3.50 4.80 7.11 9.66 9.64 3.07 \n",
      "\n",
      "2.06 2.94 3.13 2.36 4.69 3.77 4.22 3.00 1.75 5.02 6.76 4.48 2.62 2.49 2.38 3.81 3.64 3.33 2.96 3.57 3.19 0.35 3.52 7.32 9.02 15.41 15.95 10.72 -1.85 -10.29 -6.06 -5.99 -7.91 -10.02 -7.56 -8.37 -15.24 -16.33 -15.65 -8.59 -7.33 -9.11 -8.86 -8.58 -13.00 -4.00 17.66 34.37 31.97 12.54 15.59 23.04 4.30 -5.69 -6.77 -1.80 -4.64 -3.36 -1.56 -1.55 -4.62 -11.06 -17.35 -3.52 18.67 36.87 32.80 16.00 8.57 26.15 29.74 11.20 -1.14 -6.26 -7.10 -0.29 -7.40 -9.13 -5.47 8.99 19.29 10.96 6.16 0.55 -0.77 2.20 3.42 6.31 11.52 3.98 1.33 5.31 3.10 2.55 1.87 1.83 1.13 0.80 1.45 3.48 3.52 3.74 2.78 1.84 1.48 4.14 4.52 4.00 3.46 2.90 3.62 5.74 10.24 14.69 12.19 4.18 \n",
      "\n",
      "1.46 2.05 3.96 2.05 4.54 4.25 4.06 2.71 1.49 4.27 6.35 4.97 3.27 3.08 2.94 3.48 2.81 3.38 2.69 3.16 2.49 4.11 3.46 -0.28 1.68 12.55 20.40 21.00 14.39 6.63 3.60 -2.50 -5.60 -5.70 -10.61 -11.07 -13.09 -9.74 -7.47 -2.83 0.74 -0.56 -7.33 -10.74 -10.03 7.17 31.93 34.35 20.85 1.90 6.80 19.73 13.22 -3.90 -8.44 -2.76 1.34 4.86 1.99 1.42 -3.12 -5.47 -10.30 6.95 27.34 33.88 13.04 0.90 5.87 16.22 33.75 25.72 13.17 6.06 2.03 5.13 0.20 -8.45 -6.02 12.63 15.88 3.39 -1.95 -17.63 -17.16 -13.98 -9.19 0.32 5.79 3.43 5.75 5.06 2.55 2.58 1.20 1.16 -0.06 -1.11 -0.07 2.86 2.16 2.31 2.59 2.65 2.71 3.89 3.92 2.56 2.39 2.74 4.12 5.53 4.77 6.34 7.93 4.03 \n",
      "\n",
      "0.29 0.73 4.95 2.04 4.22 4.64 3.87 2.51 1.63 3.64 5.29 5.38 3.95 3.67 3.48 2.92 2.04 3.55 2.33 2.80 1.97 2.15 -0.45 -0.25 2.52 12.02 23.62 33.88 34.64 22.79 13.32 6.99 11.91 21.76 12.93 3.54 -4.93 -8.63 -11.27 -7.86 -1.75 -5.10 -6.46 -10.15 -1.48 20.00 36.85 29.65 10.68 1.76 6.16 18.34 19.87 4.17 -3.07 -2.98 -1.09 2.99 1.52 5.96 5.52 -0.20 -3.95 17.96 31.49 22.92 -3.23 -6.04 -2.82 -5.67 14.85 13.22 7.33 2.77 -1.07 3.05 4.22 -4.17 2.64 16.81 11.94 -5.56 -9.65 -11.45 -1.20 -5.42 -5.53 2.83 11.53 5.82 8.54 4.26 2.13 2.57 0.58 0.37 -1.17 -2.52 -1.14 1.23 0.45 0.54 2.02 2.73 3.21 2.87 2.97 0.98 1.14 2.77 4.92 5.10 -2.14 -4.40 1.62 2.30 \n",
      "\n",
      "0.41 0.72 5.79 2.36 3.92 5.03 3.81 2.57 2.27 3.36 4.10 5.61 4.48 4.15 3.90 2.40 1.67 3.83 2.00 2.73 1.89 1.28 5.23 3.47 1.06 8.32 23.15 37.09 44.32 35.98 17.68 17.15 36.21 33.86 19.08 5.27 -4.17 -9.47 -14.90 -16.34 -3.20 -1.16 -0.97 -0.85 20.49 37.36 36.36 18.37 5.26 7.66 1.54 12.40 20.25 7.65 1.41 -2.55 -5.10 -7.48 -0.04 10.72 18.31 12.93 13.51 31.95 31.98 11.89 -5.30 2.41 7.44 -0.73 13.01 26.40 17.51 5.83 -2.81 -5.97 0.33 -4.49 8.94 21.97 11.27 -5.31 -5.91 -1.22 9.37 6.33 0.59 8.14 14.80 7.48 3.18 4.15 1.98 2.52 0.08 -0.40 -1.97 -3.13 -1.83 0.38 -0.88 -0.87 1.40 2.49 3.02 1.59 2.01 -0.13 0.06 2.87 5.45 5.43 -0.32 -2.79 0.56 0.68 \n",
      "\n",
      "0.99 1.61 6.16 2.96 3.81 5.49 3.98 2.98 3.39 3.49 3.24 5.54 4.76 4.40 4.13 2.21 1.92 4.16 1.90 3.06 2.35 4.44 3.08 5.11 6.99 11.32 19.47 27.10 43.87 42.78 26.53 25.26 33.18 32.36 34.89 34.28 27.09 13.83 4.87 -2.81 10.80 12.81 11.63 15.25 38.40 46.25 31.27 6.38 -2.53 2.15 -2.24 5.94 17.40 9.68 -4.16 -11.38 -12.68 -13.12 4.74 18.56 31.44 30.88 35.84 42.24 27.66 5.56 5.05 12.42 14.16 17.47 16.09 34.44 26.23 11.37 1.44 -8.19 -6.50 -3.31 10.58 17.50 5.01 -3.64 -5.72 -0.18 2.56 5.64 4.51 2.17 5.56 3.74 1.40 3.81 2.16 2.44 -0.23 -1.01 -2.32 -2.93 -1.84 0.55 -1.21 -1.39 1.00 2.33 2.47 0.60 1.30 -0.43 -0.60 2.90 5.54 5.67 3.09 0.75 1.53 1.37 \n",
      "\n",
      "1.44 2.35 5.88 3.71 3.98 5.95 4.35 3.66 4.80 3.91 2.89 5.13 4.78 4.39 4.15 2.51 2.82 4.48 2.09 3.70 3.27 4.58 3.16 -1.05 -0.60 1.90 5.47 10.94 25.90 32.82 31.90 31.56 29.41 36.21 51.77 53.89 46.87 33.58 21.31 3.10 6.65 11.93 19.33 29.63 45.31 32.92 7.40 -6.64 -3.80 -2.88 -1.29 3.34 11.72 7.67 -5.77 -11.07 -6.70 -7.75 12.37 34.23 43.22 46.57 53.05 42.22 19.73 6.92 10.26 6.81 0.40 4.32 -2.50 15.86 22.95 12.33 4.85 -7.60 -10.40 -3.88 12.75 14.67 1.94 -2.67 0.38 5.24 -3.41 -2.57 2.66 3.98 6.78 4.22 2.09 1.78 2.60 2.36 -0.34 -1.37 -2.20 -2.22 -1.26 0.33 -0.36 -0.89 0.87 2.27 1.99 0.22 0.97 0.04 -0.77 2.75 5.29 5.33 2.38 -0.58 1.50 3.10 \n",
      "\n",
      "40\n",
      "-309.92 110.71 -6.24 20.42 1.60 -6.61 -16.28 3.71 -3.85 -10.44 7.73 -1.65 -5.88 3.42 1.09 -1.27 4.44 0.44 -1.46 1.57 0.05 -0.41 1.12 -2.70 -3.28 -1.66 -0.37 1.63 -2.05 -0.80 1.55 2.40 3.41 2.82 3.18 3.53 4.41 6.43 8.80 8.75 "
     ]
    }
   ],
   "source": [
    "\n",
    "audio, sample_rate = librosa.load(\"C:\\\\Users\\\\bacs2\\\\Downloads\\\\Taller De Grado\\\\Previous\\\\RNNC\\\\Datasets\\\\ALL\\\\DC_a07.wav\", res_type='kaiser_fast')\n",
    "mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "print(f\"{len(mfccs_features)} {len(mfccs_features[0])}\")\n",
    "for j in mfccs_features:\n",
    "    for i in j:\n",
    "        print(f\"{i:.2f}\",end=' ')\n",
    "    print(\"\\n\")\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "print(len(mfccs_scaled_features))\n",
    "for i in mfccs_scaled_features:\n",
    "    print(f\"{i:.2f}\",end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8cf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872e8e",
   "metadata": {},
   "source": [
    "Esta función permite guardar los MFCC en 'features', el código en 'code' y su dirección en 'path' en un archivo json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd2259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_elements_in_json(examples_saved, name):\n",
    "    json_files = []\n",
    "    json_file = {}\n",
    "    index = 0\n",
    "    for file in examples_saved:\n",
    "        json_file = {\"id\": index, \"features\":[str(elem) for elem in file[0]] ,\"code\":file[1], \"path\":file[2]}\n",
    "        json_files.append(json_file)\n",
    "        index += 1\n",
    "    json_object = json.dumps(json_files)\n",
    "    with open(f\"{name}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b9065",
   "metadata": {},
   "source": [
    "La función permite cargar datos del MFCC y código desde un archivo json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278f895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elements_from_json(name):\n",
    "    f = open(f'{name}.json')\n",
    "    data = json.load(f)\n",
    "    examples = []\n",
    "    for element in data:\n",
    "        examples.append(([float(feature) for feature in (element[\"features\"])], element[\"code\"]))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f4074",
   "metadata": {},
   "source": [
    "La función nos permite devolver una lista de MFCC obtenidos de una lista de paths. \n",
    "\n",
    "\n",
    "El MFCC tiene un límite que no le permite cargar archivos menor o igual a 44 kb.\n",
    "\n",
    "Con el diccionario obtenemos el total de audios recuperados por emoción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed0232cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(paths,get_code, files_filters = dict()):\n",
    "    examples = []\n",
    "    for path in paths:\n",
    "        code = get_code(path)\n",
    "        file_stats = os.stat(path)\n",
    "        if (file_stats.st_size > 44):\n",
    "            feature = features_extractor(path)\n",
    "            files_filters[code]+= 1\n",
    "            examples.append((feature,code))\n",
    "    print(f\"Se obtuvo el MFCC de unos {len(paths)} sobre {sum(files_filters[files] for files in files_filters)} audios.\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302b38",
   "metadata": {},
   "source": [
    "Selecciona n ejemplos que necesitemos y los mezcla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab7c5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elements(examples, code, quantity, new_code):\n",
    "    random.shuffle(examples)\n",
    "    elements = []\n",
    "    counter = 1\n",
    "    for example in examples:\n",
    "        if (counter > quantity):\n",
    "            break\n",
    "        if code == example[1]:\n",
    "            elements.append((example[0],new_code))\n",
    "            counter = counter + 1\n",
    "    return elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e14ce",
   "metadata": {},
   "source": [
    "Los siguientes bloques obtienen los paths y filtra las emociones que necesitemos en cada dataset (CREMA-D y SAVEE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a4bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\AudioWav\")\n",
    "emotions_code = [\"NEU\", \"FEA\",\"ANG\"]\n",
    "datas_files = extract_paths_for_emotions_keys(emotions_code, files_path, get_code_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54a6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path_s = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\ALL\")\n",
    "emotions_code_s = [\"a\", \"f\",\"n\"]\n",
    "datas_files_s = extract_paths_for_emotions_keys(emotions_code_s, files_path_s, get_code_savee) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fcc9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "files_filters = dict()\n",
    "files_filters[\"NEU\"] = 0\n",
    "files_filters[\"FEA\"] = 0\n",
    "files_filters[\"ANG\"] = 0\n",
    "files_filters[\"a\"] = 0\n",
    "files_filters[\"f\"] = 0\n",
    "files_filters[\"n\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2bf7a",
   "metadata": {},
   "source": [
    "Obtenemos los MFCC con su código de emoción para posteriormente unir los datos de ambos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13fa56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtuvo el MFCC de unos 3629 sobre 6 audios.\n",
      "Se obtuvo el MFCC de unos 240 sobre 6 audios.\n",
      "transcurrio 0.27 s\n"
     ]
    }
   ],
   "source": [
    "examples = get_features(datas_files, get_code_crema_d, files_filters)\n",
    "examples_s = get_features(datas_files_s, get_code_savee, files_filters)\n",
    "print(f\"transcurrio {round((end.microseconds/1000000),2)} s\")\n",
    "es = examples + examples_s\n",
    "entries = []\n",
    "for example in es:\n",
    "    entries.append((example[0], example[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a9251",
   "metadata": {},
   "source": [
    "Obtendremos una cantidad de datos y cambiamos las claves a los labels {\"without_stress\" y \"stress\"} posteriormente se intercambia los datos, guardamos en X los MFCC e en y los labels, estos también se categorizan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7fe14bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for example in es:\n",
    "    entries.append((example[0], example[1]))\n",
    "datas = select_elements(entries, 'NEU', 896,\"without_stress\")\n",
    "datas += select_elements(entries, 'ANG', 550, \"stress\")\n",
    "datas += select_elements(entries, 'FEA', 550, \"stress\")\n",
    "datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "random.shuffle(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "deae85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for data in datas:\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder=preprocessing.LabelEncoder()\n",
    "y = to_categorical(labelencoder.fit_transform(y))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d475f6",
   "metadata": {},
   "source": [
    "Se separa los datos en una parte para el entrenamiento y en otro para el testeo a partir de un porcentaje (0.8, 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3986252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "27a37435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 448 1788 448\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c906f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bef3f",
   "metadata": {},
   "source": [
    "Esta es la implementación y arquitectura de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "419b81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion del modelo\n",
    "num_labels = y.shape[1]\n",
    "dim_entrada = (X_train.shape[1],1)\n",
    "    \n",
    "#definiendo modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f395eb",
   "metadata": {},
   "source": [
    "Aqui se entrena el modelo y realizará varias iteraciones, posteriormente se puede conseguir el val_loss y el val_accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "60df327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56/56 [==============================] - 7s 61ms/step - loss: 0.6754 - accuracy: 0.5744 - val_loss: 0.6507 - val_accuracy: 0.6094\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.6300 - accuracy: 0.6566 - val_loss: 0.6553 - val_accuracy: 0.6272\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.6008 - accuracy: 0.6706 - val_loss: 0.6198 - val_accuracy: 0.6384\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5687 - accuracy: 0.7075 - val_loss: 0.6046 - val_accuracy: 0.6384\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5618 - accuracy: 0.7109 - val_loss: 0.5807 - val_accuracy: 0.6585\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 3s 49ms/step - loss: 0.5407 - accuracy: 0.7092 - val_loss: 0.5819 - val_accuracy: 0.6607\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.5339 - accuracy: 0.7265 - val_loss: 0.5505 - val_accuracy: 0.6786\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.5415 - accuracy: 0.7136 - val_loss: 0.5600 - val_accuracy: 0.6719\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.5213 - accuracy: 0.7332 - val_loss: 0.5478 - val_accuracy: 0.6674\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5133 - accuracy: 0.7315 - val_loss: 0.5602 - val_accuracy: 0.6719\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.5136 - accuracy: 0.7332 - val_loss: 0.5518 - val_accuracy: 0.6942\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.5022 - accuracy: 0.7438 - val_loss: 0.5583 - val_accuracy: 0.6808\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 3s 57ms/step - loss: 0.5168 - accuracy: 0.7371 - val_loss: 0.5306 - val_accuracy: 0.7009\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 3s 48ms/step - loss: 0.4933 - accuracy: 0.7567 - val_loss: 0.5310 - val_accuracy: 0.7009\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 3s 54ms/step - loss: 0.4841 - accuracy: 0.7455 - val_loss: 0.5312 - val_accuracy: 0.6920\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.4804 - accuracy: 0.7562 - val_loss: 0.5436 - val_accuracy: 0.6942\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4788 - accuracy: 0.7511 - val_loss: 0.5265 - val_accuracy: 0.7121\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4844 - accuracy: 0.7522 - val_loss: 0.5420 - val_accuracy: 0.6964\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4855 - accuracy: 0.7438 - val_loss: 0.5248 - val_accuracy: 0.6942\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 45ms/step - loss: 0.4734 - accuracy: 0.7578 - val_loss: 0.5431 - val_accuracy: 0.6719\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4859 - accuracy: 0.7433 - val_loss: 0.5252 - val_accuracy: 0.7031\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4712 - accuracy: 0.7573 - val_loss: 0.5188 - val_accuracy: 0.7299\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4654 - accuracy: 0.7668 - val_loss: 0.5115 - val_accuracy: 0.7254\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4645 - accuracy: 0.7640 - val_loss: 0.5191 - val_accuracy: 0.7143\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.4446 - accuracy: 0.7729 - val_loss: 0.5179 - val_accuracy: 0.7188\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4598 - accuracy: 0.7673 - val_loss: 0.5159 - val_accuracy: 0.7121\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4458 - accuracy: 0.7651 - val_loss: 0.5103 - val_accuracy: 0.7433\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4479 - accuracy: 0.7657 - val_loss: 0.5036 - val_accuracy: 0.7232\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.4705 - accuracy: 0.7528 - val_loss: 0.5251 - val_accuracy: 0.7076\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 3s 49ms/step - loss: 0.4414 - accuracy: 0.7701 - val_loss: 0.5227 - val_accuracy: 0.7009\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 3s 50ms/step - loss: 0.4415 - accuracy: 0.7746 - val_loss: 0.5111 - val_accuracy: 0.7545\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 3s 61ms/step - loss: 0.4495 - accuracy: 0.7673 - val_loss: 0.5171 - val_accuracy: 0.7188\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 3s 56ms/step - loss: 0.4649 - accuracy: 0.7578 - val_loss: 0.5218 - val_accuracy: 0.7210\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 3s 48ms/step - loss: 0.4302 - accuracy: 0.7852 - val_loss: 0.5219 - val_accuracy: 0.7121\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 3s 52ms/step - loss: 0.4337 - accuracy: 0.7791 - val_loss: 0.5700 - val_accuracy: 0.6853\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 3s 50ms/step - loss: 0.4976 - accuracy: 0.7366 - val_loss: 0.5293 - val_accuracy: 0.7121\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 3s 60ms/step - loss: 0.4574 - accuracy: 0.7567 - val_loss: 0.5481 - val_accuracy: 0.7098\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.4474 - accuracy: 0.7651 - val_loss: 0.5358 - val_accuracy: 0.7076\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 3s 46ms/step - loss: 0.4411 - accuracy: 0.7690 - val_loss: 0.5330 - val_accuracy: 0.7076\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 3s 55ms/step - loss: 0.4431 - accuracy: 0.7735 - val_loss: 0.5130 - val_accuracy: 0.7299\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 3s 50ms/step - loss: 0.4303 - accuracy: 0.7785 - val_loss: 0.5303 - val_accuracy: 0.6830\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 3s 56ms/step - loss: 0.4410 - accuracy: 0.7763 - val_loss: 0.4994 - val_accuracy: 0.7277\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 4s 65ms/step - loss: 0.4175 - accuracy: 0.7858 - val_loss: 0.5280 - val_accuracy: 0.7388\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 3s 56ms/step - loss: 0.4199 - accuracy: 0.7914 - val_loss: 0.4910 - val_accuracy: 0.7388\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 3s 48ms/step - loss: 0.4253 - accuracy: 0.7897 - val_loss: 0.5178 - val_accuracy: 0.7344\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 3s 47ms/step - loss: 0.4283 - accuracy: 0.7796 - val_loss: 0.5280 - val_accuracy: 0.7165\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4108 - accuracy: 0.7847 - val_loss: 0.5068 - val_accuracy: 0.7277\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4048 - accuracy: 0.7931 - val_loss: 0.5008 - val_accuracy: 0.7567\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 3s 59ms/step - loss: 0.4122 - accuracy: 0.7970 - val_loss: 0.5060 - val_accuracy: 0.7321\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4191 - accuracy: 0.7886 - val_loss: 0.5422 - val_accuracy: 0.7254\n"
     ]
    }
   ],
   "source": [
    "#numero de epocas\n",
    "num_epochs = 50\n",
    "num_batch_size = 32\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "duration = datetime.datetime.now() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d79330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.5421816110610962 val_accuracy: 0.7254464030265808\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"val_loss: {test_accuracy[0]}\", f\"val_accuracy: {test_accuracy[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4dbfb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_values = model.predict(X_test)\n",
    "y_prediction=[([1,0] if i[0]>i[1] else [0,1]) for i in y_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95dc43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_i = len(y_values)\n",
    "i = 0\n",
    "true_values = 0\n",
    "while (i < y_i):\n",
    "    true_values += (1 if (y_test[i][0] == y_prediction[i][0] or y_test[i][1] == y_prediction[i][1]) else 0)\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "318bffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo acerto 325 veces sobre los 448 casos.\n"
     ]
    }
   ],
   "source": [
    "print(f\"El algoritmo acerto {true_values} veces sobre los {y_i} casos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea07bfc",
   "metadata": {},
   "source": [
    "El siguiente archivo guarda la configuración y los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4fcbace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_5.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3cb61",
   "metadata": {},
   "source": [
    "Esta línea guarda los datos obtenidos en cada corrida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "105d1165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naccuracy \\nval_loss: 0.4896196722984314 val_accuracy: 0.7209821343421936\\nval_loss: 0.5390793085098267 val_accuracy: 0.7388392686843872\\nval_loss: 0.3872809410095215 val_accuracy: 0.8102678656578064 model_2 \\nval_loss: 0.41717106103897095 val_accuracy: 0.8035714030265808 model_3\\nval_loss: 0.4412716329097748 val_accuracy: 0.7767857313156128\\nval_loss: 0.43090111017227173 val_accuracy: 0.7901785969734192\\nval_loss: 0.4307461678981781 val_accuracy: 0.8013392686843872\\nval_loss: 0.44158974289894104 val_accuracy: 0.7410714030265808\\nval_loss: 0.5022664070129395 val_accuracy: 0.7455357313156128\\nval_loss: 0.4658648669719696 val_accuracy: 0.7700892686843872\\nval_loss: 0.46482276916503906 val_accuracy: 0.7633928656578064\\nval_loss: 0.46192440390586853 val_accuracy: 0.765625\\nval_loss: 0.4675053656101227 val_accuracy: 0.7388392686843872\\nval_loss: 0.42560023069381714 val_accuracy: 0.7924107313156128\\nval_loss: 0.44755128026008606 val_accuracy: 0.7700892686843872\\nval_loss: 0.4769285023212433 val_accuracy: 0.7522321343421936\\nval_loss: 0.4537601172924042 val_accuracy: 0.7544642686843872\\nval_loss: 0.4555659890174866 val_accuracy: 0.7611607313156128\\nval_loss: 0.459335058927536 val_accuracy: 0.75                model_4\\nval_loss: 0.4552021026611328 val_accuracy: 0.7723214030265808\\nval_loss: 0.46012476086616516 val_accuracy: 0.7321428656578064\\nval_loss: 0.4430236220359802 val_accuracy: 0.7723214030265808\\nval_loss: 0.46520107984542847 val_accuracy: 0.7678571343421936\\nval_loss: 0.4261465072631836 val_accuracy: 0.7946428656578064\\nval_loss: 0.4542466700077057 val_accuracy: 0.7700892686843872\\nval_loss: 0.4542466700077057 val_accuracy: 0.7700892686843872\\nval_loss: 0.4661361873149872 val_accuracy: 0.7388392686843872\\nval_loss: 0.4661361873149872 val_accuracy: 0.7388392686843872\\nval_loss: 0.5038288831710815 val_accuracy: 0.7477678656578064\\nval_loss: 0.4211486876010895 val_accuracy: 0.7879464030265808\\nval_loss: 0.4537201523780823 val_accuracy: 0.7433035969734192\\nval_loss: 0.41213124990463257 val_accuracy: 0.765625\\nval_loss: 0.44190025329589844 val_accuracy: 0.7834821343421936\\nval_loss: 0.4493030905723572 val_accuracy: 0.7879464030265808\\nval_loss: 0.6083762049674988 val_accuracy: 0.7008928656578064\\nval_loss: 0.4399203956127167 val_accuracy: 0.7901785969734192\\nval_loss: 0.4300425052642822 val_accuracy: 0.7991071343421936\\nval_loss: 0.43003806471824646 val_accuracy: 0.7767857313156128\\nval_loss: 0.4450111389160156 val_accuracy: 0.7611607313156128\\nval_loss: 0.4859310984611511 val_accuracy: 0.7209821343421936\\nval_loss: 0.4417710602283478 val_accuracy: 0.7700892686843872\\nval_loss: 0.4944555461406708 val_accuracy: 0.7321428656578064\\nval_loss: 0.5189602971076965 val_accuracy: 0.7276785969734192\\nval_loss: 0.4687376022338867 val_accuracy: 0.7834821343421936\\nval_loss: 0.45363569259643555 val_accuracy: 0.7790178656578064\\nval_loss: 0.5052199959754944 val_accuracy: 0.7254464030265808\\nval_loss: 0.42957931756973267 val_accuracy: 0.7857142686843872\\nval_loss: 0.43377751111984253 val_accuracy: 0.7678571343421936\\nval_loss: 0.5221102833747864 val_accuracy: 0.7410714030265808\\nval_loss: 0.40920349955558777 val_accuracy: 0.7946428656578064\\nval_loss: 0.43967631459236145 val_accuracy: 0.7991071343421936\\nval_loss: 0.493486613035202 val_accuracy: 0.7232142686843872\\nval_loss: 0.44643810391426086 val_accuracy: 0.7700892686843872\\nval_loss: 0.43446046113967896 val_accuracy: 0.7790178656578064\\nval_loss: 0.4538533687591553 val_accuracy: 0.7767857313156128\\nval_loss: 0.5259029269218445 val_accuracy: 0.75\\nval_loss: 0.4805241525173187 val_accuracy: 0.7544642686843872\\nval_loss: 0.4938252866268158 val_accuracy: 0.7165178656578064\\nval_loss: 0.3968351483345032 val_accuracy: 0.7879464030265808\\nval_loss: 0.46318870782852173 val_accuracy: 0.7455357313156128\\nval_loss: 0.4486755430698395 val_accuracy: 0.7745535969734192\\nval_loss: 0.4509487748146057 val_accuracy: 0.7700892686843872\\nval_loss: 0.45683008432388306 val_accuracy: 0.7366071343421936\\nval_loss: 0.41022810339927673 val_accuracy: 0.7700892686843872\\nval_loss: 0.4046490788459778 val_accuracy: 0.7924107313156128\\nval_loss: 0.5198861956596375 val_accuracy: 0.7455357313156128\\nval_loss: 0.5119696259498596 val_accuracy: 0.7366071343421936\\nval_loss: 0.5051974058151245 val_accuracy: 0.765625\\nval_loss: 0.4657873809337616 val_accuracy: 0.7611607313156128\\nval_loss: 0.4428311288356781 val_accuracy: 0.7767857313156128\\nval_loss: 0.4056004583835602 val_accuracy: 0.7946428656578064\\nval_loss: 0.4947616457939148 val_accuracy: 0.7388392686843872\\nval_loss: 0.4718573987483978 val_accuracy: 0.7790178656578064\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.8125\n",
    "'''\n",
    "accuracy \n",
    "val_loss: 0.4896196722984314 val_accuracy: 0.7209821343421936\n",
    "val_loss: 0.5390793085098267 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.3872809410095215 val_accuracy: 0.8102678656578064 model_2 \n",
    "val_loss: 0.41717106103897095 val_accuracy: 0.8035714030265808 model_3\n",
    "val_loss: 0.4412716329097748 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.43090111017227173 val_accuracy: 0.7901785969734192\n",
    "val_loss: 0.4307461678981781 val_accuracy: 0.8013392686843872\n",
    "val_loss: 0.44158974289894104 val_accuracy: 0.7410714030265808\n",
    "val_loss: 0.5022664070129395 val_accuracy: 0.7455357313156128\n",
    "val_loss: 0.4658648669719696 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.46482276916503906 val_accuracy: 0.7633928656578064\n",
    "val_loss: 0.46192440390586853 val_accuracy: 0.765625\n",
    "val_loss: 0.4675053656101227 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.42560023069381714 val_accuracy: 0.7924107313156128\n",
    "val_loss: 0.44755128026008606 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.4769285023212433 val_accuracy: 0.7522321343421936\n",
    "val_loss: 0.4537601172924042 val_accuracy: 0.7544642686843872\n",
    "val_loss: 0.4555659890174866 val_accuracy: 0.7611607313156128\n",
    "val_loss: 0.459335058927536 val_accuracy: 0.75                model_4\n",
    "val_loss: 0.4552021026611328 val_accuracy: 0.7723214030265808\n",
    "val_loss: 0.46012476086616516 val_accuracy: 0.7321428656578064\n",
    "val_loss: 0.4430236220359802 val_accuracy: 0.7723214030265808\n",
    "val_loss: 0.46520107984542847 val_accuracy: 0.7678571343421936\n",
    "val_loss: 0.4261465072631836 val_accuracy: 0.7946428656578064\n",
    "val_loss: 0.4542466700077057 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.4542466700077057 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.4661361873149872 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.4661361873149872 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.5038288831710815 val_accuracy: 0.7477678656578064\n",
    "val_loss: 0.4211486876010895 val_accuracy: 0.7879464030265808\n",
    "val_loss: 0.4537201523780823 val_accuracy: 0.7433035969734192\n",
    "val_loss: 0.41213124990463257 val_accuracy: 0.765625\n",
    "val_loss: 0.44190025329589844 val_accuracy: 0.7834821343421936\n",
    "val_loss: 0.4493030905723572 val_accuracy: 0.7879464030265808\n",
    "val_loss: 0.6083762049674988 val_accuracy: 0.7008928656578064\n",
    "val_loss: 0.4399203956127167 val_accuracy: 0.7901785969734192\n",
    "val_loss: 0.4300425052642822 val_accuracy: 0.7991071343421936\n",
    "val_loss: 0.43003806471824646 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.4450111389160156 val_accuracy: 0.7611607313156128\n",
    "val_loss: 0.4859310984611511 val_accuracy: 0.7209821343421936\n",
    "val_loss: 0.4417710602283478 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.4944555461406708 val_accuracy: 0.7321428656578064\n",
    "val_loss: 0.5189602971076965 val_accuracy: 0.7276785969734192\n",
    "val_loss: 0.4687376022338867 val_accuracy: 0.7834821343421936\n",
    "val_loss: 0.45363569259643555 val_accuracy: 0.7790178656578064\n",
    "val_loss: 0.5052199959754944 val_accuracy: 0.7254464030265808\n",
    "val_loss: 0.42957931756973267 val_accuracy: 0.7857142686843872\n",
    "val_loss: 0.43377751111984253 val_accuracy: 0.7678571343421936\n",
    "val_loss: 0.5221102833747864 val_accuracy: 0.7410714030265808\n",
    "val_loss: 0.40920349955558777 val_accuracy: 0.7946428656578064\n",
    "val_loss: 0.43967631459236145 val_accuracy: 0.7991071343421936\n",
    "val_loss: 0.493486613035202 val_accuracy: 0.7232142686843872\n",
    "val_loss: 0.44643810391426086 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.43446046113967896 val_accuracy: 0.7790178656578064\n",
    "val_loss: 0.4538533687591553 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.5259029269218445 val_accuracy: 0.75\n",
    "val_loss: 0.4805241525173187 val_accuracy: 0.7544642686843872\n",
    "val_loss: 0.4938252866268158 val_accuracy: 0.7165178656578064\n",
    "val_loss: 0.3968351483345032 val_accuracy: 0.7879464030265808\n",
    "val_loss: 0.46318870782852173 val_accuracy: 0.7455357313156128\n",
    "val_loss: 0.4486755430698395 val_accuracy: 0.7745535969734192\n",
    "val_loss: 0.4509487748146057 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.45683008432388306 val_accuracy: 0.7366071343421936\n",
    "val_loss: 0.41022810339927673 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.4046490788459778 val_accuracy: 0.7924107313156128\n",
    "val_loss: 0.5198861956596375 val_accuracy: 0.7455357313156128\n",
    "val_loss: 0.5119696259498596 val_accuracy: 0.7366071343421936\n",
    "val_loss: 0.5051974058151245 val_accuracy: 0.765625\n",
    "val_loss: 0.4657873809337616 val_accuracy: 0.7611607313156128\n",
    "val_loss: 0.4428311288356781 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.4056004583835602 val_accuracy: 0.7946428656578064\n",
    "val_loss: 0.4947616457939148 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.4718573987483978 val_accuracy: 0.7790178656578064\n",
    "val_loss: 0.44807955622673035 val_accuracy: 0.7589285969734192\n",
    "val_loss: 0.4626654088497162 val_accuracy: 0.7589285969734192\n",
    "val_loss: 0.49828478693962097 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.49322938919067383 val_accuracy: 0.7433035969734192\n",
    "val_loss: 0.4238942265510559 val_accuracy: 0.7790178656578064\n",
    "val_loss: 0.460832804441452 val_accuracy: 0.7455357313156128\n",
    "val_loss: 0.46732380986213684 val_accuracy: 0.7589285969734192\n",
    "val_loss: 0.40039685368537903 val_accuracy: 0.7745535969734192\n",
    "val_loss: 0.4678298830986023 val_accuracy: 0.7433035969734192\n",
    "val_loss: 0.4330616593360901 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.469801664352417 val_accuracy: 0.78125\n",
    "val_loss: 0.4441928267478943 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.5477870106697083 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.5036300420761108 val_accuracy: 0.7410714030265808\n",
    "val_loss: 0.41244035959243774 val_accuracy: 0.7946428656578064\n",
    "val_loss: 0.4877232015132904 val_accuracy: 0.7366071343421936\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb14f13",
   "metadata": {},
   "source": [
    "Este bloque ejecuta todo el código que vimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5bbce660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se obtuvo el MFCC de unos 3629 sobre 3628 audios.\n",
      "Se obtuvo el MFCC de unos 240 sobre 3868 audios.\n",
      "transcurrio 0.46 s\n",
      "1788 448 1788 448\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 10s 76ms/step - loss: 0.6837 - accuracy: 0.5330 - val_loss: 0.6629 - val_accuracy: 0.5335\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.6250 - accuracy: 0.6493 - val_loss: 0.5975 - val_accuracy: 0.6786\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.6002 - accuracy: 0.6739 - val_loss: 0.5908 - val_accuracy: 0.6987\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.5912 - accuracy: 0.6773 - val_loss: 0.5780 - val_accuracy: 0.7210\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5730 - accuracy: 0.6935 - val_loss: 0.5514 - val_accuracy: 0.7210\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.5511 - accuracy: 0.7013 - val_loss: 0.5724 - val_accuracy: 0.6897\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5517 - accuracy: 0.7030 - val_loss: 0.5397 - val_accuracy: 0.7188\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.5366 - accuracy: 0.7114 - val_loss: 0.5291 - val_accuracy: 0.7210\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 3s 50ms/step - loss: 0.5236 - accuracy: 0.7170 - val_loss: 0.5243 - val_accuracy: 0.7210\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.5107 - accuracy: 0.7388 - val_loss: 0.5096 - val_accuracy: 0.7366\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 3s 53ms/step - loss: 0.5092 - accuracy: 0.7215 - val_loss: 0.5191 - val_accuracy: 0.7478\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5318 - accuracy: 0.7131 - val_loss: 0.5600 - val_accuracy: 0.6942\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5218 - accuracy: 0.7114 - val_loss: 0.5145 - val_accuracy: 0.7455\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.5106 - accuracy: 0.7399 - val_loss: 0.5099 - val_accuracy: 0.7388\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5016 - accuracy: 0.7327 - val_loss: 0.5003 - val_accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 44ms/step - loss: 0.5021 - accuracy: 0.7411 - val_loss: 0.5345 - val_accuracy: 0.7031\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4965 - accuracy: 0.7377 - val_loss: 0.5048 - val_accuracy: 0.7388\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 3s 46ms/step - loss: 0.4871 - accuracy: 0.7528 - val_loss: 0.4877 - val_accuracy: 0.7589\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4786 - accuracy: 0.7461 - val_loss: 0.4936 - val_accuracy: 0.7589\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.5010 - accuracy: 0.7304 - val_loss: 0.5172 - val_accuracy: 0.7411\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 3s 49ms/step - loss: 0.4809 - accuracy: 0.7466 - val_loss: 0.4879 - val_accuracy: 0.7478\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 3s 51ms/step - loss: 0.4753 - accuracy: 0.7522 - val_loss: 0.4889 - val_accuracy: 0.7500\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 3s 52ms/step - loss: 0.4913 - accuracy: 0.7450 - val_loss: 0.4903 - val_accuracy: 0.7522\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 3s 55ms/step - loss: 0.4716 - accuracy: 0.7589 - val_loss: 0.4794 - val_accuracy: 0.7589\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4618 - accuracy: 0.7713 - val_loss: 0.4792 - val_accuracy: 0.7567\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4695 - accuracy: 0.7645 - val_loss: 0.4936 - val_accuracy: 0.7545\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4808 - accuracy: 0.7539 - val_loss: 0.4806 - val_accuracy: 0.7522\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4670 - accuracy: 0.7545 - val_loss: 0.4800 - val_accuracy: 0.7634\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 3s 47ms/step - loss: 0.4597 - accuracy: 0.7651 - val_loss: 0.4747 - val_accuracy: 0.7478\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 44ms/step - loss: 0.4494 - accuracy: 0.7740 - val_loss: 0.4838 - val_accuracy: 0.7656\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4585 - accuracy: 0.7673 - val_loss: 0.4762 - val_accuracy: 0.7634\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4675 - accuracy: 0.7578 - val_loss: 0.4662 - val_accuracy: 0.7679\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4428 - accuracy: 0.7729 - val_loss: 0.4858 - val_accuracy: 0.7656\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4475 - accuracy: 0.7757 - val_loss: 0.4891 - val_accuracy: 0.7478\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4582 - accuracy: 0.7640 - val_loss: 0.4785 - val_accuracy: 0.7768\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4334 - accuracy: 0.7808 - val_loss: 0.4602 - val_accuracy: 0.7835\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4544 - accuracy: 0.7589 - val_loss: 0.5019 - val_accuracy: 0.7500\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 44ms/step - loss: 0.4459 - accuracy: 0.7813 - val_loss: 0.4524 - val_accuracy: 0.7701\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4308 - accuracy: 0.7869 - val_loss: 0.4605 - val_accuracy: 0.7656\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4399 - accuracy: 0.7802 - val_loss: 0.4680 - val_accuracy: 0.7567\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 2s 44ms/step - loss: 0.4328 - accuracy: 0.7802 - val_loss: 0.4584 - val_accuracy: 0.7656\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4316 - accuracy: 0.7763 - val_loss: 0.4679 - val_accuracy: 0.7768\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4178 - accuracy: 0.7886 - val_loss: 0.4433 - val_accuracy: 0.7679\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4492 - accuracy: 0.7690 - val_loss: 0.4544 - val_accuracy: 0.7746\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4327 - accuracy: 0.7908 - val_loss: 0.4560 - val_accuracy: 0.7567\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4196 - accuracy: 0.7992 - val_loss: 0.4724 - val_accuracy: 0.7835\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4034 - accuracy: 0.7925 - val_loss: 0.4504 - val_accuracy: 0.7790\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4051 - accuracy: 0.7947 - val_loss: 0.4527 - val_accuracy: 0.7701\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4326 - accuracy: 0.7729 - val_loss: 0.4789 - val_accuracy: 0.7388\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4006 - accuracy: 0.8043 - val_loss: 0.4484 - val_accuracy: 0.7790\n",
      "val_loss: 0.4483540952205658 val_accuracy: 0.7790178656578064\n",
      "14/14 [==============================] - 1s 15ms/step\n",
      "El algoritmo acerto 349 veces sobre los 448 casos.\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "files_filters = dict()\n",
    "files_filters[\"NEU\"] = 0\n",
    "files_filters[\"FEA\"] = 0\n",
    "files_filters[\"ANG\"] = 0\n",
    "files_filters[\"a\"] = 0\n",
    "files_filters[\"f\"] = 0\n",
    "files_filters[\"n\"] = 0\n",
    "start = datetime.datetime.now()\n",
    "examples = get_features(datas_files, get_code_crema_d, files_filters)\n",
    "examples_s = get_features(datas_files_s, get_code_savee, files_filters)\n",
    "end = datetime.datetime.now() - start\n",
    "print(f\"transcurrio {round((end.microseconds/1000000),2)} s\")\n",
    "es = examples + examples_s\n",
    "entries = []\n",
    "for example in es:\n",
    "    entries.append((example[0], example[1]))\n",
    "datas = select_elements(entries, 'NEU', 896,\"without_stress\")\n",
    "datas += select_elements(entries, 'ANG', 550, \"stress\")\n",
    "datas += select_elements(entries, 'FEA', 550, \"stress\")\n",
    "datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "random.shuffle(datas)\n",
    "X = []\n",
    "y = []\n",
    "for data in datas:\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])\n",
    "labelencoder=preprocessing.LabelEncoder()\n",
    "y = to_categorical(labelencoder.fit_transform(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2,random_state=0)\n",
    "print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
    "y = np.array(y)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "#creacion del modelo\n",
    "num_labels = y.shape[1]\n",
    "dim_entrada = (X_train.shape[1],1)\n",
    "    \n",
    "#definiendo modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "#numero de epocas\n",
    "num_epochs = 50\n",
    "num_batch_size = 32\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "duration = datetime.datetime.now() - start\n",
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"val_loss: {test_accuracy[0]}\", f\"val_accuracy: {test_accuracy[1]}\")\n",
    "y_values = model.predict(X_test)\n",
    "y_prediction=[([1,0] if i[0]>i[1] else [0,1]) for i in y_values]\n",
    "y_i = len(y_values)\n",
    "i = 0\n",
    "true_values = 0\n",
    "while (i < y_i):\n",
    "    true_values += (1 if (y_test[i][0] == y_prediction[i][0] or y_test[i][1] == y_prediction[i][1]) else 0)\n",
    "    i = i + 1 \n",
    "print(f\"El algoritmo acerto {true_values} veces sobre los {y_i} casos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03527f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "val_loss: 0.42675861716270447 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.44326552748680115 val_accuracy: 0.7611607313156128\n",
    "val_loss: 0.4475260078907013 val_accuracy: 0.7566964030265808\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f0a73",
   "metadata": {},
   "source": [
    "#### Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b478ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f9995db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:(2236, 40) y:(2236, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(f\"X:{X.shape} y:{y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa77694",
   "metadata": {},
   "source": [
    "El bloque realizara el k fold cross validation dividiendolo en 5 folds, se uso los MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "941cf153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train - X:(1788, 40) y:(1788, 2)\n",
      "Test - X:(448, 40) y:(448, 2)\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 7s 55ms/step - loss: 0.6818 - accuracy: 0.5330 - val_loss: 0.6681 - val_accuracy: 0.5692\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.6364 - accuracy: 0.6443 - val_loss: 0.5988 - val_accuracy: 0.6562\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.6073 - accuracy: 0.6695 - val_loss: 0.5759 - val_accuracy: 0.6964\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 0.5801 - accuracy: 0.6952 - val_loss: 0.5607 - val_accuracy: 0.7076\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 0.5774 - accuracy: 0.6840 - val_loss: 0.5574 - val_accuracy: 0.7054\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 0.5535 - accuracy: 0.6980 - val_loss: 0.5501 - val_accuracy: 0.6875\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.5515 - accuracy: 0.6974 - val_loss: 0.5288 - val_accuracy: 0.7210\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.5429 - accuracy: 0.7075 - val_loss: 0.5482 - val_accuracy: 0.7098\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5414 - accuracy: 0.7120 - val_loss: 0.5168 - val_accuracy: 0.7366\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5321 - accuracy: 0.7209 - val_loss: 0.5334 - val_accuracy: 0.7143\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5291 - accuracy: 0.7103 - val_loss: 0.5100 - val_accuracy: 0.7366\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5387 - accuracy: 0.7136 - val_loss: 0.5052 - val_accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5064 - accuracy: 0.7332 - val_loss: 0.4923 - val_accuracy: 0.7366\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4987 - accuracy: 0.7416 - val_loss: 0.5007 - val_accuracy: 0.7411\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4917 - accuracy: 0.7399 - val_loss: 0.4934 - val_accuracy: 0.7188\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5038 - accuracy: 0.7366 - val_loss: 0.4968 - val_accuracy: 0.7478\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4964 - accuracy: 0.7483 - val_loss: 0.5096 - val_accuracy: 0.7299\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4898 - accuracy: 0.7528 - val_loss: 0.4847 - val_accuracy: 0.7433\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4830 - accuracy: 0.7494 - val_loss: 0.5050 - val_accuracy: 0.7344\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4760 - accuracy: 0.7528 - val_loss: 0.4826 - val_accuracy: 0.7478\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4748 - accuracy: 0.7573 - val_loss: 0.4686 - val_accuracy: 0.7433\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4720 - accuracy: 0.7640 - val_loss: 0.4682 - val_accuracy: 0.7567\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4574 - accuracy: 0.7735 - val_loss: 0.5000 - val_accuracy: 0.7277\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4697 - accuracy: 0.7629 - val_loss: 0.4667 - val_accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4496 - accuracy: 0.7791 - val_loss: 0.4750 - val_accuracy: 0.7433\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4703 - accuracy: 0.7528 - val_loss: 0.4994 - val_accuracy: 0.7433\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4619 - accuracy: 0.7651 - val_loss: 0.4889 - val_accuracy: 0.7500\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4495 - accuracy: 0.7724 - val_loss: 0.4855 - val_accuracy: 0.7411\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4630 - accuracy: 0.7668 - val_loss: 0.5137 - val_accuracy: 0.7210\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4573 - accuracy: 0.7651 - val_loss: 0.4632 - val_accuracy: 0.7500\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4754 - accuracy: 0.7539 - val_loss: 0.4986 - val_accuracy: 0.7121\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4589 - accuracy: 0.7662 - val_loss: 0.4653 - val_accuracy: 0.7634\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4575 - accuracy: 0.7617 - val_loss: 0.4794 - val_accuracy: 0.7478\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4401 - accuracy: 0.7785 - val_loss: 0.4567 - val_accuracy: 0.7567\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4441 - accuracy: 0.7752 - val_loss: 0.4755 - val_accuracy: 0.7567\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.4305 - accuracy: 0.7785 - val_loss: 0.4684 - val_accuracy: 0.7634\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 0.4517 - accuracy: 0.7668 - val_loss: 0.4808 - val_accuracy: 0.7433\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4242 - accuracy: 0.7897 - val_loss: 0.4636 - val_accuracy: 0.7612\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4243 - accuracy: 0.7959 - val_loss: 0.4521 - val_accuracy: 0.7746\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4684 - accuracy: 0.7550 - val_loss: 0.4999 - val_accuracy: 0.7054\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 4s 64ms/step - loss: 0.4560 - accuracy: 0.7707 - val_loss: 0.4906 - val_accuracy: 0.7210\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 3s 53ms/step - loss: 0.4522 - accuracy: 0.7685 - val_loss: 0.4991 - val_accuracy: 0.7054\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 3s 53ms/step - loss: 0.4460 - accuracy: 0.7768 - val_loss: 0.4518 - val_accuracy: 0.7679\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 4s 65ms/step - loss: 0.4181 - accuracy: 0.7931 - val_loss: 0.4764 - val_accuracy: 0.7433\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 3s 58ms/step - loss: 0.4179 - accuracy: 0.7953 - val_loss: 0.4589 - val_accuracy: 0.7634\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.4198 - accuracy: 0.7891 - val_loss: 0.4511 - val_accuracy: 0.7768\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.3998 - accuracy: 0.8015 - val_loss: 0.4535 - val_accuracy: 0.7589\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.3879 - accuracy: 0.8104 - val_loss: 0.5124 - val_accuracy: 0.7545\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4309 - accuracy: 0.7791 - val_loss: 0.4686 - val_accuracy: 0.7366\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 3s 47ms/step - loss: 0.4053 - accuracy: 0.8015 - val_loss: 0.4810 - val_accuracy: 0.7500\n",
      "14/14 [==============================] - 1s 15ms/step\n",
      "Fold score (MSE): 0.25\n",
      "Fold #2\n",
      "Train - X:(1789, 40) y:(1789, 2)\n",
      "Test - X:(447, 40) y:(447, 2)\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 11s 84ms/step - loss: 0.6806 - accuracy: 0.5266 - val_loss: 0.6670 - val_accuracy: 0.5347\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.6255 - accuracy: 0.6339 - val_loss: 0.5770 - val_accuracy: 0.7092\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5929 - accuracy: 0.6702 - val_loss: 0.5721 - val_accuracy: 0.6957\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.5791 - accuracy: 0.6719 - val_loss: 0.5601 - val_accuracy: 0.7204\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5682 - accuracy: 0.6814 - val_loss: 0.5643 - val_accuracy: 0.7092\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.5691 - accuracy: 0.6713 - val_loss: 0.5454 - val_accuracy: 0.7181\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5525 - accuracy: 0.6937 - val_loss: 0.5409 - val_accuracy: 0.6957\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.5434 - accuracy: 0.7004 - val_loss: 0.5227 - val_accuracy: 0.7204\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5362 - accuracy: 0.7088 - val_loss: 0.5207 - val_accuracy: 0.7293\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5319 - accuracy: 0.7093 - val_loss: 0.5221 - val_accuracy: 0.7338\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5344 - accuracy: 0.7132 - val_loss: 0.5166 - val_accuracy: 0.7360\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 3s 48ms/step - loss: 0.5395 - accuracy: 0.7082 - val_loss: 0.5147 - val_accuracy: 0.7427\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5252 - accuracy: 0.7099 - val_loss: 0.5119 - val_accuracy: 0.7159\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.5084 - accuracy: 0.7228 - val_loss: 0.4759 - val_accuracy: 0.7696\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 3s 56ms/step - loss: 0.5469 - accuracy: 0.6998 - val_loss: 0.5103 - val_accuracy: 0.7181\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 3s 45ms/step - loss: 0.5328 - accuracy: 0.7093 - val_loss: 0.5110 - val_accuracy: 0.7159\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5129 - accuracy: 0.7300 - val_loss: 0.5226 - val_accuracy: 0.7271\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5007 - accuracy: 0.7323 - val_loss: 0.5285 - val_accuracy: 0.7360\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.5004 - accuracy: 0.7373 - val_loss: 0.4876 - val_accuracy: 0.7606\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4883 - accuracy: 0.7412 - val_loss: 0.4738 - val_accuracy: 0.7629\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4860 - accuracy: 0.7401 - val_loss: 0.4925 - val_accuracy: 0.7651\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4877 - accuracy: 0.7373 - val_loss: 0.4499 - val_accuracy: 0.7696\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4981 - accuracy: 0.7228 - val_loss: 0.5081 - val_accuracy: 0.7383\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4903 - accuracy: 0.7412 - val_loss: 0.4655 - val_accuracy: 0.7785\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4687 - accuracy: 0.7518 - val_loss: 0.4454 - val_accuracy: 0.7740\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4832 - accuracy: 0.7412 - val_loss: 0.4612 - val_accuracy: 0.7651\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4617 - accuracy: 0.7546 - val_loss: 0.4541 - val_accuracy: 0.7718\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4657 - accuracy: 0.7580 - val_loss: 0.4234 - val_accuracy: 0.7875\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4735 - accuracy: 0.7501 - val_loss: 0.4749 - val_accuracy: 0.7584\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4613 - accuracy: 0.7636 - val_loss: 0.4518 - val_accuracy: 0.7897\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4653 - accuracy: 0.7703 - val_loss: 0.4508 - val_accuracy: 0.7763\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4778 - accuracy: 0.7535 - val_loss: 0.4927 - val_accuracy: 0.7271\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4810 - accuracy: 0.7524 - val_loss: 0.4233 - val_accuracy: 0.7875\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4552 - accuracy: 0.7624 - val_loss: 0.4333 - val_accuracy: 0.7987\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4426 - accuracy: 0.7731 - val_loss: 0.4250 - val_accuracy: 0.7919\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4446 - accuracy: 0.7675 - val_loss: 0.4218 - val_accuracy: 0.7964\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4486 - accuracy: 0.7647 - val_loss: 0.4545 - val_accuracy: 0.7673\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4785 - accuracy: 0.7479 - val_loss: 0.4962 - val_accuracy: 0.7271\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4732 - accuracy: 0.7496 - val_loss: 0.4229 - val_accuracy: 0.7919\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.4493 - accuracy: 0.7624 - val_loss: 0.4175 - val_accuracy: 0.7897\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 1s 25ms/step - loss: 0.4455 - accuracy: 0.7580 - val_loss: 0.4032 - val_accuracy: 0.8031\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4445 - accuracy: 0.7658 - val_loss: 0.4661 - val_accuracy: 0.7629\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4383 - accuracy: 0.7719 - val_loss: 0.4551 - val_accuracy: 0.8098\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4313 - accuracy: 0.7770 - val_loss: 0.4215 - val_accuracy: 0.7942\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 2s 43ms/step - loss: 0.4203 - accuracy: 0.7742 - val_loss: 0.4332 - val_accuracy: 0.8076\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4116 - accuracy: 0.7831 - val_loss: 0.4090 - val_accuracy: 0.7763\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4342 - accuracy: 0.7731 - val_loss: 0.4577 - val_accuracy: 0.7494\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 0.4305 - accuracy: 0.7697 - val_loss: 0.4724 - val_accuracy: 0.7942\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 1s 27ms/step - loss: 0.4260 - accuracy: 0.7725 - val_loss: 0.4310 - val_accuracy: 0.7987\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4144 - accuracy: 0.7837 - val_loss: 0.4496 - val_accuracy: 0.7606\n",
      "14/14 [==============================] - 1s 16ms/step\n",
      "Fold score (MSE): 0.23937360178970918\n",
      "Fold #3\n",
      "Train - X:(1789, 40) y:(1789, 2)\n",
      "Test - X:(447, 40) y:(447, 2)\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 6s 54ms/step - loss: 0.6831 - accuracy: 0.5467 - val_loss: 0.6598 - val_accuracy: 0.6331\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.6275 - accuracy: 0.6478 - val_loss: 0.5837 - val_accuracy: 0.6667\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 0.5881 - accuracy: 0.6808 - val_loss: 0.5463 - val_accuracy: 0.6935\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.5757 - accuracy: 0.6959 - val_loss: 0.5739 - val_accuracy: 0.6689\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 1s 25ms/step - loss: 0.5599 - accuracy: 0.6982 - val_loss: 0.5214 - val_accuracy: 0.7181\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 0.5615 - accuracy: 0.7049 - val_loss: 0.5182 - val_accuracy: 0.7114\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.5435 - accuracy: 0.7144 - val_loss: 0.5096 - val_accuracy: 0.7248\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 0.5297 - accuracy: 0.7194 - val_loss: 0.5056 - val_accuracy: 0.7226\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.5265 - accuracy: 0.7272 - val_loss: 0.5304 - val_accuracy: 0.7114\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.5275 - accuracy: 0.7233 - val_loss: 0.4983 - val_accuracy: 0.7383\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.5096 - accuracy: 0.7339 - val_loss: 0.4870 - val_accuracy: 0.7360\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5133 - accuracy: 0.7311 - val_loss: 0.4687 - val_accuracy: 0.7427\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4953 - accuracy: 0.7446 - val_loss: 0.5097 - val_accuracy: 0.7092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4893 - accuracy: 0.7513 - val_loss: 0.5160 - val_accuracy: 0.7159\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.5024 - accuracy: 0.7367 - val_loss: 0.4611 - val_accuracy: 0.7539\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4832 - accuracy: 0.7501 - val_loss: 0.4848 - val_accuracy: 0.7338\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4866 - accuracy: 0.7546 - val_loss: 0.4542 - val_accuracy: 0.7673\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4718 - accuracy: 0.7630 - val_loss: 0.4606 - val_accuracy: 0.7539\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 0.4718 - accuracy: 0.7552 - val_loss: 0.4372 - val_accuracy: 0.7808\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 0.4708 - accuracy: 0.7479 - val_loss: 0.4313 - val_accuracy: 0.7875\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.4724 - accuracy: 0.7546 - val_loss: 0.4338 - val_accuracy: 0.7785\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4519 - accuracy: 0.7658 - val_loss: 0.4269 - val_accuracy: 0.7740\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4528 - accuracy: 0.7568 - val_loss: 0.4487 - val_accuracy: 0.7584\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.4513 - accuracy: 0.7669 - val_loss: 0.4709 - val_accuracy: 0.7606\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 0.4702 - accuracy: 0.7602 - val_loss: 0.4494 - val_accuracy: 0.7494\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4476 - accuracy: 0.7680 - val_loss: 0.4396 - val_accuracy: 0.7808\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 0.4475 - accuracy: 0.7596 - val_loss: 0.4302 - val_accuracy: 0.7785\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.4443 - accuracy: 0.7641 - val_loss: 0.4974 - val_accuracy: 0.7315\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 1s 23ms/step - loss: 0.4639 - accuracy: 0.7552 - val_loss: 0.4096 - val_accuracy: 0.7875\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.4571 - accuracy: 0.7708 - val_loss: 0.4800 - val_accuracy: 0.7383\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 0.4536 - accuracy: 0.7602 - val_loss: 0.4226 - val_accuracy: 0.7740\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4506 - accuracy: 0.7641 - val_loss: 0.4407 - val_accuracy: 0.7785\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4392 - accuracy: 0.7703 - val_loss: 0.4017 - val_accuracy: 0.7919\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4304 - accuracy: 0.7865 - val_loss: 0.4029 - val_accuracy: 0.7875\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4297 - accuracy: 0.7786 - val_loss: 0.4049 - val_accuracy: 0.7830\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4390 - accuracy: 0.7747 - val_loss: 0.4118 - val_accuracy: 0.7875\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4279 - accuracy: 0.7909 - val_loss: 0.4126 - val_accuracy: 0.8009\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4139 - accuracy: 0.7915 - val_loss: 0.4410 - val_accuracy: 0.7740\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4260 - accuracy: 0.7865 - val_loss: 0.4036 - val_accuracy: 0.8054\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4240 - accuracy: 0.7814 - val_loss: 0.4020 - val_accuracy: 0.8031\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.3954 - accuracy: 0.8066 - val_loss: 0.4045 - val_accuracy: 0.8031\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4047 - accuracy: 0.7988 - val_loss: 0.4633 - val_accuracy: 0.7808\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4409 - accuracy: 0.7781 - val_loss: 0.4104 - val_accuracy: 0.7852\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4102 - accuracy: 0.7988 - val_loss: 0.4477 - val_accuracy: 0.7964\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4228 - accuracy: 0.7859 - val_loss: 0.4009 - val_accuracy: 0.8031\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4091 - accuracy: 0.7915 - val_loss: 0.3985 - val_accuracy: 0.8166\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 0.4341 - accuracy: 0.7714 - val_loss: 0.4069 - val_accuracy: 0.7852\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 1s 24ms/step - loss: 0.4160 - accuracy: 0.7943 - val_loss: 0.4239 - val_accuracy: 0.7919\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 1s 23ms/step - loss: 0.4048 - accuracy: 0.8021 - val_loss: 0.4112 - val_accuracy: 0.7852\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 1s 24ms/step - loss: 0.4125 - accuracy: 0.7949 - val_loss: 0.3904 - val_accuracy: 0.8121\n",
      "14/14 [==============================] - 1s 10ms/step\n",
      "Fold score (MSE): 0.18791946308724833\n",
      "Fold #4\n",
      "Train - X:(1789, 40) y:(1789, 2)\n",
      "Test - X:(447, 40) y:(447, 2)\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 7s 54ms/step - loss: 0.6860 - accuracy: 0.5226 - val_loss: 0.6691 - val_accuracy: 0.5682\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.6484 - accuracy: 0.6082 - val_loss: 0.5870 - val_accuracy: 0.6801\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.6100 - accuracy: 0.6685 - val_loss: 0.5893 - val_accuracy: 0.6711\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.5827 - accuracy: 0.6847 - val_loss: 0.6022 - val_accuracy: 0.6600\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.5687 - accuracy: 0.7026 - val_loss: 0.5612 - val_accuracy: 0.6913\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5530 - accuracy: 0.7077 - val_loss: 0.5698 - val_accuracy: 0.6667\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.5402 - accuracy: 0.7183 - val_loss: 0.5557 - val_accuracy: 0.6711\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5327 - accuracy: 0.7244 - val_loss: 0.5358 - val_accuracy: 0.7159\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.5180 - accuracy: 0.7272 - val_loss: 0.5162 - val_accuracy: 0.7226\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.5298 - accuracy: 0.7155 - val_loss: 0.5327 - val_accuracy: 0.7002\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5122 - accuracy: 0.7378 - val_loss: 0.5140 - val_accuracy: 0.7271\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5042 - accuracy: 0.7367 - val_loss: 0.4939 - val_accuracy: 0.7338\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5170 - accuracy: 0.7323 - val_loss: 0.5318 - val_accuracy: 0.7159\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.5078 - accuracy: 0.7378 - val_loss: 0.5104 - val_accuracy: 0.7427\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.5015 - accuracy: 0.7451 - val_loss: 0.4946 - val_accuracy: 0.7293\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4871 - accuracy: 0.7641 - val_loss: 0.4933 - val_accuracy: 0.7383\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4882 - accuracy: 0.7529 - val_loss: 0.4940 - val_accuracy: 0.7405\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4776 - accuracy: 0.7641 - val_loss: 0.4960 - val_accuracy: 0.7293\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4861 - accuracy: 0.7513 - val_loss: 0.4964 - val_accuracy: 0.7159\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4765 - accuracy: 0.7703 - val_loss: 0.4980 - val_accuracy: 0.7517\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4680 - accuracy: 0.7686 - val_loss: 0.4873 - val_accuracy: 0.7383\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4609 - accuracy: 0.7680 - val_loss: 0.5048 - val_accuracy: 0.7114\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4633 - accuracy: 0.7725 - val_loss: 0.4773 - val_accuracy: 0.7405\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4504 - accuracy: 0.7798 - val_loss: 0.4804 - val_accuracy: 0.7383\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4634 - accuracy: 0.7691 - val_loss: 0.4747 - val_accuracy: 0.7472\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4712 - accuracy: 0.7630 - val_loss: 0.4876 - val_accuracy: 0.7293\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4548 - accuracy: 0.7742 - val_loss: 0.4671 - val_accuracy: 0.7696\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4449 - accuracy: 0.7798 - val_loss: 0.4715 - val_accuracy: 0.7472\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4438 - accuracy: 0.7831 - val_loss: 0.4650 - val_accuracy: 0.7517\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4296 - accuracy: 0.7865 - val_loss: 0.4588 - val_accuracy: 0.7696\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4277 - accuracy: 0.7932 - val_loss: 0.4820 - val_accuracy: 0.7539\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4303 - accuracy: 0.7887 - val_loss: 0.4974 - val_accuracy: 0.7248\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4514 - accuracy: 0.7658 - val_loss: 0.4999 - val_accuracy: 0.7584\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4462 - accuracy: 0.7786 - val_loss: 0.4916 - val_accuracy: 0.7360\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4299 - accuracy: 0.7876 - val_loss: 0.4620 - val_accuracy: 0.7696\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4293 - accuracy: 0.7926 - val_loss: 0.4613 - val_accuracy: 0.7606\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4353 - accuracy: 0.7820 - val_loss: 0.4748 - val_accuracy: 0.7405\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4208 - accuracy: 0.7949 - val_loss: 0.4849 - val_accuracy: 0.7271\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4164 - accuracy: 0.7982 - val_loss: 0.4739 - val_accuracy: 0.7472\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4048 - accuracy: 0.8055 - val_loss: 0.4933 - val_accuracy: 0.7405\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4211 - accuracy: 0.7932 - val_loss: 0.4621 - val_accuracy: 0.7606\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4130 - accuracy: 0.7937 - val_loss: 0.4669 - val_accuracy: 0.7517\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4113 - accuracy: 0.7993 - val_loss: 0.4604 - val_accuracy: 0.7584\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4275 - accuracy: 0.7893 - val_loss: 0.4666 - val_accuracy: 0.7494\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4251 - accuracy: 0.7893 - val_loss: 0.4689 - val_accuracy: 0.7673\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4105 - accuracy: 0.7909 - val_loss: 0.4942 - val_accuracy: 0.7338\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 41ms/step - loss: 0.4234 - accuracy: 0.7904 - val_loss: 0.4695 - val_accuracy: 0.7427\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4201 - accuracy: 0.7988 - val_loss: 0.4606 - val_accuracy: 0.7427\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.3957 - accuracy: 0.8004 - val_loss: 0.4628 - val_accuracy: 0.7494\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.3929 - accuracy: 0.8167 - val_loss: 0.4446 - val_accuracy: 0.7740\n",
      "14/14 [==============================] - 1s 15ms/step\n",
      "Fold score (MSE): 0.22595078299776286\n",
      "Fold #5\n",
      "Train - X:(1789, 40) y:(1789, 2)\n",
      "Test - X:(447, 40) y:(447, 2)\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 5s 50ms/step - loss: 0.6923 - accuracy: 0.5165 - val_loss: 0.6782 - val_accuracy: 0.5123\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.6424 - accuracy: 0.5931 - val_loss: 0.6455 - val_accuracy: 0.6309\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.6025 - accuracy: 0.6574 - val_loss: 0.6197 - val_accuracy: 0.6532\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5791 - accuracy: 0.6948 - val_loss: 0.5891 - val_accuracy: 0.6868\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5516 - accuracy: 0.7026 - val_loss: 0.5993 - val_accuracy: 0.6398\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5423 - accuracy: 0.7054 - val_loss: 0.5830 - val_accuracy: 0.6644\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5235 - accuracy: 0.7272 - val_loss: 0.5632 - val_accuracy: 0.6957\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5504 - accuracy: 0.7071 - val_loss: 0.5927 - val_accuracy: 0.6555\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5333 - accuracy: 0.7099 - val_loss: 0.5611 - val_accuracy: 0.7025\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5118 - accuracy: 0.7350 - val_loss: 0.5386 - val_accuracy: 0.7002\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 0.5037 - accuracy: 0.7384 - val_loss: 0.5523 - val_accuracy: 0.6913\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4905 - accuracy: 0.7390 - val_loss: 0.5403 - val_accuracy: 0.7114\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5001 - accuracy: 0.7306 - val_loss: 0.5346 - val_accuracy: 0.7136\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4829 - accuracy: 0.7373 - val_loss: 0.5455 - val_accuracy: 0.6890\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4810 - accuracy: 0.7501 - val_loss: 0.5327 - val_accuracy: 0.7002\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4791 - accuracy: 0.7423 - val_loss: 0.5232 - val_accuracy: 0.7271\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4771 - accuracy: 0.7501 - val_loss: 0.4990 - val_accuracy: 0.7450\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4675 - accuracy: 0.7490 - val_loss: 0.5226 - val_accuracy: 0.7047\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4690 - accuracy: 0.7473 - val_loss: 0.5097 - val_accuracy: 0.7338\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4599 - accuracy: 0.7647 - val_loss: 0.5038 - val_accuracy: 0.7271\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4613 - accuracy: 0.7535 - val_loss: 0.5412 - val_accuracy: 0.6756\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4727 - accuracy: 0.7468 - val_loss: 0.5167 - val_accuracy: 0.7338\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4522 - accuracy: 0.7591 - val_loss: 0.4829 - val_accuracy: 0.7315\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4458 - accuracy: 0.7613 - val_loss: 0.4774 - val_accuracy: 0.7338\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4581 - accuracy: 0.7591 - val_loss: 0.5109 - val_accuracy: 0.7450\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4539 - accuracy: 0.7736 - val_loss: 0.4896 - val_accuracy: 0.7338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4521 - accuracy: 0.7619 - val_loss: 0.5360 - val_accuracy: 0.7114\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4487 - accuracy: 0.7619 - val_loss: 0.5018 - val_accuracy: 0.7315\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4358 - accuracy: 0.7697 - val_loss: 0.4814 - val_accuracy: 0.7539\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4281 - accuracy: 0.7708 - val_loss: 0.4833 - val_accuracy: 0.7360\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4341 - accuracy: 0.7731 - val_loss: 0.4823 - val_accuracy: 0.7562\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4214 - accuracy: 0.7775 - val_loss: 0.4936 - val_accuracy: 0.7427\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4631 - accuracy: 0.7535 - val_loss: 0.4898 - val_accuracy: 0.7427\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4142 - accuracy: 0.7703 - val_loss: 0.5036 - val_accuracy: 0.7405\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4355 - accuracy: 0.7837 - val_loss: 0.4861 - val_accuracy: 0.7450\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4152 - accuracy: 0.7803 - val_loss: 0.4647 - val_accuracy: 0.7584\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4042 - accuracy: 0.7988 - val_loss: 0.4683 - val_accuracy: 0.7584\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4024 - accuracy: 0.7893 - val_loss: 0.5007 - val_accuracy: 0.7517\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4019 - accuracy: 0.8038 - val_loss: 0.4616 - val_accuracy: 0.7517\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4073 - accuracy: 0.7949 - val_loss: 0.4652 - val_accuracy: 0.7405\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.3946 - accuracy: 0.7876 - val_loss: 0.4788 - val_accuracy: 0.7584\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.3951 - accuracy: 0.8010 - val_loss: 0.4594 - val_accuracy: 0.7740\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4272 - accuracy: 0.7814 - val_loss: 0.5268 - val_accuracy: 0.7204\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4059 - accuracy: 0.7893 - val_loss: 0.4585 - val_accuracy: 0.7606\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.3831 - accuracy: 0.8021 - val_loss: 0.4643 - val_accuracy: 0.7517\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.3788 - accuracy: 0.8099 - val_loss: 0.4978 - val_accuracy: 0.7606\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.3874 - accuracy: 0.8038 - val_loss: 0.4644 - val_accuracy: 0.7517\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4055 - accuracy: 0.7915 - val_loss: 0.5271 - val_accuracy: 0.7405\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.3937 - accuracy: 0.7859 - val_loss: 0.5216 - val_accuracy: 0.7360\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.3813 - accuracy: 0.8072 - val_loss: 0.4600 - val_accuracy: 0.7651\n",
      "14/14 [==============================] - 1s 13ms/step\n",
      "Fold score (MSE): 0.2348993288590604\n",
      "-----------------------\n",
      "Cross-validated score (MSE): 0.2348993288590604\n",
      "-----------------------\n",
      "Resumen\n",
      "Fold score (MSE): 0.25\n",
      "Fold score (MSE): 0.23937360178970918\n",
      "Fold score (MSE): 0.18791946308724833\n",
      "Fold score (MSE): 0.22595078299776286\n",
      "Fold score (MSE): 0.2348993288590604\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(5)\n",
    "fold = 0\n",
    "y_tests = []\n",
    "predictions = []\n",
    "results = []\n",
    "for train, test in k_fold.split(X):\n",
    "    fold = fold + 1\n",
    "    print(f\"Fold #{fold}\")\n",
    "    \n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    print(f\"Train - X:{X_train.shape} y:{y_train.shape}\")\n",
    "    print(f\"Test - X:{X_test.shape} y:{y_test.shape}\")\n",
    "    \n",
    "    num_labels = y.shape[1]\n",
    "    dim_entrada = (X_train.shape[1],1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    \n",
    "    num_epochs = 50\n",
    "    num_batch_size = 32\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    y_tests.append(y_test)\n",
    "    pred=[([1,0] if i[0]>i[1] else [0,1]) for i in pred]\n",
    "    predictions.append(pred)\n",
    "    score = metrics.mean_squared_error(pred, y_test)\n",
    "    results.append([score])\n",
    "    print(f\"Fold score (MSE): {score}\")\n",
    "\n",
    "y_tests = np.concatenate(y_tests)\n",
    "predictions = np.concatenate(predictions)\n",
    "score = metrics.mean_squared_error(pred, y_test)\n",
    "print(\"-----------------------\")\n",
    "print(f\"Cross-validated score (MSE): {score}\")\n",
    "print(\"-----------------------\")\n",
    "print(\"Resumen\")\n",
    "for result in results:\n",
    "    print(f\"Fold score (MSE): {result[0]}\")\n",
    "    \n",
    "#https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_2_kfold.ipynb\n",
    "#https://www.youtube.com/watch?v=maiQf8ray_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "725c04a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error([[1,0],[0,1],[0,1],[1,0]], [[1,0],[0,1],[0,1],[0,1]])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ca5f97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25727069351230425"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5072185855351756**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f59fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
