{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2afd9a5",
   "metadata": {},
   "source": [
    "## Antiguo proyecto de Redes Neuronales Recurrentes usando audios .wav apartir de direcciones, convertiendolos en vectores de MFCC y aplicandolos a una RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9e513",
   "metadata": {},
   "source": [
    "Las siguientes funciones se dedican a obtener las direcciones, filtrarlas por emociones y ingresarlos al MFCC.\n",
    "\n",
    "_ Se agregan los archivos buscando en 2 direcciones (los datasets de CREMA-D y SAVEE).\n",
    "\n",
    "_ Para el dataset de CREMA-D se buscan n casos aleatorios de una emocion como entrada.\n",
    "\n",
    "_ Para el dataset de SAVEE se obtienen todos los casos como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcf75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import soundfile as sf\n",
    "import json\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4830c4a",
   "metadata": {},
   "source": [
    "#### Conseguir los paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfde04d",
   "metadata": {},
   "source": [
    "La funcion consigue los caminos a los archivos de audio pasandole una direccion y devuelve el camino completa al audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ffb974",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_files_from_path(directory):\n",
    "    path_files = []\n",
    "    dir_list = os.listdir(directory)\n",
    "    for path in dir_list:\n",
    "        path_files.append(directory+\"\\\\\"+path)\n",
    "    return path_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c67b2",
   "metadata": {},
   "source": [
    "Con una lista de codigos de emociones, caminos a los audios y una funcion para obtener el codigo a partir del path, devuelve solo los paths de las emociones buscadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364254b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths_for_emotions_keys(emotions_code, files_path, get_code):\n",
    "    paths = []\n",
    "    emotions_set = set(emotions_code)\n",
    "    for code_file in files_path:\n",
    "        if (get_code(code_file) in emotions_set):\n",
    "            paths.append(code_file)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78353db8",
   "metadata": {},
   "source": [
    "Esta funcion abre el archivo y obtiene el mfcc escalado en un vector de 40 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8cf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872e8e",
   "metadata": {},
   "source": [
    "Esta funcion permite guardar los MFCC con referencia del audio obtenido en un archivo json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd2259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_elements_in_json(examples_saved, name):\n",
    "    json_files = []\n",
    "    json_file = {}\n",
    "    index = 0\n",
    "    for file in examples_saved:\n",
    "        json_file = {\"id\": index, \"features\":[str(elem) for elem in file[0]] ,\"code\":file[1], \"path\":file[2]}\n",
    "        json_files.append(json_file)\n",
    "        index += 1\n",
    "    json_object = json.dumps(json_files)\n",
    "    with open(f\"{name}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b9065",
   "metadata": {},
   "source": [
    "La funcion permite cargar datos como el MFCC y referencias a la ubicacion del audio de un archivo json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278f895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elements_from_json(name):\n",
    "    f = open(f'{name}.json')\n",
    "    data = json.load(f)\n",
    "    examples = []\n",
    "    for element in data:\n",
    "        examples.append(([float(feature) for feature in (element[\"features\"])], element[\"code\"]))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f4074",
   "metadata": {},
   "source": [
    "La funcion nos permite devolver una lista de MFCC obtenidos de una lista de paths, el MFCC tiene un limite que no le permite cargar archivos de menos igual a 44 kb, al final si se le paso un diccionario imprime las estadisticas de los datos obtenidos en el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0232cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(datas_file,get_code, files_filters = dict()):\n",
    "    examples = []\n",
    "    for data_file in datas_file:\n",
    "        file_stats = os.stat(data_file)\n",
    "        if (file_stats.st_size > 44):\n",
    "            feature = features_extractor(data_file)\n",
    "            files_filters[get_code(data_file)]+= 1\n",
    "            examples.append((feature,get_code(data_file), data_file))\n",
    "    print(files_filters)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302b38",
   "metadata": {},
   "source": [
    "Selecciona n lineas a paritir de unos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab7c5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elements(examples, code, quantity, new_code):\n",
    "    random.shuffle(examples)\n",
    "    elements = []\n",
    "    counter = 1\n",
    "    for example in examples:\n",
    "        if (counter > quantity):\n",
    "            break\n",
    "        if code == example[1]:\n",
    "            elements.append((example[0],new_code))\n",
    "            counter = counter + 1\n",
    "    return elements\n",
    "\n",
    "def get_code_crema_d(path):\n",
    "    return path[107:110]\n",
    "\n",
    "def get_code_savee(path):\n",
    "    return path[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a4bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\AudioWav\")\n",
    "emotions_code = [\"NEU\", \"FEA\",\"ANG\"]\n",
    "datas_files = extract_paths_for_emotions_keys(emotions_code, files_path, get_code_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54a6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path_s = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\ALL\")\n",
    "emotions_code_s = [\"a\", \"f\",\"n\"]\n",
    "datas_files_s = extract_paths_for_emotions_keys(emotions_code_s, files_path_s, get_code_savee) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e14ce",
   "metadata": {},
   "source": [
    "Los siguientes bloques obtienen todos los MFCC de una lista de paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fcc9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "files_filters = dict()\n",
    "files_filters[\"NEU\"] = 0\n",
    "files_filters[\"FEA\"] = 0\n",
    "files_filters[\"ANG\"] = 0\n",
    "files_filters[\"a\"] = 0\n",
    "files_filters[\"f\"] = 0\n",
    "files_filters[\"n\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a0cd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 0, 'f': 0, 'n': 0}\n"
     ]
    }
   ],
   "source": [
    "examples = get_features(datas_files, get_code_crema_d, files_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e3cd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 60, 'f': 60, 'n': 120}\n"
     ]
    }
   ],
   "source": [
    "examples_s = get_features(datas_files_s, get_code_savee, files_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71d26b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = examples + examples_s\n",
    "entries = []\n",
    "for example in es:\n",
    "    entries.append((example[0], example[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a9251",
   "metadata": {},
   "source": [
    "Se filtra en un lista la cantidad de datos por cada emocion, devuelve la cantidad de entradas con una salida que indique si existe o no estres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13fa56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = select_elements(entries, 'NEU', 896,\"without_stress\")\n",
    "datas += select_elements(entries, 'ANG', 550, \"stress\")\n",
    "datas += select_elements(entries, 'FEA', 550, \"stress\")\n",
    "datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "random.shuffle(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "deae85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for data in datas:\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder=preprocessing.LabelEncoder()\n",
    "y = to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d475f6",
   "metadata": {},
   "source": [
    "Se separa los datos en una parte para el entrenamiento y en otro para el testeo apartir de un porcentaje (0.8, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3986252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27a37435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 448 1788 448\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fb8ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:\\\\Users\\\\bacs2\\\\Downloads\\\\Taller De Grado\\\\Previous\\\\Datasets\\\\AudioWAV\\\\1001_DFA_ANG_XX.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5319a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c906f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "419b81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion del modelo\n",
    "num_labels = y.shape[1]\n",
    "dim_entrada = (X_train.shape[1],1)\n",
    "    \n",
    "#definiendo modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60df327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56/56 [==============================] - 9s 75ms/step - loss: 0.6780 - accuracy: 0.5408 - val_loss: 0.6587 - val_accuracy: 0.5625\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 3s 55ms/step - loss: 0.6184 - accuracy: 0.6449 - val_loss: 0.6088 - val_accuracy: 0.6116\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.5802 - accuracy: 0.6779 - val_loss: 0.6203 - val_accuracy: 0.6406\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.5465 - accuracy: 0.7030 - val_loss: 0.5627 - val_accuracy: 0.6741\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.5654 - accuracy: 0.6918 - val_loss: 0.5673 - val_accuracy: 0.6786\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.5405 - accuracy: 0.7103 - val_loss: 0.5616 - val_accuracy: 0.6786\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.5333 - accuracy: 0.7125 - val_loss: 0.5360 - val_accuracy: 0.6897\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5168 - accuracy: 0.7254 - val_loss: 0.5540 - val_accuracy: 0.6987\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5074 - accuracy: 0.7164 - val_loss: 0.5512 - val_accuracy: 0.7076\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5089 - accuracy: 0.7204 - val_loss: 0.5279 - val_accuracy: 0.6786\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.5056 - accuracy: 0.7282 - val_loss: 0.5099 - val_accuracy: 0.7076\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.5054 - accuracy: 0.7243 - val_loss: 0.5218 - val_accuracy: 0.7054\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4882 - accuracy: 0.7366 - val_loss: 0.5208 - val_accuracy: 0.6987\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4989 - accuracy: 0.7237 - val_loss: 0.5402 - val_accuracy: 0.6897\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4773 - accuracy: 0.7489 - val_loss: 0.5080 - val_accuracy: 0.7143\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4684 - accuracy: 0.7528 - val_loss: 0.5232 - val_accuracy: 0.7165\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4831 - accuracy: 0.7360 - val_loss: 0.5055 - val_accuracy: 0.6920\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4747 - accuracy: 0.7455 - val_loss: 0.5351 - val_accuracy: 0.6987\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 38ms/step - loss: 0.4751 - accuracy: 0.7494 - val_loss: 0.5029 - val_accuracy: 0.7054\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4640 - accuracy: 0.7578 - val_loss: 0.5057 - val_accuracy: 0.7054\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4714 - accuracy: 0.7517 - val_loss: 0.5084 - val_accuracy: 0.7054\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4607 - accuracy: 0.7573 - val_loss: 0.4932 - val_accuracy: 0.7232\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4486 - accuracy: 0.7679 - val_loss: 0.4957 - val_accuracy: 0.7121\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 39ms/step - loss: 0.4622 - accuracy: 0.7612 - val_loss: 0.5632 - val_accuracy: 0.6875\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4620 - accuracy: 0.7567 - val_loss: 0.5155 - val_accuracy: 0.7232\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 3s 48ms/step - loss: 0.4744 - accuracy: 0.7461 - val_loss: 0.4871 - val_accuracy: 0.7321\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 3s 46ms/step - loss: 0.4532 - accuracy: 0.7584 - val_loss: 0.5264 - val_accuracy: 0.7232\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 40ms/step - loss: 0.4516 - accuracy: 0.7584 - val_loss: 0.4811 - val_accuracy: 0.7277\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 2s 44ms/step - loss: 0.4525 - accuracy: 0.7713 - val_loss: 0.5333 - val_accuracy: 0.7210\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 44ms/step - loss: 0.4384 - accuracy: 0.7662 - val_loss: 0.5076 - val_accuracy: 0.7411\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 23s 426ms/step - loss: 0.4430 - accuracy: 0.7746 - val_loss: 0.5038 - val_accuracy: 0.7277\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 5s 81ms/step - loss: 0.4459 - accuracy: 0.7785 - val_loss: 0.5513 - val_accuracy: 0.7277\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 3s 57ms/step - loss: 0.4395 - accuracy: 0.7668 - val_loss: 0.5148 - val_accuracy: 0.7254\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 3s 53ms/step - loss: 0.4343 - accuracy: 0.7763 - val_loss: 0.4785 - val_accuracy: 0.7321\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 3s 47ms/step - loss: 0.4222 - accuracy: 0.7808 - val_loss: 0.5068 - val_accuracy: 0.7321\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 3s 48ms/step - loss: 0.4384 - accuracy: 0.7780 - val_loss: 0.4870 - val_accuracy: 0.7299\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 42ms/step - loss: 0.4179 - accuracy: 0.7886 - val_loss: 0.4896 - val_accuracy: 0.7388\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.4053 - accuracy: 0.7936 - val_loss: 0.5177 - val_accuracy: 0.7366\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.4268 - accuracy: 0.7718 - val_loss: 0.4809 - val_accuracy: 0.7299\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.4092 - accuracy: 0.7964 - val_loss: 0.5704 - val_accuracy: 0.7321\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.4231 - accuracy: 0.7886 - val_loss: 0.4947 - val_accuracy: 0.7277\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.3972 - accuracy: 0.7975 - val_loss: 0.5360 - val_accuracy: 0.7411\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 0.3949 - accuracy: 0.8026 - val_loss: 0.5028 - val_accuracy: 0.7522\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 0.4222 - accuracy: 0.7914 - val_loss: 0.4850 - val_accuracy: 0.7344\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 0.4302 - accuracy: 0.7824 - val_loss: 0.5297 - val_accuracy: 0.7344\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4019 - accuracy: 0.7981 - val_loss: 0.5136 - val_accuracy: 0.7321\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 0.4017 - accuracy: 0.8059 - val_loss: 0.4818 - val_accuracy: 0.7433\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 0.3783 - accuracy: 0.8166 - val_loss: 0.4940 - val_accuracy: 0.7433\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 1s 14ms/step - loss: 0.3999 - accuracy: 0.8043 - val_loss: 0.4963 - val_accuracy: 0.7232\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 1s 14ms/step - loss: 0.4111 - accuracy: 0.7936 - val_loss: 0.5282 - val_accuracy: 0.7232\n",
      "0.7232142686843872\n"
     ]
    }
   ],
   "source": [
    "#numero de epocas\n",
    "num_epochs = 50\n",
    "num_batch_size = 32\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "duration = datetime.datetime.now() - start\n",
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b0dbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7455357313156128\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4fcbace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d1165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
