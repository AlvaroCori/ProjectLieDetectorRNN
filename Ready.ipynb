{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2afd9a5",
   "metadata": {},
   "source": [
    "## Antiguo proyecto de Redes Neuronales Recurrentes usando audios .wav apartir de direcciones, convertiendolos en vectores de MFCC y aplicandolos a una RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9e513",
   "metadata": {},
   "source": [
    "Las siguientes funciones se dedican a obtener las direcciones, filtrarlas por emociones y ingresarlos al MFCC.\n",
    "\n",
    "_ Se agregan los archivos buscando en 2 direcciones (los datasets de CREMA-D y SAVEE).\n",
    "\n",
    "_ Para el dataset de CREMA-D se buscan n casos aleatorios de una emocion como entrada.\n",
    "\n",
    "_ Para el dataset de SAVEE se obtienen todos los casos como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcf75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import soundfile as sf\n",
    "import json\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4830c4a",
   "metadata": {},
   "source": [
    "#### Conseguir los paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfde04d",
   "metadata": {},
   "source": [
    "La funcion consigue los caminos a los archivos de audio pasandole una direccion y devuelve el camino completa al audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ffb974",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_files_from_path(directory):\n",
    "    path_files = []\n",
    "    dir_list = os.listdir(directory)\n",
    "    for path in dir_list:\n",
    "        path_files.append(directory+\"\\\\\"+path)\n",
    "    return path_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c67b2",
   "metadata": {},
   "source": [
    "Con una lista de codigos de emociones, caminos a los audios y una funcion para obtener el codigo a partir del path, devuelve solo los paths de las emociones buscadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364254b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths_for_emotions_keys(emotions_code, files_path, get_code):\n",
    "    paths = []\n",
    "    emotions_set = set(emotions_code)\n",
    "    for code_file in files_path:\n",
    "        if (get_code(code_file) in emotions_set):\n",
    "            paths.append(code_file)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78353db8",
   "metadata": {},
   "source": [
    "Esta funcion abre el archivo y obtiene el mfcc escalado en un vector de 40 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8cf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872e8e",
   "metadata": {},
   "source": [
    "Esta funcion permite guardar los MFCC con referencia del audio obtenido en un archivo json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd2259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_elements_in_json(examples_saved, name):\n",
    "    json_files = []\n",
    "    json_file = {}\n",
    "    index = 0\n",
    "    for file in examples_saved:\n",
    "        json_file = {\"id\": index, \"features\":[str(elem) for elem in file[0]] ,\"code\":file[1], \"path\":file[2]}\n",
    "        json_files.append(json_file)\n",
    "        index += 1\n",
    "    json_object = json.dumps(json_files)\n",
    "    with open(f\"{name}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b9065",
   "metadata": {},
   "source": [
    "La funcion permite cargar datos como el MFCC y referencias a la ubicacion del audio de un archivo json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278f895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elements_from_json(name):\n",
    "    f = open(f'{name}.json')\n",
    "    data = json.load(f)\n",
    "    examples = []\n",
    "    for element in data:\n",
    "        examples.append(([float(feature) for feature in (element[\"features\"])], element[\"code\"]))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f4074",
   "metadata": {},
   "source": [
    "La funcion nos permite devolver una lista de MFCC obtenidos de una lista de paths, el MFCC tiene un limite que no le permite cargar archivos de menos igual a 44 kb, al final si se le paso un diccionario imprime las estadisticas de los datos obtenidos en el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0232cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(datas_file,get_code, files_filters = dict()):\n",
    "    examples = []\n",
    "    for data_file in datas_file:\n",
    "        file_stats = os.stat(data_file)\n",
    "        if (file_stats.st_size > 44):\n",
    "            feature = features_extractor(data_file)\n",
    "            files_filters[get_code(data_file)]+= 1\n",
    "            examples.append((feature,get_code(data_file), data_file))\n",
    "    print(files_filters)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302b38",
   "metadata": {},
   "source": [
    "Selecciona n lineas a paritir de unos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ab7c5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elements(examples, code, quantity, new_code):\n",
    "    random.shuffle(examples)\n",
    "    elements = []\n",
    "    counter = 1\n",
    "    for example in examples:\n",
    "        if (counter > quantity):\n",
    "            break\n",
    "        if code == example[1]:\n",
    "            elements.append((example[0],new_code))\n",
    "            counter = counter + 1\n",
    "    return elements\n",
    "\n",
    "def get_code_crema_d(path):\n",
    "    return path[107:110]\n",
    "\n",
    "def get_code_savee(path):\n",
    "    return path[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "2a4bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\AudioWav\")\n",
    "emotions_code = [\"NEU\", \"FEA\",\"ANG\"]\n",
    "datas_files = extract_paths_for_emotions_keys(emotions_code, files_path, get_code_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "54a6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path_s = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\ALL\")\n",
    "emotions_code_s = [\"a\", \"f\",\"n\"]\n",
    "datas_files_s = extract_paths_for_emotions_keys(emotions_code_s, files_path_s, get_code_savee) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e14ce",
   "metadata": {},
   "source": [
    "Los siguientes bloques obtienen todos los MFCC de una lista de paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "0fcc9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "files_filters = dict()\n",
    "files_filters[\"NEU\"] = 0\n",
    "files_filters[\"FEA\"] = 0\n",
    "files_filters[\"ANG\"] = 0\n",
    "files_filters[\"a\"] = 0\n",
    "files_filters[\"f\"] = 0\n",
    "files_filters[\"n\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "7fe14bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 0, 'f': 0, 'n': 0}\n",
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 60, 'f': 60, 'n': 120}\n",
      "transcurrio 0.4 s\n",
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 60, 'f': 60, 'n': 120}\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "examples = get_features(datas_files, get_code_crema_d, files_filters)\n",
    "examples_s = get_features(datas_files_s, get_code_savee, files_filters)\n",
    "end = datetime.datetime.now() - start\n",
    "print(f\"transcurrio {round((end.microseconds/1000000),2)} s\")\n",
    "print(files_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "71d26b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = examples + examples_s\n",
    "entries = []\n",
    "for example in es:\n",
    "    entries.append((example[0], example[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a9251",
   "metadata": {},
   "source": [
    "Se filtra en un lista la cantidad de datos por cada emocion, devuelve la cantidad de entradas con una salida que indique si existe o no estres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "13fa56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = select_elements(entries, 'NEU', 896,\"without_stress\")\n",
    "datas += select_elements(entries, 'ANG', 550, \"stress\")\n",
    "datas += select_elements(entries, 'FEA', 550, \"stress\")\n",
    "datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "random.shuffle(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "deae85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for data in datas:\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "80e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder=preprocessing.LabelEncoder()\n",
    "y = to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d475f6",
   "metadata": {},
   "source": [
    "Se separa los datos en una parte para el entrenamiento y en otro para el testeo apartir de un porcentaje (0.8, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "3986252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "27a37435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 448 1788 448\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "2fb8ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:\\\\Users\\\\bacs2\\\\Downloads\\\\Taller De Grado\\\\Previous\\\\Datasets\\\\AudioWAV\\\\1001_DFA_ANG_XX.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "5319a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "c906f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "419b81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion del modelo\n",
    "num_labels = y.shape[1]\n",
    "dim_entrada = (X_train.shape[1],1)\n",
    "    \n",
    "#definiendo modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "60df327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56/56 [==============================] - 12s 53ms/step - loss: 0.6813 - accuracy: 0.5537 - val_loss: 0.6515 - val_accuracy: 0.5781\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.6221 - accuracy: 0.6393 - val_loss: 0.5974 - val_accuracy: 0.6451\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.5658 - accuracy: 0.6930 - val_loss: 0.5705 - val_accuracy: 0.6719\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.5497 - accuracy: 0.7019 - val_loss: 0.5746 - val_accuracy: 0.6786\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.5397 - accuracy: 0.7131 - val_loss: 0.5573 - val_accuracy: 0.6853\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.5224 - accuracy: 0.7170 - val_loss: 0.5479 - val_accuracy: 0.6920\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.5129 - accuracy: 0.7422 - val_loss: 0.5147 - val_accuracy: 0.7031\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.5114 - accuracy: 0.7248 - val_loss: 0.5235 - val_accuracy: 0.7009\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.5038 - accuracy: 0.7243 - val_loss: 0.5185 - val_accuracy: 0.7277\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.5050 - accuracy: 0.7343 - val_loss: 0.5119 - val_accuracy: 0.7277\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4948 - accuracy: 0.7394 - val_loss: 0.5168 - val_accuracy: 0.7054\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4945 - accuracy: 0.7394 - val_loss: 0.5048 - val_accuracy: 0.7321\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4863 - accuracy: 0.7466 - val_loss: 0.4832 - val_accuracy: 0.7344\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4896 - accuracy: 0.7355 - val_loss: 0.5087 - val_accuracy: 0.7143\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4865 - accuracy: 0.7500 - val_loss: 0.4993 - val_accuracy: 0.7344\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4774 - accuracy: 0.7511 - val_loss: 0.5012 - val_accuracy: 0.7478\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4775 - accuracy: 0.7534 - val_loss: 0.4972 - val_accuracy: 0.7188\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4713 - accuracy: 0.7556 - val_loss: 0.5013 - val_accuracy: 0.6964\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4824 - accuracy: 0.7472 - val_loss: 0.5041 - val_accuracy: 0.7121\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4612 - accuracy: 0.7573 - val_loss: 0.5148 - val_accuracy: 0.6942\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4837 - accuracy: 0.7466 - val_loss: 0.5393 - val_accuracy: 0.6920\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4934 - accuracy: 0.7355 - val_loss: 0.5007 - val_accuracy: 0.6897\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4812 - accuracy: 0.7483 - val_loss: 0.4899 - val_accuracy: 0.7254\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4774 - accuracy: 0.7511 - val_loss: 0.4829 - val_accuracy: 0.7210\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4634 - accuracy: 0.7528 - val_loss: 0.4784 - val_accuracy: 0.7299\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4631 - accuracy: 0.7589 - val_loss: 0.4986 - val_accuracy: 0.7232\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4559 - accuracy: 0.7651 - val_loss: 0.4872 - val_accuracy: 0.7344\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4412 - accuracy: 0.7740 - val_loss: 0.4732 - val_accuracy: 0.7277\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4949 - accuracy: 0.7416 - val_loss: 0.5249 - val_accuracy: 0.6964\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4797 - accuracy: 0.7366 - val_loss: 0.4997 - val_accuracy: 0.7188\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4455 - accuracy: 0.7729 - val_loss: 0.4725 - val_accuracy: 0.7411\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4352 - accuracy: 0.7685 - val_loss: 0.4873 - val_accuracy: 0.7188\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4444 - accuracy: 0.7707 - val_loss: 0.4707 - val_accuracy: 0.7388\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4284 - accuracy: 0.7819 - val_loss: 0.4675 - val_accuracy: 0.7344\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4250 - accuracy: 0.7763 - val_loss: 0.4757 - val_accuracy: 0.7478\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4207 - accuracy: 0.7785 - val_loss: 0.4759 - val_accuracy: 0.7232\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4191 - accuracy: 0.7830 - val_loss: 0.5391 - val_accuracy: 0.7165\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4444 - accuracy: 0.7791 - val_loss: 0.4684 - val_accuracy: 0.7455\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4309 - accuracy: 0.7757 - val_loss: 0.4802 - val_accuracy: 0.7143\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4374 - accuracy: 0.7701 - val_loss: 0.4699 - val_accuracy: 0.7254\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4349 - accuracy: 0.7685 - val_loss: 0.4791 - val_accuracy: 0.7433\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4417 - accuracy: 0.7696 - val_loss: 0.4681 - val_accuracy: 0.7433\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4225 - accuracy: 0.7869 - val_loss: 0.4709 - val_accuracy: 0.7388\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4259 - accuracy: 0.7808 - val_loss: 0.4813 - val_accuracy: 0.7210\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4161 - accuracy: 0.7914 - val_loss: 0.4887 - val_accuracy: 0.7455\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.4070 - accuracy: 0.7847 - val_loss: 0.4532 - val_accuracy: 0.7388\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4120 - accuracy: 0.7925 - val_loss: 0.4634 - val_accuracy: 0.7500\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.4092 - accuracy: 0.7869 - val_loss: 0.4532 - val_accuracy: 0.7299\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4030 - accuracy: 0.7875 - val_loss: 0.4702 - val_accuracy: 0.7433\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.4085 - accuracy: 0.7886 - val_loss: 0.4601 - val_accuracy: 0.7321\n"
     ]
    }
   ],
   "source": [
    "#numero de epocas\n",
    "num_epochs = 50\n",
    "num_batch_size = 32\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "duration = datetime.datetime.now() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "9d79330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.46012476086616516 val_accuracy: 0.7321428656578064\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"val_loss: {test_accuracy[0]}\", f\"val_accuracy: {test_accuracy[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "4dbfb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "y_values = model.predict(X_test)\n",
    "y_prediction=[([1,0] if i[0]>i[1] else [0,1]) for i in y_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "95dc43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_i = len(y_values)\n",
    "i = 0\n",
    "true_values = 0\n",
    "while (i < y_i):\n",
    "    true_values += (1 if (y_test[i][0] == y_prediction[i][0] or y_test[i][1] == y_prediction[i][1]) else 0)\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "318bffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El algoritmo acerto 328 veces sobre los 448 casos.\n"
     ]
    }
   ],
   "source": [
    "print(f\"El algoritmo acerto {true_values} veces sobre los {y_i} casos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "f4fcbace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_5.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "105d1165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naccuracy \\nval_loss: 0.4896196722984314 val_accuracy: 0.7209821343421936\\nval_loss: 0.5390793085098267 val_accuracy: 0.7388392686843872\\nval_loss: 0.3872809410095215 val_accuracy: 0.8102678656578064 model_2 \\nval_loss: 0.41717106103897095 val_accuracy: 0.8035714030265808 model_3\\nval_loss: 0.4412716329097748 val_accuracy: 0.7767857313156128\\nval_loss: 0.43090111017227173 val_accuracy: 0.7901785969734192\\n'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.8125\n",
    "'''\n",
    "accuracy \n",
    "val_loss: 0.4896196722984314 val_accuracy: 0.7209821343421936\n",
    "val_loss: 0.5390793085098267 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.3872809410095215 val_accuracy: 0.8102678656578064 model_2 \n",
    "val_loss: 0.41717106103897095 val_accuracy: 0.8035714030265808 model_3\n",
    "val_loss: 0.4412716329097748 val_accuracy: 0.7767857313156128\n",
    "val_loss: 0.43090111017227173 val_accuracy: 0.7901785969734192\n",
    "val_loss: 0.4307461678981781 val_accuracy: 0.8013392686843872\n",
    "val_loss: 0.44158974289894104 val_accuracy: 0.7410714030265808\n",
    "val_loss: 0.5022664070129395 val_accuracy: 0.7455357313156128\n",
    "val_loss: 0.4658648669719696 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.46482276916503906 val_accuracy: 0.7633928656578064\n",
    "val_loss: 0.46192440390586853 val_accuracy: 0.765625\n",
    "val_loss: 0.4675053656101227 val_accuracy: 0.7388392686843872\n",
    "val_loss: 0.42560023069381714 val_accuracy: 0.7924107313156128\n",
    "val_loss: 0.44755128026008606 val_accuracy: 0.7700892686843872\n",
    "val_loss: 0.4769285023212433 val_accuracy: 0.7522321343421936\n",
    "val_loss: 0.4537601172924042 val_accuracy: 0.7544642686843872\n",
    "val_loss: 0.4555659890174866 val_accuracy: 0.7611607313156128\n",
    "val_loss: 0.459335058927536 val_accuracy: 0.75                model_4\n",
    "val_loss: 0.4552021026611328 val_accuracy: 0.7723214030265808\n",
    "val_loss: 0.46012476086616516 val_accuracy: 0.7321428656578064\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbce660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
