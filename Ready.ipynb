{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2afd9a5",
   "metadata": {},
   "source": [
    "## Antiguo proyecto de Redes Neuronales Recurrentes usando audios .wav apartir de direcciones, convertiendolos en vectores de MFCC y aplicandolos a una RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9e513",
   "metadata": {},
   "source": [
    "Las siguientes funciones se dedican a obtener las direcciones, filtrarlas por emociones y ingresarlos al MFCC.\n",
    "\n",
    "_ Se agregan los archivos buscando en 2 direcciones (los datasets de CREMA-D y SAVEE).\n",
    "\n",
    "_ Para el dataset de CREMA-D se buscan n casos aleatorios de una emocion como entrada.\n",
    "\n",
    "_ Para el dataset de SAVEE se obtienen todos los casos como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bbcf75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import soundfile as sf\n",
    "import json\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4830c4a",
   "metadata": {},
   "source": [
    "#### Conseguir los paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfde04d",
   "metadata": {},
   "source": [
    "La funcion consigue los caminos a los archivos de audio pasandole una direccion y devuelve el camino completa al audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c6ffb974",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_files_from_path(directory):\n",
    "    path_files = []\n",
    "    dir_list = os.listdir(directory)\n",
    "    for path in dir_list:\n",
    "        path_files.append(directory+\"\\\\\"+path)\n",
    "    return path_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c67b2",
   "metadata": {},
   "source": [
    "Con una lista de codigos de emociones, caminos a los audios y una funcion para obtener el codigo a partir del path, devuelve solo los paths de las emociones buscadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "364254b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths_for_emotions_keys(emotions_code, files_path, get_code):\n",
    "    paths = []\n",
    "    emotions_set = set(emotions_code)\n",
    "    for code_file in files_path:\n",
    "        if (get_code(code_file) in emotions_set):\n",
    "            paths.append(code_file)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78353db8",
   "metadata": {},
   "source": [
    "Esta funcion abre el archivo y obtiene el mfcc escalado en un vector de 40 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc8cf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872e8e",
   "metadata": {},
   "source": [
    "Esta funcion permite guardar los MFCC con referencia del audio obtenido en un archivo json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddd2259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_elements_in_json(examples_saved, name):\n",
    "    json_files = []\n",
    "    json_file = {}\n",
    "    index = 0\n",
    "    for file in examples_saved:\n",
    "        json_file = {\"id\": index, \"features\":[str(elem) for elem in file[0]] ,\"code\":file[1], \"path\":file[2]}\n",
    "        json_files.append(json_file)\n",
    "        index += 1\n",
    "    json_object = json.dumps(json_files)\n",
    "    with open(f\"{name}.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b9065",
   "metadata": {},
   "source": [
    "La funcion permite cargar datos como el MFCC y referencias a la ubicacion del audio de un archivo json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "278f895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elements_from_json(name):\n",
    "    f = open(f'{name}.json')\n",
    "    data = json.load(f)\n",
    "    examples = []\n",
    "    for element in data:\n",
    "        examples.append(([float(feature) for feature in (element[\"features\"])], element[\"code\"]))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f4074",
   "metadata": {},
   "source": [
    "La funcion nos permite devolver una lista de MFCC obtenidos de una lista de paths, el MFCC tiene un limite que no le permite cargar archivos de menos igual a 44 kb, al final si se le paso un diccionario imprime las estadisticas de los datos obtenidos en el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed0232cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(datas_file,get_code, files_filters = dict()):\n",
    "    examples = []\n",
    "    for data_file in datas_file:\n",
    "        file_stats = os.stat(data_file)\n",
    "        if (file_stats.st_size > 44):\n",
    "            feature = features_extractor(data_file)\n",
    "            files_filters[get_code(data_file)]+= 1\n",
    "            examples.append((feature,get_code(data_file), data_file))\n",
    "    print(files_filters)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302b38",
   "metadata": {},
   "source": [
    "Selecciona n lineas a paritir de unos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab7c5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elements(examples, code, quantity, new_code):\n",
    "    random.shuffle(examples)\n",
    "    elements = []\n",
    "    counter = 1\n",
    "    for example in examples:\n",
    "        if (counter > quantity):\n",
    "            break\n",
    "        if code == example[1]:\n",
    "            elements.append((example[0],new_code))\n",
    "            counter = counter + 1\n",
    "    return elements\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a4bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\AudioWav\")\n",
    "emotions_code = [\"NEU\", \"FEA\",\"ANG\"]\n",
    "datas_files = extract_paths_for_emotions_keys(emotions_code, files_path, get_code_crema_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "54a6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path_s = get_files_from_path(f\"{os.getcwd()}\\\\..\\\\Datasets\\\\ALL\")\n",
    "emotions_code_s = [\"a\", \"f\",\"n\"]\n",
    "datas_files_s = extract_paths_for_emotions_keys(emotions_code_s, files_path_s, get_code_savee) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e14ce",
   "metadata": {},
   "source": [
    "Los siguientes bloques obtienen todos los MFCC de una lista de paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0fcc9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "files_filters = dict()\n",
    "files_filters[\"NEU\"] = 0\n",
    "files_filters[\"FEA\"] = 0\n",
    "files_filters[\"ANG\"] = 0\n",
    "files_filters[\"a\"] = 0\n",
    "files_filters[\"f\"] = 0\n",
    "files_filters[\"n\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a0cd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 0, 'f': 0, 'n': 0}\n"
     ]
    }
   ],
   "source": [
    "examples = get_features(datas_files, get_code_crema_d, files_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e3cd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEU': 1087, 'FEA': 1271, 'ANG': 1270, 'a': 60, 'f': 60, 'n': 120}\n"
     ]
    }
   ],
   "source": [
    "examples_s = get_features(datas_files_s, get_code_savee, files_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71d26b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = examples + examples_s\n",
    "entries = []\n",
    "for example in es:\n",
    "    entries.append((example[0], example[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a9251",
   "metadata": {},
   "source": [
    "Se filtra en un lista la cantidad de datos por cada emocion, devuelve la cantidad de entradas con una salida que indique si existe o no estres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "13fa56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = select_elements(entries, 'NEU', 896,\"without_stress\")\n",
    "datas += select_elements(entries, 'ANG', 550, \"stress\")\n",
    "datas += select_elements(entries, 'FEA', 550, \"stress\")\n",
    "datas += select_elements(entries, 'a', 60, \"stress\")\n",
    "datas += select_elements(entries, 'f', 60, \"stress\")\n",
    "datas += select_elements(entries, 'n', 120, \"without_stress\")\n",
    "random.shuffle(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "deae85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for data in datas:\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder=preprocessing.LabelEncoder()\n",
    "y = to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d475f6",
   "metadata": {},
   "source": [
    "Se separa los datos en una parte para el entrenamiento y en otro para el testeo apartir de un porcentaje (0.8, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3986252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27a37435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 448 1788 448\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2fb8ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:\\\\Users\\\\bacs2\\\\Downloads\\\\Taller De Grado\\\\Previous\\\\Datasets\\\\AudioWAV\\\\1001_DFA_ANG_XX.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5319a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c906f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "419b81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion del modelo\n",
    "num_labels = y.shape[1]\n",
    "dim_entrada = (X_train.shape[1],1)\n",
    "    \n",
    "#definiendo modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50,input_shape= dim_entrada))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "60df327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56/56 [==============================] - 4s 27ms/step - loss: 0.6816 - accuracy: 0.5503 - val_loss: 0.6595 - val_accuracy: 0.5915\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.6264 - accuracy: 0.6594 - val_loss: 0.5843 - val_accuracy: 0.6808\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 0.5831 - accuracy: 0.6879 - val_loss: 0.5630 - val_accuracy: 0.7009\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 0.5609 - accuracy: 0.7008 - val_loss: 0.5309 - val_accuracy: 0.7188\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.5460 - accuracy: 0.7192 - val_loss: 0.5315 - val_accuracy: 0.7031\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 1s 27ms/step - loss: 0.5712 - accuracy: 0.6834 - val_loss: 0.5350 - val_accuracy: 0.7188\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.5365 - accuracy: 0.7131 - val_loss: 0.5380 - val_accuracy: 0.7098\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.5207 - accuracy: 0.7220 - val_loss: 0.5134 - val_accuracy: 0.7344\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.5194 - accuracy: 0.7204 - val_loss: 0.5365 - val_accuracy: 0.7143\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 0.5193 - accuracy: 0.7299 - val_loss: 0.6173 - val_accuracy: 0.7054\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.5203 - accuracy: 0.7215 - val_loss: 0.5750 - val_accuracy: 0.6920\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.5099 - accuracy: 0.7416 - val_loss: 0.5156 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.5060 - accuracy: 0.7276 - val_loss: 0.5019 - val_accuracy: 0.7344\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.5065 - accuracy: 0.7321 - val_loss: 0.4982 - val_accuracy: 0.7433\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.5017 - accuracy: 0.7399 - val_loss: 0.5037 - val_accuracy: 0.7478\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4950 - accuracy: 0.7511 - val_loss: 0.4929 - val_accuracy: 0.7500\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4922 - accuracy: 0.7444 - val_loss: 0.5056 - val_accuracy: 0.7344\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4925 - accuracy: 0.7332 - val_loss: 0.5071 - val_accuracy: 0.7232\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4891 - accuracy: 0.7411 - val_loss: 0.5361 - val_accuracy: 0.7031\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.5079 - accuracy: 0.7271 - val_loss: 0.5248 - val_accuracy: 0.7031\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4874 - accuracy: 0.7388 - val_loss: 0.4864 - val_accuracy: 0.7612\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4771 - accuracy: 0.7522 - val_loss: 0.4701 - val_accuracy: 0.7679\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4758 - accuracy: 0.7433 - val_loss: 0.4728 - val_accuracy: 0.7589\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4731 - accuracy: 0.7506 - val_loss: 0.4772 - val_accuracy: 0.7612\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4821 - accuracy: 0.7506 - val_loss: 0.4953 - val_accuracy: 0.7500\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4640 - accuracy: 0.7545 - val_loss: 0.4785 - val_accuracy: 0.7701\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.4718 - accuracy: 0.7556 - val_loss: 0.4789 - val_accuracy: 0.7545\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 0.4712 - accuracy: 0.7562 - val_loss: 0.4893 - val_accuracy: 0.7478\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 0.4582 - accuracy: 0.7690 - val_loss: 0.4787 - val_accuracy: 0.7545\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4542 - accuracy: 0.7690 - val_loss: 0.4915 - val_accuracy: 0.7277\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4977 - accuracy: 0.7248 - val_loss: 0.4933 - val_accuracy: 0.7522\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.4813 - accuracy: 0.7416 - val_loss: 0.4738 - val_accuracy: 0.7545\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 0.4443 - accuracy: 0.7707 - val_loss: 0.4741 - val_accuracy: 0.7589\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.4506 - accuracy: 0.7534 - val_loss: 0.4426 - val_accuracy: 0.7835\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 1s 23ms/step - loss: 0.4347 - accuracy: 0.7690 - val_loss: 0.4688 - val_accuracy: 0.7411\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.4253 - accuracy: 0.7707 - val_loss: 0.4649 - val_accuracy: 0.7879\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 1s 23ms/step - loss: 0.4751 - accuracy: 0.7472 - val_loss: 0.5828 - val_accuracy: 0.6272\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 0.5146 - accuracy: 0.7114 - val_loss: 0.5167 - val_accuracy: 0.7054\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 1s 23ms/step - loss: 0.4635 - accuracy: 0.7545 - val_loss: 0.4652 - val_accuracy: 0.7812\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 0.4730 - accuracy: 0.7522 - val_loss: 0.4662 - val_accuracy: 0.7522\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.4437 - accuracy: 0.7640 - val_loss: 0.4656 - val_accuracy: 0.7701\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.4546 - accuracy: 0.7511 - val_loss: 0.5171 - val_accuracy: 0.7411\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.4426 - accuracy: 0.7752 - val_loss: 0.4597 - val_accuracy: 0.7812\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.4399 - accuracy: 0.7763 - val_loss: 0.4573 - val_accuracy: 0.7768\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.4286 - accuracy: 0.7813 - val_loss: 0.4498 - val_accuracy: 0.7723\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 0.4166 - accuracy: 0.7824 - val_loss: 0.4460 - val_accuracy: 0.7879\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.4194 - accuracy: 0.7819 - val_loss: 0.4876 - val_accuracy: 0.7902\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.4159 - accuracy: 0.7768 - val_loss: 0.4462 - val_accuracy: 0.7924\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4239 - accuracy: 0.7852 - val_loss: 0.4509 - val_accuracy: 0.7790\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 0.4226 - accuracy: 0.7696 - val_loss: 0.4395 - val_accuracy: 0.7991\n",
      "0.7991071343421936\n"
     ]
    }
   ],
   "source": [
    "#numero de epocas\n",
    "num_epochs = 50\n",
    "num_batch_size = 32\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "model.fit(X_train, y_train, batch_size=num_batch_size,epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "duration = datetime.datetime.now() - start\n",
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b0dbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7991071343421936\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f4fcbace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dea51c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "#https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d1165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
